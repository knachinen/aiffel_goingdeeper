{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bc1703",
   "metadata": {},
   "source": [
    "# 프로젝트: KoChatGPT 업그레이드 하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4bc50e",
   "metadata": {},
   "source": [
    "1. 우리가 지난시간 살펴본 KoChatGPT 모델에 사용한 데이터셋은 아직 완벽히 정제되지 않았습니다.\n",
    "\n",
    "2. Hunman Feedback이 반영된 데이터셋을 대체하기 위해 SFT와 RM 모델에 사용할 다양한 benchmark 데이터셋도 검토해볼 수 있습니다.\n",
    "\n",
    "3. 언어모델의 생성능력을 좌우하는 최선의 디코딩을 위한 하이퍼파라미터 서치가 필요합니다.\n",
    "\n",
    "4. 생성된 답변에 대한 주관적인 평가를 보완할 수 있는 정량적인 메트릭은 도입하지 않았었습니다.\n",
    "\n",
    "5. LLM Trend Note1에서 살펴본 다양한 Instruction Tuning 및 Prompting 기법들도 적용해볼만 합니다.\n",
    "\n",
    "6. 무엇보다 foundation model로 사용한 KoGPT-2는 Emergent abilities를 기대하기엔 다소 작은 사이즈의 모델입니다. 더 큰 파라미터 스케일을 가진 모델을 사용해보거나,\n",
    "\n",
    "7. 더 효율적인 연산을 수행할 수 있는 LoRA의 적용 또는 새로운 Instruction Tuning 및 reward ranking 알고리즘을 도입해볼 수도 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b88a758c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 저장 커스텀 모듈\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../custom\")\n",
    "\n",
    "from importlib import reload\n",
    "import custom_utils\n",
    "reload(custom_utils)\n",
    "\n",
    "from custom_utils import save_var, load_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9272315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "# Check for and set up GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_cuda_memory_summary():\n",
    "    # Obtain and print GPU memory summary\n",
    "    memory_summary = torch.cuda.memory_summary(device=device, abbreviated=False)\n",
    "    print(memory_summary)\n",
    "    \n",
    "def empty_cuda_cache():\n",
    "    # Run your deep learning code on the GPU\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "882cfc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer\n",
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10588b42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   13501 MB |   13557 MB |   23851 MB |   10350 MB |\n",
      "|       from large pool |   13473 MB |   13529 MB |   23177 MB |    9704 MB |\n",
      "|       from small pool |      28 MB |     176 MB |     674 MB |     645 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   13501 MB |   13557 MB |   23851 MB |   10350 MB |\n",
      "|       from large pool |   13473 MB |   13529 MB |   23177 MB |    9704 MB |\n",
      "|       from small pool |      28 MB |     176 MB |     674 MB |     645 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   14230 MB |   14274 MB |   16260 MB |    2030 MB |\n",
      "|       from large pool |   14146 MB |   14190 MB |   16078 MB |    1932 MB |\n",
      "|       from small pool |      84 MB |     182 MB |     182 MB |      98 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  745803 KB |     921 MB |    7349 MB |    6621 MB |\n",
      "|       from large pool |  689088 KB |     866 MB |    6606 MB |    5933 MB |\n",
      "|       from small pool |   56715 KB |      56 MB |     742 MB |     687 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     734    |    1127    |    3819    |    3085    |\n",
      "|       from large pool |     293    |     343    |    1241    |     948    |\n",
      "|       from small pool |     441    |     784    |    2578    |    2137    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     734    |    1127    |    3819    |    3085    |\n",
      "|       from large pool |     293    |     343    |    1241    |     948    |\n",
      "|       from small pool |     441    |     784    |    2578    |    2137    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     308    |     389    |     425    |     117    |\n",
      "|       from large pool |     266    |     298    |     334    |      68    |\n",
      "|       from small pool |      42    |      91    |      91    |      49    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     370    |     371    |    1765    |    1395    |\n",
      "|       from large pool |     262    |     263    |     802    |     540    |\n",
      "|       from small pool |     108    |     110    |     963    |     855    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "empty_cuda_cache()\n",
    "get_cuda_memory_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48679873",
   "metadata": {},
   "source": [
    "# 1. 기존 데이터셋 추가 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388445a8",
   "metadata": {},
   "source": [
    "```\n",
    "data_kochatgpt 폴더에는 세 파일이 있습니다.\n",
    "ㄱ. kochatgpt_1_SFT.jsonl : SFT를 위한 prompt와 completion 문장셋\n",
    "ㄴ. kochatgpt_1_RM.jsonl : RM 학습을 위한 prompt와 세 가지 ranking 문장셋\n",
    "ㄷ. kochatgpt_1_PPO.jsonl : promt 문장\n",
    "\n",
    "각 말뭉치를 EDA하여 도메인과 문체, 길이분포, 문장의 완성도 등을 분석합니다.\n",
    "언어모델의 문장생성능력은 말뭉치의 전처리 수준에 큰 영향을 받습니다.\n",
    "말뭉치의 분석결과를 토대로 데이터를 정제하여 모델을 재학습시켜봅니다.\n",
    "(정제후 데이터셋 크기가 줄어들지 않도록, 다양한 augmentation 기법을 활용하여 크기를 유지 내지 증량합니다.)\n",
    "추가 전처리 후, 기존 인퍼런스 결과와 성능을 비교해봅니다.\n",
    "(주관적인 평가와 BLEU, ROUGE 등을 활용한 정량적인 평가 결과를 비교 분석하여 제시합니다.)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "398f25c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "def get_json_dataset(data):\n",
    "    with open(data, \"r\", encoding='utf-8-sig') as json_file:\n",
    "        dataset = json.load(json_file)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25496e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "data_sft = 'KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_sft, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7efd0476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288}]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_data_dict[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "98163504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "path_tmp = 'data/kochatgpt_1_sft_aug.jsonl' \n",
    "with open(path_tmp, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    tmp = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "f5f9c0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(list_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06dd6cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   prompt      12000 non-null  object\n",
      " 1   completion  12000 non-null  object\n",
      " 2   tokens      12000 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 281.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e25adefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAVWUlEQVR4nO3df4zc9X3n8ef7cEMSfGcb6G19tnXrNlYqDl9bsgJHqU7ruEcMpDF/0AhkFZv6ZJ2ONLRxlUD7B7pWUR3dpdSRWtRtcANVhElpVCxKS10no6hSoeA0xQaSshAHvDI4CY57m6SXuPe+P+ZjMt3s2vNjd3ZnPs+HNNrv9/P9zMznre/q9f3OZ74zE5mJJKkO/2axByBJ6h9DX5IqYuhLUkUMfUmqiKEvSRVZttgDOJ/LL788R0dH2+r77W9/m0suuWRhB7SIhrm+Ya4NrG+QDWptR44c+UZm/uhs25Z06I+OjvL000+31bfRaDA+Pr6wA1pEw1zfMNcG1jfIBrW2iPjaXNuc3pGkihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIos6U/k9tvonX8+a/vxvTf0eSSStDA805ekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIpcMPQjYn9EnIqIY7Ns2xMRGRGXl/WIiE9ExGREPBMRV7X03RERL5TbjvktQ5LUjnbO9D8FbJ3ZGBHrgGuBl1uarwM2lNtu4N7S91LgbuAa4Grg7ohY1cvAJUmdu2DoZ+YXgNdn2XQP8GEgW9q2AQ9k0xPAyohYDbwHOJSZr2fmaeAQsxxIJEkLq6svXIuIbcBUZv5DRLRuWgO80rJ+orTN1T7bY++m+SqBkZERGo1GW2Oanp5uu+9c9mw8O2t7r487H+ajvqVqmGsD6xtkw1hbx6EfEW8Ffp3m1M68y8wJYAJgbGwsx8fH27pfo9Gg3b5z2TnXt2xu7+1x58N81LdUDXNtYH2DbBhr6+bqnZ8A1gP/EBHHgbXAFyPix4ApYF1L37Wlba52SVIfdRz6mXk0M/99Zo5m5ijNqZqrMvNV4CBwa7mKZxNwJjNPAo8D10bEqvIG7rWlTZLUR+1csvkg8LfA2yPiRETsOk/3x4CXgEngD4H/AZCZrwO/BTxVbr9Z2iRJfXTBOf3MvOUC20dblhO4fY5++4H9HY5vSfOXtiQNGj+RK0kVMfQlqSKGviRVpKsPZ9Vmrrl7SRo0nulLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFDH1JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUkQt+n35E7AfeC5zKzCtL2/8Cfh74HvAicFtmfqtsuwvYBfwL8MHMfLy0bwX2ARcBn8zMvfNezRLhb+dKWqraOdP/FLB1Rtsh4MrM/M/APwJ3AUTEFcDNwH8q9/n9iLgoIi4Cfg+4DrgCuKX0lST10QVDPzO/ALw+o+2vMvNsWX0CWFuWtwEHMvP/ZuZXgUng6nKbzMyXMvN7wIHSV5LUR/Pxc4m/BDxUltfQPAicc6K0Abwyo/2a2R4sInYDuwFGRkZoNBptDWJ6errtvnPZs/HshTv1oJfxzUd9S9Uw1wbWN8iGsbaeQj8ifgM4C3x6foYDmTkBTACMjY3l+Ph4W/drNBq023cuOxf4t3CPbx/v+r7zUd9SNcy1gfUNsmGsrevQj4idNN/g3ZKZWZqngHUt3daWNs7TLknqk64u2SxX4nwYeF9mfqdl00Hg5oi4OCLWAxuAvwOeAjZExPqIeBPNN3sP9jZ0SVKn2rlk80FgHLg8Ik4Ad9O8Wudi4FBEADyRmf89M5+NiM8Az9Gc9rk9M/+lPM4HgMdpXrK5PzOfXYB6JEnnccHQz8xbZmm+7zz9Pwp8dJb2x4DHOhqdJGle+YlcSaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkXm41s2B85cP3IiScPOM31JqkiVZ/qLxZ9RlLTYPNOXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0JakiFwz9iNgfEaci4lhL26URcSgiXih/V5X2iIhPRMRkRDwTEVe13GdH6f9CROxYmHIkSefTzpn+p4CtM9ruBA5n5gbgcFkHuA7YUG67gXuheZCg+YPq1wBXA3efO1BIkvrngqGfmV8AXp/RvA24vyzfD9zY0v5ANj0BrIyI1cB7gEOZ+XpmngYO8cMHEknSAuv2axhGMvNkWX4VGCnLa4BXWvqdKG1ztf+QiNhN81UCIyMjNBqNtgY0PT3ddt89G8+21a9f2hl3J/UNmmGuDaxvkA1jbT1/905mZkTkfAymPN4EMAEwNjaW4+Pjbd2v0WjQbt+dS+xbNo9vH79gn07qGzTDXBtY3yAbxtq6vXrntTJtQ/l7qrRPAeta+q0tbXO1S5L6qNvQPwicuwJnB/BIS/ut5SqeTcCZMg30OHBtRKwqb+BeW9okSX10wemdiHgQGAcuj4gTNK/C2Qt8JiJ2AV8D3l+6PwZcD0wC3wFuA8jM1yPit4CnSr/fzMyZbw5LkhbYBUM/M2+ZY9OWWfomcPscj7Mf2N/R6CRJ88pP5EpSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SK9Pw1DOrd6BxfC3F87w19HomkYeeZviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5Iq0lPoR8SvRsSzEXEsIh6MiDdHxPqIeDIiJiPioYh4U+l7cVmfLNtH56UCSVLbug79iFgDfBAYy8wrgYuAm4GPAfdk5tuA08CucpddwOnSfk/pJ0nqo16nd5YBb4mIZcBbgZPAu4GHy/b7gRvL8rayTtm+JSKix+eXJHUgMrP7O0fcAXwU+C7wV8AdwBPlbJ6IWAf8RWZeGRHHgK2ZeaJsexG4JjO/MeMxdwO7AUZGRt5x4MCBtsYyPT3N8uXL2+p7dOpMW/0W28Y1K95Y7qS+QTPMtYH1DbJBrW3z5s1HMnNstm1d/4hKRKyiefa+HvgW8CfA1m4f75zMnAAmAMbGxnJ8fLyt+zUaDdrtu3OOHy1Zao5vH39juZP6Bs0w1wbWN8iGsbZepnd+DvhqZn49M78PfBZ4F7CyTPcArAWmyvIUsA6gbF8BfLOH55ckdaiX0H8Z2BQRby1z81uA54DPAzeVPjuAR8rywbJO2f657GVuSZLUsa5DPzOfpPmG7BeBo+WxJoCPAB+KiEngMuC+cpf7gMtK+4eAO3sYtySpCz39MHpm3g3cPaP5JeDqWfr+M/ALvTyfJKk3fiJXkipi6EtSRXqa3tHCGm25tHTPxrNvXGp6fO8NizUkSQPOM31JqoihL0kVMfQlqSKGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkV6Cv2IWBkRD0fElyPi+Yh4Z0RcGhGHIuKF8ndV6RsR8YmImIyIZyLiqvkpQZLUrl7P9PcBf5mZPwn8FPA8cCdwODM3AIfLOsB1wIZy2w3c2+NzS5I61HXoR8QK4L8A9wFk5vcy81vANuD+0u1+4MayvA14IJueAFZGxOpun1+S1LnIzO7uGPHTwATwHM2z/CPAHcBUZq4sfQI4nZkrI+JRYG9m/k3Zdhj4SGY+PeNxd9N8JcDIyMg7Dhw40NZ4pqenWb58eVt9j06daavfUjLyFnjtu83ljWtWLO5g5lkn+24QWd/gGtTaNm/efCQzx2bb1ssPoy8DrgJ+OTOfjIh9/GAqB4DMzIjo6KiSmRM0DyaMjY3l+Ph4W/drNBq023dnyw+OD4o9G8/y8aPN3XV8+/jiDmaedbLvBpH1Da5hrK2X0D8BnMjMJ8v6wzRD/7WIWJ2ZJ8v0zamyfQpY13L/taVNHRo9z0Hr+N4b+jgSSYOm6zn9zHwVeCUi3l6attCc6jkI7ChtO4BHyvJB4NZyFc8m4Exmnuz2+SVJnevlTB/gl4FPR8SbgJeA22geSD4TEbuArwHvL30fA64HJoHvlL6SpD7qKfQz80vAbG8WbJmlbwK39/J8kqTe+IlcSaqIoS9JFTH0Jakihr4kVcTQl6SKGPqSVBFDX5IqYuhLUkUMfUmqiKEvSRUx9CWpIoa+JFXE0Jekihj6klQRQ1+SKmLoS1JFev3lLC0xc/1+rr+dKwk805ekqvQc+hFxUUT8fUQ8WtbXR8STETEZEQ+V388lIi4u65Nl+2ivzy1J6sx8nOnfATzfsv4x4J7MfBtwGthV2ncBp0v7PaWfJKmPegr9iFgL3AB8sqwH8G7g4dLlfuDGsrytrFO2byn9JUl9EpnZ/Z0jHgZ+G/i3wK8BO4Enytk8EbEO+IvMvDIijgFbM/NE2fYicE1mfmPGY+4GdgOMjIy848CBA22NZXp6muXLl7fV9+jUmbb6LSUjb4HXvtv9/TeuWTF/g5lnney7QWR9g2tQa9u8efORzBybbVvXV+9ExHuBU5l5JCLGu32cmTJzApgAGBsby/Hx9h660WjQbt+dc1zhspTt2XiWjx/t/mKr49vH528w86yTfTeIrG9wDWNtvVyy+S7gfRFxPfBm4N8B+4CVEbEsM88Ca4Gp0n8KWAeciIhlwArgmz08vySpQ13P6WfmXZm5NjNHgZuBz2XmduDzwE2l2w7gkbJ8sKxTtn8ue5lbkiR1bCGu0/8I8KGImAQuA+4r7fcBl5X2DwF3LsBzS5LOY14+kZuZDaBRll8Crp6lzz8DvzAfzydJ6o6fyJWkihj6klQRQ1+SKmLoS1JFDH1JqoihL0kV8UdUKuGPq0gCz/QlqSqGviRVxNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFTH0JakiQ/2J3Lk+hSpJtfJMX5IqYuhLUkW6nt6JiHXAA8AIkMBEZu6LiEuBh4BR4Djw/sw8HREB7AOuB74D7MzML/Y2fPXKL2KT6tLLmf5ZYE9mXgFsAm6PiCuAO4HDmbkBOFzWAa4DNpTbbuDeHp5bktSFrkM/M0+eO1PPzP8DPA+sAbYB95du9wM3luVtwAPZ9ASwMiJWd/v8kqTORWb2/iARo8AXgCuBlzNzZWkP4HRmroyIR4G9mfk3Zdth4COZ+fSMx9pN85UAIyMj7zhw4EBbY5ienmb58uX/qu3o1JkeqlpaRt4Cr323f8+3cc2Kvj3XbPtumFjf4BrU2jZv3nwkM8dm29bzJZsRsRz4U+BXMvOfmjnflJkZER0dVTJzApgAGBsby/Hx8bbu12g0mNl35xBdsrln41k+frR/V9ge3z7et+eabd8NE+sbXMNYW09X70TEj9AM/E9n5mdL82vnpm3K31OlfQpY13L3taVNktQnXYd+mbq5D3g+M3+nZdNBYEdZ3gE80tJ+azRtAs5k5slun1+S1Lle5gveBfwicDQivlTafh3YC3wmInYBXwPeX7Y9RvNyzUmal2ze1sNzS5K60HXolzdkY47NW2bpn8Dt3T6fJKl3fiJXkipi6EtSRQx9SaqIoS9JFRnq79NX9/wiNmk4eaYvSRUx9CWpIk7vqCNO+0iDzTN9SaqIoS9JFXF6R/PCaR9pMHimL0kV8Uxfi6L1lcGejWff+MEbXxlIC8vQ14Kaa9qn0/4eDKT5YehrIHgwkOaHoa/qeABRzQx96QI8SGiYGPoaaJ2+ZyDVztCXCt90Vg36HvoRsRXYB1wEfDIz9/Z7DNJCmnkwaL0kdTYeJNRPfQ39iLgI+D3gvwIngKci4mBmPtfPcUhLyXxNUc118Djf43vAqU+/z/SvBiYz8yWAiDgAbAMMfalH3Rw8Op2imq3/+V7JeFBZeiIz+/dkETcBWzPzv5X1XwSuycwPtPTZDewuq28HvtLmw18OfGMeh7vUDHN9w1wbWN8gG9Ta/mNm/uhsG5bcG7mZOQFMdHq/iHg6M8cWYEhLwjDXN8y1gfUNsmGsrd9fuDYFrGtZX1vaJEl90O/QfwrYEBHrI+JNwM3AwT6PQZKq1dfpncw8GxEfAB6necnm/sx8dp4evuMpoQEzzPUNc21gfYNs6Grr6xu5kqTF5Y+oSFJFDH1JqsjAh35EbI2Ir0TEZETcudjj6UZErIuIz0fEcxHxbETcUdovjYhDEfFC+buqtEdEfKLU/ExEXLW4FVxYRFwUEX8fEY+W9fUR8WSp4aHyxj4RcXFZnyzbRxd14G2IiJUR8XBEfDkino+Idw7ZvvvV8n95LCIejIg3D/L+i4j9EXEqIo61tHW8vyJiR+n/QkTsWIxaujHQod/ytQ7XAVcAt0TEFYs7qq6cBfZk5hXAJuD2UsedwOHM3AAcLuvQrHdDue0G7u3/kDt2B/B8y/rHgHsy823AaWBXad8FnC7t95R+S90+4C8z8yeBn6JZ51Dsu4hYA3wQGMvMK2legHEzg73/PgVsndHW0f6KiEuBu4FraH7TwN3nDhRLXmYO7A14J/B4y/pdwF2LPa55qOsRmt9P9BVgdWlbDXylLP8BcEtL/zf6LcUbzc9jHAbeDTwKBM1POS6buR9pXtn1zrK8rPSLxa7hPLWtAL46c4xDtO/WAK8Al5b98SjwnkHff8AocKzb/QXcAvxBS/u/6reUbwN9ps8P/iHPOVHaBlZ5OfwzwJPASGaeLJteBUbK8qDV/bvAh4H/V9YvA76VmWfLeuv436itbD9T+i9V64GvA39Upq8+GRGXMCT7LjOngP8NvAycpLk/jjA8+++cTvfXQO3HVoMe+kMlIpYDfwr8Smb+U+u2bJ5ODNz1tRHxXuBUZh5Z7LEskGXAVcC9mfkzwLf5wdQAMLj7DqBMWWyjeXD7D8Al/PDUyFAZ5P3VjkEP/aH5WoeI+BGagf/pzPxsaX4tIlaX7auBU6V9kOp+F/C+iDgOHKA5xbMPWBkR5z4c2Dr+N2or21cA3+zngDt0AjiRmU+W9YdpHgSGYd8B/Bzw1cz8emZ+H/gszX06LPvvnE7316DtxzcMeugPxdc6REQA9wHPZ+bvtGw6CJy7KmAHzbn+c+23lisLNgFnWl6aLimZeVdmrs3MUZr753OZuR34PHBT6TaztnM131T6L9mzrsx8FXglIt5emrbQ/Krwgd93xcvApoh4a/k/PVffUOy/Fp3ur8eBayNiVXk1dG1pW/oW+02FXm/A9cA/Ai8Cv7HY4+myhp+l+XLyGeBL5XY9zbnQw8ALwF8Dl5b+QfOqpReBozSvrFj0Otqocxx4tCz/OPB3wCTwJ8DFpf3NZX2ybP/xxR53G3X9NPB02X9/Bqwapn0H/E/gy8Ax4I+Biwd5/wEP0nx/4vs0X6nt6mZ/Ab9U6pwEblvsutq9+TUMklSRQZ/ekSR1wNCXpIoY+pJUEUNfkipi6EtSRQx9SaqIoS9JFfn/YCtdgFvSvBQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"tokens\"].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e86ecfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_underlen = df[df[\"tokens\"] < 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d5b9fadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_underlen[\"tokens\"].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d68cdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11054"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_underlen.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "24c8d2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, completion, tokens]\n",
       "Index: []"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_underlen[df_underlen['completion'].isna() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c10ed",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bebe1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6f80625",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_stopwords = 'data/stopword.txt'\n",
    "\n",
    "stopwords = []\n",
    "with open(path_stopwords, 'r') as file:\n",
    "    for line in file:\n",
    "        stopwords.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "782530d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, tokenizer, stopwords):\n",
    "    text = tokenizer.morphs(text) # 토큰화\n",
    "#     text = [word for word in text if word not in stopwords] # 불용어 제거\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e76662",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "833865e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'data/ko.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d40958f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_362/1204155077.py:4: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  fasttext_model = FastText.load_fasttext_format(model_path)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "fasttext_model = FastText.load_fasttext_format(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "250b95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_words = fasttext_model.wv.most_similar('대표', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b7f6ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def lexical_sub(sentence, wv, tokenizer, sentence_num=2, verbose=False):\n",
    "    if len(sentence) == 0: return None\n",
    "    if verbose: print(f\"lexical_sub: sentence={sentence}\")\n",
    "        \n",
    "    sentences = []\n",
    "    text = tokenizer.morphs(sentence)\n",
    "    for _ in range(sentence_num):\n",
    "        sentences.append(text)\n",
    "    \n",
    "    if not sentences[0]: return None\n",
    "    \n",
    "    indices = list(range(len(text)))\n",
    "    random.shuffle(indices)    \n",
    "    unique_indices = indices[:sentence_num]\n",
    "\n",
    "    if verbose: print(\"lexical_sub: similar_words=\", end=\"\")\n",
    "    for i, index in enumerate(unique_indices):\n",
    "        word = sentences[i][index]\n",
    "        if word in wv:\n",
    "            similar_word = wv.most_similar(word)\n",
    "            if not similar_word:\n",
    "                return None\n",
    "            if verbose: print(similar_word[0][0], end=\", \")\n",
    "\n",
    "            word_sub = similar_word[0][0]\n",
    "            sentences[i][index] = word_sub\n",
    "            sentences[i] = \" \".join(sentences[i])\n",
    "            \n",
    "    if verbose: \n",
    "        print()\n",
    "        print(f\"lexical_sub: sentences={sentences}\")\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62aabaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불고기용 고기 한우에요?\n",
      "닭고기, 전용, "
     ]
    },
    {
     "data": {
      "text/plain": [
       "['닭고기 용 고기 한우 에 요 ?', '불고기 전용 고기 한우 에 요 ?']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = lexical_sub(df_underlen['prompt'].iloc[0], fasttext_model.wv, mecab, verbose=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7f5be8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "808ecf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = df_underlen['prompt']\n",
    "answers = df_underlen['completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ccb634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corpus_aug = []\n",
    "count_none = 0\n",
    "\n",
    "for que, ans in tqdm(zip(questions, answers), total=len(questions)):\n",
    "    corpus_aug.append((que, ans))\n",
    "    sentences = lexical_sub(que, fasttext_model.wv, mecab, sentence_num=1)\n",
    "    if sentences != None:\n",
    "        for sentence in sentences:\n",
    "            if sentence is not None:\n",
    "                corpus_aug.append((sentence, ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e4b09187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22105"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a3ffaddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 ('불고기용 고기 한우에요?', \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\")\n",
      " 1 ('불고기 용 고기 축산업 에 요 ?', \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\")\n",
      " 2 ('쓰던 앱이 유료로 전환됐어', \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\")\n",
      " 3 ('쓰 던 앱 은 유료 로 전환 됐 어', \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\")\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(corpus_aug):\n",
    "    print(f\"{index:2} {item}\")\n",
    "    if index >= 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "eb0857a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = pd.DataFrame(corpus_aug, columns=[\"prompt\", \"completion\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3c59b723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>불고기용 고기 한우에요?</td>\n",
       "      <td>'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>불고기 용 고기 축산업 에 요 ?</td>\n",
       "      <td>'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>쓰던 앱이 유료로 전환됐어</td>\n",
       "      <td>'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               prompt                                         completion\n",
       "0       불고기용 고기 한우에요?  '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...\n",
       "1  불고기 용 고기 축산업 에 요 ?  '저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하...\n",
       "2      쓰던 앱이 유료로 전환됐어  '어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 ..."
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aug.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ef49e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug.to_json('data/kochatgpt_1_sft_aug.jsonl', orient='records', force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbf806",
   "metadata": {},
   "source": [
    "# 2. 새로운 데이터셋 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d495f3",
   "metadata": {},
   "source": [
    "```\n",
    "KoChatGPT는 human feedback이 반영된 데이터를 직접 사용하는 대신\n",
    "ChatGPT API를 사용하는 대안을 선택했습니다.\n",
    "LLM Trend Note1 에서 살펴보았듯이\n",
    "Anthropic의 RLHF는 StackExchange 같은 온라인 상의 댓글정보를 활용하여\n",
    "ranking dataset을 구축해 구현되었습니다.\n",
    "우리도 비슷한 로직을 적용해볼 수 있습니다.\n",
    "\n",
    "하나의 prompt에 대한 다양한 수준의 품질로 댓글이 달린 한국어로 된 웹사이트를 찾아봅시다.\n",
    "웹크롤링 기법을 사용해 reward 점수를 차등적으로 적용해볼 수 있는\n",
    "instruction dataset과 ranking dataset을 구축해봅니다.\n",
    "\n",
    "KorQuAD 2.0 같은 한국어 이해 benchmark를 활용해 고품질의 데이터셋을 확보하고,\n",
    "KoGPT-2를 사용해 빠르게 저품질 데이터셋을 페어링해볼 수도 있습니다.\n",
    "다양한 데이터 증량전략을 구사하여 기존 데이터셋에 새로 구축한 데이터셋을 추가해\n",
    "모델을 재학습시키고 추론 결과를 비교하 분석하여 제시해보세요.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d30e578f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2230779a0f504a46ae18a21d3bd02f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9f1544767f440f89c9d95f47eaa0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255891d4c7b74acfa1b829fd67c628f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/6.12k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad_kor_v1/squad_kor_v1 to /aiffel/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f1303f02284b39b6dc60666405de42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28e3a1cb10245389c5af8968fa2966a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/7.57M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea1f71a6268493485f042787b3c048a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/770k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3489d84b7346ee8d0b9dbe8a8c7859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/60407 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5774 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad_kor_v1 downloaded and prepared to /aiffel/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b1470f888f44b13aed71b2e06268b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 60407\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
      "        num_rows: 5774\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('squad_kor_v1')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "cccc1e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_ext = []\n",
    "for index, item in enumerate(ds[\"train\"]):\n",
    "    # print(index, \":\\n\", item, \"\\n\")\n",
    "    dataset_ext.append({'prompt':item['question'], \n",
    "                        'completion':f\"{item['answers']['text'][0]}. {item['context']}\"})\n",
    "    # if index >= 3: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "3cd6cc9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
       "  'completion': '교향곡. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'},\n",
       " {'prompt': '바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?',\n",
       "  'completion': '1악장. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'},\n",
       " {'prompt': '바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?',\n",
       "  'completion': '베토벤의 교향곡 9번. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'}]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ext[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "4ac97791",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext = df_aug.append(dataset_ext, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "1f372cb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82512, 2)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "98dd25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ext.to_json('data/kochatgpt_1_sft_ext.jsonl', orient='records', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "87e99dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ds = get_json_dataset('data/kochatgpt_1_sft_ext.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "50e276bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '샤블리는 어떤 품종으로 만들어집니까?',\n",
       " 'completion': '샤르도네. 모든 샤블리는 100% 샤르도네 품종으로 만들어지며, 단순한 스타일의 양조기술을 선호하기 때문에 가장 순수한 샤르도네의 특성을 간직하고 있다. 샤블리 양조자들은 석회 토양 및 서늘한 기후의 떼루아에 의한 높은 산미감을 부각시키고자 노력한다. 샤블리 와인은 옅은 녹색에 노랑빛을 띄며 투명한 특성을 가지고 있고, 청사과 같은 산미를 가지고 있다. 숙성됨에 따라 이러한 산미는 완화될 수 있고, 샤블리는 샤르도네 중 가장 보관 기간이 긴 와인이다. 이는 종종 무감미, 혹은 부싯돌의 향, 혹은 쇠 맛이 난다고 표현된다. 일부 샤블리는 젖은 돌의 향을 가지고 있는데, 일정 기간이 지난 후에는 꿀향으로 변한다. 대부분 부르고뉴의 화이트 와인처럼 샤블리는 병 속에서 숙성됨에 따라 더 좋아지게 된다. 물론 생산자의 스타일과 빈티지가 영향을 미칠 수 있지만, 대부분의 그랑크뤼 샤블리는 약 15년, 프리미에 크뤼는 최소 10년 정도 숙성될 수 있다.'}"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ds[30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "2fc1f98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 82512 entries, 0 to 82511\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   prompt      82512 non-null  object\n",
      " 1   completion  82512 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df_ext.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "39a56b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18220</th>\n",
       "      <td>보르도 의 여 명편 이 참고 한 고고학 가설 이 뭔지 아니</td>\n",
       "      <td>'죄송합니다. 저는 인공지능 언어모델로, 직접적으로 정보를 가지고 있지는 않습니다....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50302</th>\n",
       "      <td>2008년 제작중이었던 람슈타인의 앨범의 수록곡이 확정되었을 때 총 몇 트랙이었는가?</td>\n",
       "      <td>11트랙. 2006년 잠시 공백기를 가진 람슈타인은 2007년 다시 음반작업에 착수...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24584</th>\n",
       "      <td>1867년 혼센지 범종이 파리 만국박람회 이후 옮겨진 미술관의 이름은?</td>\n",
       "      <td>아리아나 미술관. 일본은 1864년에 시모노세키 전쟁으로 빼앗겼던 조슈 번의 대포 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "18220                 보르도 의 여 명편 이 참고 한 고고학 가설 이 뭔지 아니   \n",
       "50302  2008년 제작중이었던 람슈타인의 앨범의 수록곡이 확정되었을 때 총 몇 트랙이었는가?   \n",
       "24584          1867년 혼센지 범종이 파리 만국박람회 이후 옮겨진 미술관의 이름은?   \n",
       "\n",
       "                                              completion  \n",
       "18220  '죄송합니다. 저는 인공지능 언어모델로, 직접적으로 정보를 가지고 있지는 않습니다....  \n",
       "50302  11트랙. 2006년 잠시 공백기를 가진 람슈타인은 2007년 다시 음반작업에 착수...  \n",
       "24584  아리아나 미술관. 일본은 1864년에 시모노세키 전쟁으로 빼앗겼던 조슈 번의 대포 ...  "
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ext.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1c858a",
   "metadata": {},
   "source": [
    "# 3. Foundation Model 교체"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3de9167",
   "metadata": {},
   "source": [
    "```\n",
    "현재 제공되는 LMS GPU 사양으로는 수십 billion 단위 이상의 LLM을 튜닝하기 어렵습니다.\n",
    "그러나 허깅페이스에서 제공하는 큰 규모의 모델을 적은 컴퓨팅 자원으로도 사용할 수 있게 해주는\n",
    "경량화, 최적화 라이브러리를 사용하면\n",
    "속도는 느리지만 우리의 LMS에서도 학습 및 추론이 가능해질 수 있습니다.\n",
    "(힌트 : LLM Trend Note1 노드의 마지막 스텝을 참고해보세요)\n",
    "\n",
    "허깅페이스에서 제공되는 1.2B 사이즈의 한국어 GPT pretrain model로 skt/ko-gpt-trinity-1.2B-v0.5 가 있습니다.\n",
    "해당 모델로 foundation model을 교체해보세요.\n",
    "(단 OOM 문제를 해소하기 위해 허깅페이스에서 제공하는\n",
    "다양한 training argument들을 조합하여 최상의 하이퍼파라미터를 찾아내야 합니다.)\n",
    "데이터셋을 아예 바꿔 모델 선택의 폭을 늘려보는 것도 좋은 선택지입니다.\n",
    "\n",
    "foundation model 교체에 성공했다면, generator 함수를 수정하여 모델 인퍼런스 결과를 제시해보세요.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcce6eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd3c6d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b189258",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd008af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'skt/kogpt2-base-v2'\n",
    "# model_name = 'skt/ko-gpt-trinity-1.2B-v0.5'\n",
    "# model_name = 'taeminlee/kogpt2'\n",
    "# model_name = 'psyche/kogpt'\n",
    "model_name_short = model_name.split(\"/\")[1]\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ad25f8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/ko-gpt-trinity-1.2B-v0.5', vocab_size=51200, model_max_length=16, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\", model_max_length=model_max_length,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18a64fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "#         data_path_1_SFT = 'KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl'\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            if verbose: print(f\"SFT_dataset: example={example}\")\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32005a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc38066f",
   "metadata": {},
   "source": [
    "### 데이터 정제 및 데이터 증강:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0e2cf0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 22105\n"
     ]
    }
   ],
   "source": [
    "# train_dataset = SFT_dataset(data_path_1_SFT='KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "train_dataset = SFT_dataset(data_path_1_SFT='data/kochatgpt_1_sft_aug.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc04b816",
   "metadata": {},
   "source": [
    "### 추가 데이터셋:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473b17c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='data/kochatgpt_1_sft_ext.jsonl', tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c2d3271",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "afb4d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_var(train_dataset, \"train_dataset_ext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ada66dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_var(\"train_dataset_ext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2652d0",
   "metadata": {},
   "source": [
    "### trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626d014c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7990694b",
   "metadata": {},
   "source": [
    "### model: 'psyche/kogpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30dbc843",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 21:22, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.515200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.465200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.381900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.338000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(f'KoChatGPT/{model_name_short}_output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8573e37",
   "metadata": {},
   "source": [
    "- 'psyche/kogpt'\n",
    "  - model max length: 128\n",
    "  - batch size: 4\n",
    "  - ...로 낮추었을 때, 메모리 부족 에러나지 않고 학습 진행 가능함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd730b07",
   "metadata": {},
   "source": [
    "```\n",
    "500     2.731100\n",
    "1000\t2.515200\n",
    "1500\t2.465200\n",
    "2000\t2.386100\n",
    "2500\t2.381900\n",
    "3000\t2.338000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2eb81",
   "metadata": {},
   "source": [
    "### model: 'skt/kogpt2-base-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "8137bd61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5527' max='5527' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5527/5527 18:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.074800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.929700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.489200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.364600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.258700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.185900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(f'KoChatGPT/{model_name_short}_output_1_SFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e601741a",
   "metadata": {},
   "source": [
    "데이터 정제 및 데이터 증강 이전 수치\n",
    "- model: 'skt/kogpt2-base-v2'\n",
    "- batch_size: 8\n",
    "```\n",
    "500\t2.984100\n",
    "1000\t2.776800\n",
    "1500\t2.687200\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30e2eb4",
   "metadata": {},
   "source": [
    "이후 수치\n",
    "- batch_size: 4\n",
    "\n",
    "```\n",
    "500\t3.074800\n",
    "1000\t2.929700\n",
    "1500\t2.773500\n",
    "2000\t2.680000\n",
    "2500\t2.603100\n",
    "3000\t2.489200\n",
    "3500\t2.413400\n",
    "4000\t2.364600\n",
    "4500\t2.300000\n",
    "5000\t2.258700\n",
    "5500\t2.185900\n",
    "```\n",
    "\n",
    "- 수치는 훨씬 낮아지긴 했는데, 과적합이 된건 아닐런지..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c60a450",
   "metadata": {},
   "source": [
    "### model: 'skt/kogpt2-base-v2' - 추가 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a8a8646",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20628' max='20628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20628/20628 1:25:06, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.051000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.956500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.859700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.784300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.734400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.673600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.614400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.565700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.450900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.403000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.332200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.335300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.256800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>3.232700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.157600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>3.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>3.061100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.026700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>2.986100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.947700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>2.926200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.912500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>2.857300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.829100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>2.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.724600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>2.758300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.735800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>2.707100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.708000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>2.657200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>2.653700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.629400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>2.594800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.603600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>2.600600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(f'KoChatGPT/{model_name_short}_output_1_SFT_ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ab04f6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Enter 'c' at the ipdb>  prompt to continue execution.\n",
      "> \u001b[0;32m<string>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='217' max='20628' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  217/20628 00:57 < 1:31:02, 3.74 it/s, Epoch 0.01/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<string>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\n",
      "ipdb> q\n"
     ]
    }
   ],
   "source": [
    "%debug (trainer.train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f8c1290",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = [4.11, 4.051, 3.9565, 3.881, 3.8597, 3.7843, 3.7344, 3.6736, 3.6144, 3.5657, 3.5102, 3.4509, 3.403, 3.3322, \n",
    "          3.3353, 3.2568, 3.2327, 3.1576, 3.1227, 3.1213, 3.0611, 3.0267, 2.9861, 2.9477, 2.9262, 2.9125, 2.8573, \n",
    "          2.8291, 2.805, 2.7246, 2.7583, 2.7358, 2.7071, 2.708, 2.6572, 2.6652, 2.6537, 2.6294, 2.5948, 2.6036, 2.6006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79d76fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "steps = np.arange(500, 21000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fd12c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd11c69f3a0>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(steps, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c83de4",
   "metadata": {},
   "source": [
    "- 추가 데이터셋을 단순하게 덧붙여서 그런지 결과가 향상되지 않았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "991b6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f'KoChatGPT/{model_name_short}_output_1_SFT_ext'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bbcd7c",
   "metadata": {},
   "source": [
    "### model: 'skt/ko-gpt-trinity-1.2B-v0.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cef794b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'skt/ko-gpt-trinity-1.2B-v0.5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bbb477",
   "metadata": {},
   "source": [
    "#### trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594dd4b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"KoChatGPT/test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ac541d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 trainer.train()                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>model.save_pretrained(<span style=\"color: #808000; text-decoration-color: #808000\">f'KoChatGPT/{</span>model_name_short<span style=\"color: #808000; text-decoration-color: #808000\">}_output_1_SFT_aug'</span>)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">3 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1662</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1659 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1660 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1661 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1662 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1663 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1664 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1665 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1991</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1988 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   │   </span>xm.optimizer_step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1989 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.do_grad_scaling:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1990 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>scale_before = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1991 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.step(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.optimizer)                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1992 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.update()                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1993 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>scale_after = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.scaler.get_scale()                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1994 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   │   </span>optimizer_was_run = scale_before &lt;= scale_after                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">338</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">335 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">336 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">assert</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>]) &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">\"No inf checks were rec</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">337 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>338 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retval = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">339 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">340 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"stage\"</span>] = OptState.STEPPED                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">341 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">grad_scaler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">285</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">282 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_maybe_opt_step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, optimizer_state, *args, **kwargs):                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">283 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>retval = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">284 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">sum</span>(v.item() <span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> v <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> optimizer_state[<span style=\"color: #808000; text-decoration-color: #808000\">\"found_inf_per_device\"</span>].values()):    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>285 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>retval = optimizer.step(*args, **kwargs)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">286 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> retval                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">287 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">288 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, optimizer, *args, **kwargs):                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lr_scheduler.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">65</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  62 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>instance = instance_ref()                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>instance._step_count += <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  64 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>wrapped = func.<span style=\"color: #00ff00; text-decoration-color: #00ff00\">__get__</span>(instance, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">cls</span>)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>  65 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapped(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  66 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  67 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Note that the returned function here is no longer a bound method,</span>           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">  68 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># so attributes like `__func__` and `__self__` no longer exist.</span>               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/torch/optim/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimizer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">113</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">wrapper</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">110 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>obj, *_ = args                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>profile_name = <span style=\"color: #808000; text-decoration-color: #808000\">\"Optimizer.step#{}.step\"</span>.format(obj.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span>)     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.autograd.profiler.record_function(profile_name):                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>113 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> func(*args, **kwargs)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">114 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> wrapper                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hooked = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">getattr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__class__</span>.step, <span style=\"color: #808000; text-decoration-color: #808000\">\"hooked\"</span>, <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.9/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">optimization.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">434</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">step</span>                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">431 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(state) == <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">432 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">\"step\"</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">433 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Exponential moving average of gradient values</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>434 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg\"</span>] = torch.zeros_like(p.data)                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">435 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Exponential moving average of squared gradient values</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">436 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>state[<span style=\"color: #808000; text-decoration-color: #808000\">\"exp_avg_sq\"</span>] = torch.zeros_like(p.data)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">437 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA out of memory. Tried to allocate <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58.00</span> MiB <span style=\"font-weight: bold\">(</span>GPU <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14.76</span> GiB total capacity; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.19</span> GiB already \n",
       "allocated; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.75</span> MiB free; <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">13.92</span> GiB reserved in total by PyTorch<span style=\"font-weight: bold\">)</span> If reserved memory is &gt;&gt; allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 trainer.train()                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0mmodel.save_pretrained(\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mKoChatGPT/\u001b[0m\u001b[33m{\u001b[0mmodel_name_short\u001b[33m}\u001b[0m\u001b[33m_output_1_SFT_aug\u001b[0m\u001b[33m'\u001b[0m)                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m3 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1662\u001b[0m in \u001b[92mtrain\u001b[0m                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1659 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1660 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1661 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1662 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1663 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1664 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1665 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1991\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1988 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mxm.optimizer_step(\u001b[96mself\u001b[0m.optimizer)                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1989 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.do_grad_scaling:                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1990 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mscale_before = \u001b[96mself\u001b[0m.scaler.get_scale()                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1991 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.step(\u001b[96mself\u001b[0m.optimizer)                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1992 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.scaler.update()                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1993 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0mscale_after = \u001b[96mself\u001b[0m.scaler.get_scale()                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1994 \u001b[0m\u001b[2m│   │   │   │   │   │   \u001b[0moptimizer_was_run = scale_before <= scale_after                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m338\u001b[0m in \u001b[92mstep\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m335 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m336 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94massert\u001b[0m \u001b[96mlen\u001b[0m(optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m]) > \u001b[94m0\u001b[0m, \u001b[33m\"\u001b[0m\u001b[33mNo inf checks were rec\u001b[0m   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m337 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m338 \u001b[2m│   │   \u001b[0mretval = \u001b[96mself\u001b[0m._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m339 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m340 \u001b[0m\u001b[2m│   │   \u001b[0moptimizer_state[\u001b[33m\"\u001b[0m\u001b[33mstage\u001b[0m\u001b[33m\"\u001b[0m] = OptState.STEPPED                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m341 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/cuda/amp/\u001b[0m\u001b[1;33mgrad_scaler.py\u001b[0m:\u001b[94m285\u001b[0m in \u001b[92m_maybe_opt_step\u001b[0m      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m282 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_maybe_opt_step\u001b[0m(\u001b[96mself\u001b[0m, optimizer, optimizer_state, *args, **kwargs):                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m283 \u001b[0m\u001b[2m│   │   \u001b[0mretval = \u001b[94mNone\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m284 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96msum\u001b[0m(v.item() \u001b[94mfor\u001b[0m v \u001b[95min\u001b[0m optimizer_state[\u001b[33m\"\u001b[0m\u001b[33mfound_inf_per_device\u001b[0m\u001b[33m\"\u001b[0m].values()):    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m285 \u001b[2m│   │   │   \u001b[0mretval = optimizer.step(*args, **kwargs)                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m286 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m retval                                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m287 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m288 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mstep\u001b[0m(\u001b[96mself\u001b[0m, optimizer, *args, **kwargs):                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33mlr_scheduler.py\u001b[0m:\u001b[94m65\u001b[0m in \u001b[92mwrapper\u001b[0m                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  62 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minstance = instance_ref()                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  63 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minstance._step_count += \u001b[94m1\u001b[0m                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  64 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mwrapped = func.\u001b[92m__get__\u001b[0m(instance, \u001b[96mcls\u001b[0m)                                     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m  65 \u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapped(*args, **kwargs)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  66 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  67 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# Note that the returned function here is no longer a bound method,\u001b[0m           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m  68 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# so attributes like `__func__` and `__self__` no longer exist.\u001b[0m               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/torch/optim/\u001b[0m\u001b[1;33moptimizer.py\u001b[0m:\u001b[94m113\u001b[0m in \u001b[92mwrapper\u001b[0m                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m110 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mobj, *_ = args                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mprofile_name = \u001b[33m\"\u001b[0m\u001b[33mOptimizer.step#\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m.step\u001b[0m\u001b[33m\"\u001b[0m.format(obj.\u001b[91m__class__\u001b[0m.\u001b[91m__name__\u001b[0m)     \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.autograd.profiler.record_function(profile_name):                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m113 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m func(*args, **kwargs)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m114 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m wrapper                                                                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   │   \u001b[0mhooked = \u001b[96mgetattr\u001b[0m(\u001b[96mself\u001b[0m.\u001b[91m__class__\u001b[0m.step, \u001b[33m\"\u001b[0m\u001b[33mhooked\u001b[0m\u001b[33m\"\u001b[0m, \u001b[94mNone\u001b[0m)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.9/site-packages/transformers/\u001b[0m\u001b[1;33moptimization.py\u001b[0m:\u001b[94m434\u001b[0m in \u001b[92mstep\u001b[0m                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m431 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mlen\u001b[0m(state) == \u001b[94m0\u001b[0m:                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m432 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstate[\u001b[33m\"\u001b[0m\u001b[33mstep\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[94m0\u001b[0m                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m433 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Exponential moving average of gradient values\u001b[0m                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m434 \u001b[2m│   │   │   │   │   \u001b[0mstate[\u001b[33m\"\u001b[0m\u001b[33mexp_avg\u001b[0m\u001b[33m\"\u001b[0m] = torch.zeros_like(p.data)                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m435 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[2m# Exponential moving average of squared gradient values\u001b[0m                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m436 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mstate[\u001b[33m\"\u001b[0m\u001b[33mexp_avg_sq\u001b[0m\u001b[33m\"\u001b[0m] = torch.zeros_like(p.data)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m437 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mRuntimeError: \u001b[0mCUDA out of memory. Tried to allocate \u001b[1;36m58.00\u001b[0m MiB \u001b[1m(\u001b[0mGPU \u001b[1;36m0\u001b[0m; \u001b[1;36m14.76\u001b[0m GiB total capacity; \u001b[1;36m13.19\u001b[0m GiB already \n",
       "allocated; \u001b[1;36m23.75\u001b[0m MiB free; \u001b[1;36m13.92\u001b[0m GiB reserved in total by PyTorch\u001b[1m)\u001b[0m If reserved memory is >> allocated memory try \n",
       "setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and \n",
       "PYTORCH_CUDA_ALLOC_CONF\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained(f'KoChatGPT/{model_name_short}_output_1_SFT_aug')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae94235",
   "metadata": {},
   "source": [
    "- 'skt/ko-gpt-trinity-1.2B-v0.5' 모델은 메모리 부족으로 쓰기 어려움 \n",
    "- model_max_length=16, batch_size=1 까지 줄여보았으나 안됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59d650",
   "metadata": {},
   "source": [
    "### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b1b0fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline('text-generation', model=model_path, tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a36f4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a57387d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 인공지능 어시스턴트이기 때문에 실제로 고기를 먹을 수는 없습니다. 하지만 일반적으로 불고기는 한국 음식 중 가장 인기 있는 메뉴 중 하나입니다. \"불고기\"가 어떤 의미인지 알려주시면 더 정확한 답변을 드릴 수 있습니다. \"불고기\"가 어떤 의미를 가지는지\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 39대 부통령직을 수행했습니다. \"리처드 닉슨\"은 1952년 대선에서 공화당 후보로 출마한 존 매케인에게 패배하면서 부통령직을 맡았습니다. \"매케인\"은 닉슨 대통령의 경력에서 중요한 역할을 한 인물 중 하나였습니다. \"프리랜서\"는\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 일리노이 주 시카고에 위치해 있습니다.\\n\\n미국 일리노이 주 시카고는 시카고 시내에서 가장 큰 규모의 국제공항 중 하나입니다. : \"시카고오 헤어 국제공항\" (Chicago Huawan) \n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이므로 미세먼지 여부를 판단할 수 없습니다. 하지만 보통 미세먼지는 호흡기 질환, 심혈관계 질환, 호흡기 질환 등 다양한 원인에 의해 발생할 수 있습니다. 따라서 보건당국이 미세먼지 농도를 측정하고 예보하는 것이 좋습니다. 또한, 미세먼지 농도는 실내 공기 질을\n"
     ]
    }
   ],
   "source": [
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5b6f3d",
   "metadata": {},
   "source": [
    "#### 결과 - 추가 데이터셋:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e34291e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'죄송합니다, 저는 인공지능 어시스턴트이기 때문에 고기를 판매하는 가게나 상점에 대한 정보를 알 수 없습니다. 해당 가게나 상점의 공식 홈페이지나 전화번호를 확인하시는 것이 좋을 것 같습니다. 추가적인 정보를 제공해주시면 더 정확한 답변을 드릴 수 있을 것입니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):1961년. 1962년 리처드 닉슨은 45대 부통령을 지냈다. 그는 재임 중 제1차 세계 대전 참전을 위한 평화유지군을 창설하는 데 기여했다. 1969년부터 1972년까지 미국 육군 최고사령부(OKH) 부사령관을 역임했다. 1976년에는 미합중국의회(FOR) 부의장으로 선출됐\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤이 국제공항은 미국 시카고에 위치해 있습니다. 미국의 주요 공항 중 하나입니다. 이 공항은 항공편에서도 매우 중요한 역할을 합니다. 이 공항에 있는 대부분의 항공편도 시카고에서 출발합니다. 이 공항의 직원들은 이 공항을 이용하실 수 있습니다. 이 공항에서는 여러 가지 이벤\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이기 때문에 미세먼지 상황을 파악할 수 없습니다. 하지만, 미세먼지가 건강에 미치는 영향에 대해서는 여러 가지 가능성이 있을 것으로 생각됩니다. 예를 들어, 호흡기 질환이나 심질환 등 건강에 좋지 않은 영향을 미칠 수 있습니다. 또한, 미세먼지는 우리 몸의 면역계를 손상시킬 수도 있습니다. 따라서, 건강한 생활\n"
     ]
    }
   ],
   "source": [
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76097b70",
   "metadata": {},
   "source": [
    "### RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "021ef07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81b9c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62fc5055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained=model_name, lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00bedc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "\n",
    "for tmp in list_data_dict:\n",
    "    rankings = tmp['ranking']\n",
    "    completions = [tmp[f'completion_{i}'] for i in range(3)]\n",
    "\n",
    "    for i in range(3):\n",
    "        data = {\n",
    "            'prompt': tmp['prompt'],\n",
    "            'chosen': completions[i],\n",
    "            'rejected': completions[(i + 1) % 3]\n",
    "        }\n",
    "        total_data_ranking2chosen.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e7c5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_data_ranking2chosen = []\n",
    "\n",
    "for tmp in list_data_dict:\n",
    "    prompt = tmp['prompt']\n",
    "    ranking = tmp['ranking']\n",
    "\n",
    "    for index in range(1, len(ranking)):\n",
    "        n = ranking[0]\n",
    "        m = ranking[index]\n",
    "\n",
    "\n",
    "        data = {\n",
    "            'prompt': prompt,\n",
    "            'chosen': tmp['completion_{}'.format(n)],\n",
    "            'rejected': tmp['completion_{}'.format(m)]\n",
    "        }\n",
    "\n",
    "        total_data_ranking2chosen.append(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "adfcb475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1384.39it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1323.85it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, model_max_length)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b98d1d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(\n",
    "    model=model,\n",
    "    strategy=NaiveStrategy(),\n",
    "    optim=Adam(model.parameters(), lr=5e-5),\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    batch_size=2,\n",
    "    max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c49e5267",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/500 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/500 [00:00<03:38,  2.29it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/500 [00:00<03:38,  2.29it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 2/500 [00:00<03:09,  2.63it/s, loss=0.685]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 2/500 [00:00<03:09,  2.63it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/500 [00:01<02:56,  2.82it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/500 [00:01<02:56,  2.82it/s, loss=0.109]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 4/500 [00:01<02:50,  2.90it/s, loss=0.109]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 4/500 [00:01<02:50,  2.90it/s, loss=0.381]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 5/500 [00:01<02:47,  2.96it/s, loss=0.381]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 5/500 [00:01<02:47,  2.96it/s, loss=0.971]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 6/500 [00:02<02:44,  3.00it/s, loss=0.971]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 6/500 [00:02<02:44,  3.00it/s, loss=1.08] \u001b[A\n",
      "Train step of epoch 0:   1%|▏         | 7/500 [00:02<02:43,  3.02it/s, loss=1.08]\u001b[A\n",
      "Train step of epoch 0:   1%|▏         | 7/500 [00:02<02:43,  3.02it/s, loss=0.254]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 8/500 [00:02<02:42,  3.03it/s, loss=0.254]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 8/500 [00:02<02:42,  3.03it/s, loss=1.17] \u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 9/500 [00:03<02:40,  3.05it/s, loss=1.17]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 9/500 [00:03<02:40,  3.05it/s, loss=3.07]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 10/500 [00:03<02:41,  3.04it/s, loss=3.07]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 10/500 [00:03<02:41,  3.04it/s, loss=0.00283]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 11/500 [00:03<02:40,  3.05it/s, loss=0.00283]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 11/500 [00:03<02:40,  3.05it/s, loss=0.879]  \u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 12/500 [00:04<02:40,  3.05it/s, loss=0.879]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 12/500 [00:04<02:40,  3.05it/s, loss=0.228]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 13/500 [00:04<02:40,  3.04it/s, loss=0.228]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 13/500 [00:04<02:40,  3.04it/s, loss=1.19] \u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 14/500 [00:04<02:39,  3.04it/s, loss=1.19]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 14/500 [00:04<02:39,  3.04it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 15/500 [00:05<02:38,  3.05it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 15/500 [00:05<02:38,  3.05it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 16/500 [00:05<02:40,  3.01it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 16/500 [00:05<02:40,  3.01it/s, loss=0.264]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 17/500 [00:05<02:40,  3.00it/s, loss=0.264]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 17/500 [00:05<02:40,  3.00it/s, loss=1.38] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 18/500 [00:06<02:41,  2.99it/s, loss=1.38]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 18/500 [00:06<02:41,  2.99it/s, loss=0.236]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 19/500 [00:06<02:40,  2.99it/s, loss=0.236]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 19/500 [00:06<02:40,  2.99it/s, loss=0.33] \u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 20/500 [00:06<02:38,  3.02it/s, loss=0.33]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 20/500 [00:06<02:38,  3.02it/s, loss=0.291]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 21/500 [00:07<02:38,  3.03it/s, loss=0.291]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 21/500 [00:07<02:38,  3.03it/s, loss=0.0199]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 22/500 [00:07<02:37,  3.03it/s, loss=0.0199]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 22/500 [00:07<02:37,  3.03it/s, loss=0.128] \u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 23/500 [00:07<02:37,  3.04it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 23/500 [00:07<02:37,  3.04it/s, loss=0.21] \u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 24/500 [00:08<02:36,  3.05it/s, loss=0.21]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 24/500 [00:08<02:36,  3.05it/s, loss=0.21]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 25/500 [00:08<02:37,  3.02it/s, loss=0.21]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 25/500 [00:08<02:37,  3.02it/s, loss=0.0705]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 26/500 [00:08<02:36,  3.03it/s, loss=0.0705]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 26/500 [00:08<02:36,  3.03it/s, loss=0.294] \u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 27/500 [00:09<02:36,  3.02it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 27/500 [00:09<02:36,  3.02it/s, loss=0.00179]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 28/500 [00:09<02:36,  3.02it/s, loss=0.00179]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 28/500 [00:09<02:36,  3.02it/s, loss=6.16e-5]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 29/500 [00:09<02:36,  3.01it/s, loss=6.16e-5]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 29/500 [00:09<02:36,  3.01it/s, loss=0.17]   \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 30/500 [00:10<02:35,  3.03it/s, loss=0.17]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 30/500 [00:10<02:35,  3.03it/s, loss=0.0193]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 31/500 [00:10<02:35,  3.02it/s, loss=0.0193]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 31/500 [00:10<02:35,  3.02it/s, loss=0.473] \u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 32/500 [00:10<02:35,  3.00it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 32/500 [00:10<02:35,  3.00it/s, loss=0.697]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 33/500 [00:11<02:34,  3.02it/s, loss=0.697]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 33/500 [00:11<02:34,  3.02it/s, loss=0.428]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 34/500 [00:11<02:35,  3.00it/s, loss=0.428]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 34/500 [00:11<02:35,  3.00it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 35/500 [00:11<02:34,  3.00it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 35/500 [00:11<02:34,  3.00it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 36/500 [00:12<02:33,  3.01it/s, loss=0.402]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 36/500 [00:12<02:33,  3.01it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 37/500 [00:12<02:33,  3.02it/s, loss=0.508]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 37/500 [00:12<02:33,  3.02it/s, loss=0.000378]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 38/500 [00:12<02:33,  3.02it/s, loss=0.000378]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 38/500 [00:12<02:33,  3.02it/s, loss=0.307]   \u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 39/500 [00:12<02:32,  3.02it/s, loss=0.307]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 39/500 [00:12<02:32,  3.02it/s, loss=1.54] \u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 40/500 [00:13<02:32,  3.02it/s, loss=1.54]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 40/500 [00:13<02:32,  3.02it/s, loss=0.328]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 41/500 [00:13<02:31,  3.03it/s, loss=0.328]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 41/500 [00:13<02:31,  3.03it/s, loss=1.11] \u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 42/500 [00:13<02:31,  3.02it/s, loss=1.11]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 42/500 [00:13<02:31,  3.02it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:   9%|▊         | 43/500 [00:14<02:30,  3.04it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:   9%|▊         | 43/500 [00:14<02:30,  3.04it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 44/500 [00:14<02:30,  3.03it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 44/500 [00:14<02:30,  3.03it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 45/500 [00:14<02:29,  3.04it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 45/500 [00:14<02:29,  3.04it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 46/500 [00:15<02:30,  3.01it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 46/500 [00:15<02:30,  3.01it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 47/500 [00:15<02:30,  3.01it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 47/500 [00:15<02:30,  3.01it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 48/500 [00:15<02:29,  3.03it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 48/500 [00:15<02:29,  3.03it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 49/500 [00:16<02:28,  3.03it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 49/500 [00:16<02:28,  3.03it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 50/500 [00:16<02:28,  3.03it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 50/500 [00:16<02:28,  3.03it/s, loss=0.199]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 51/500 [00:16<02:27,  3.04it/s, loss=0.199]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 51/500 [00:16<02:27,  3.04it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 52/500 [00:17<02:27,  3.03it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 52/500 [00:17<02:27,  3.03it/s, loss=0.344]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 53/500 [00:17<02:27,  3.03it/s, loss=0.344]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 53/500 [00:17<02:27,  3.03it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 54/500 [00:17<02:26,  3.04it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 54/500 [00:17<02:26,  3.04it/s, loss=0.276]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 55/500 [00:18<02:26,  3.03it/s, loss=0.276]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 55/500 [00:18<02:26,  3.03it/s, loss=0.376]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 56/500 [00:18<02:26,  3.03it/s, loss=0.376]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 56/500 [00:18<02:26,  3.03it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  11%|█▏        | 57/500 [00:18<02:25,  3.04it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 0:  11%|█▏        | 57/500 [00:18<02:25,  3.04it/s, loss=0.334]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 58/500 [00:19<02:25,  3.03it/s, loss=0.334]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 58/500 [00:19<02:25,  3.03it/s, loss=0.252]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 59/500 [00:19<02:24,  3.05it/s, loss=0.252]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 59/500 [00:19<02:24,  3.05it/s, loss=0.0152]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 60/500 [00:19<02:24,  3.05it/s, loss=0.0152]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 60/500 [00:19<02:24,  3.05it/s, loss=0.528] \u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 61/500 [00:20<02:24,  3.04it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 61/500 [00:20<02:24,  3.04it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 62/500 [00:20<02:24,  3.03it/s, loss=0.566]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 62/500 [00:20<02:24,  3.03it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 63/500 [00:20<02:24,  3.03it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 63/500 [00:20<02:24,  3.03it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 64/500 [00:21<02:24,  3.03it/s, loss=0.333]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 64/500 [00:21<02:24,  3.03it/s, loss=0.541]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 65/500 [00:21<02:23,  3.02it/s, loss=0.541]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 65/500 [00:21<02:23,  3.02it/s, loss=0.000668]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 66/500 [00:21<02:24,  3.01it/s, loss=0.000668]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 66/500 [00:21<02:24,  3.01it/s, loss=0.2]     \u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 67/500 [00:22<02:24,  2.99it/s, loss=0.2]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 67/500 [00:22<02:24,  2.99it/s, loss=0.0269]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 68/500 [00:22<02:24,  2.99it/s, loss=0.0269]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 68/500 [00:22<02:24,  2.99it/s, loss=0.307] \u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 69/500 [00:22<02:23,  2.99it/s, loss=0.307]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 69/500 [00:22<02:23,  2.99it/s, loss=0.0463]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 70/500 [00:23<02:23,  3.00it/s, loss=0.0463]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 70/500 [00:23<02:23,  3.00it/s, loss=0.0144]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 71/500 [00:23<02:23,  2.99it/s, loss=0.0144]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 71/500 [00:23<02:23,  2.99it/s, loss=0.0907]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 72/500 [00:23<02:22,  3.00it/s, loss=0.0907]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 72/500 [00:23<02:22,  3.00it/s, loss=0.000167]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 73/500 [00:24<02:22,  3.00it/s, loss=0.000167]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 73/500 [00:24<02:22,  3.00it/s, loss=0.0179]  \u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 74/500 [00:24<02:21,  3.01it/s, loss=0.0179]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 74/500 [00:24<02:21,  3.01it/s, loss=0.775] \u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 75/500 [00:24<02:19,  3.05it/s, loss=0.775]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 75/500 [00:24<02:19,  3.05it/s, loss=-]    \u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 76/500 [00:25<02:20,  3.02it/s, loss=-]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 76/500 [00:25<02:20,  3.02it/s, loss=1.56e-5]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 77/500 [00:25<02:22,  2.97it/s, loss=1.56e-5]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 77/500 [00:25<02:22,  2.97it/s, loss=0.644]  \u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 78/500 [00:25<02:21,  2.99it/s, loss=0.644]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 78/500 [00:25<02:21,  2.99it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 79/500 [00:26<02:20,  2.99it/s, loss=0.528]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 79/500 [00:26<02:20,  2.99it/s, loss=0.134]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 80/500 [00:26<02:19,  3.02it/s, loss=0.134]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 80/500 [00:26<02:19,  3.02it/s, loss=0.0839]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 81/500 [00:26<02:18,  3.02it/s, loss=0.0839]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 81/500 [00:26<02:18,  3.02it/s, loss=0.501] \u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 82/500 [00:27<02:17,  3.04it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 82/500 [00:27<02:17,  3.04it/s, loss=-]    \u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 83/500 [00:27<02:17,  3.03it/s, loss=-]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 83/500 [00:27<02:17,  3.03it/s, loss=0.265]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 84/500 [00:27<02:17,  3.02it/s, loss=0.265]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 84/500 [00:27<02:17,  3.02it/s, loss=0.0238]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 85/500 [00:28<02:17,  3.01it/s, loss=0.0238]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 85/500 [00:28<02:17,  3.01it/s, loss=0.0628]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 86/500 [00:28<02:16,  3.04it/s, loss=0.0628]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 86/500 [00:28<02:16,  3.04it/s, loss=0.189] \u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 87/500 [00:28<02:16,  3.02it/s, loss=0.189]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 87/500 [00:28<02:16,  3.02it/s, loss=0.253]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 88/500 [00:29<02:16,  3.01it/s, loss=0.253]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 88/500 [00:29<02:16,  3.01it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 89/500 [00:29<02:16,  3.02it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 89/500 [00:29<02:16,  3.02it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 90/500 [00:29<02:15,  3.02it/s, loss=0.209]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 90/500 [00:29<02:15,  3.02it/s, loss=0.00248]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 91/500 [00:30<02:15,  3.01it/s, loss=0.00248]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 91/500 [00:30<02:15,  3.01it/s, loss=0.462]  \u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 92/500 [00:30<02:15,  3.02it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 92/500 [00:30<02:15,  3.02it/s, loss=0.284]\u001b[A\n",
      "Train step of epoch 0:  19%|█▊        | 93/500 [00:30<02:15,  3.00it/s, loss=0.284]\u001b[A\n",
      "Train step of epoch 0:  19%|█▊        | 93/500 [00:30<02:15,  3.00it/s, loss=0.283]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 94/500 [00:31<02:15,  3.00it/s, loss=0.283]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 94/500 [00:31<02:15,  3.00it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 95/500 [00:31<02:14,  3.02it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 95/500 [00:31<02:14,  3.02it/s, loss=1.79e-7]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 96/500 [00:31<02:14,  3.01it/s, loss=1.79e-7]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 96/500 [00:31<02:14,  3.01it/s, loss=0.0683] \u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 97/500 [00:32<02:14,  3.00it/s, loss=0.0683]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 97/500 [00:32<02:14,  3.00it/s, loss=0.00209]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 98/500 [00:32<02:14,  2.99it/s, loss=0.00209]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 98/500 [00:32<02:14,  2.99it/s, loss=0.00275]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 99/500 [00:32<02:13,  3.01it/s, loss=0.00275]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 99/500 [00:32<02:13,  3.01it/s, loss=0.196]  \u001b[A\n",
      "Train step of epoch 0:  20%|██        | 100/500 [00:33<02:13,  3.00it/s, loss=0.196]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 100/500 [00:33<02:13,  3.00it/s, loss=0.00542]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 101/500 [00:33<02:13,  2.99it/s, loss=0.00542]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 101/500 [00:33<02:13,  2.99it/s, loss=9.15e-5]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 102/500 [00:33<02:12,  3.01it/s, loss=9.15e-5]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 102/500 [00:33<02:12,  3.01it/s, loss=0.00113]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 103/500 [00:34<02:11,  3.01it/s, loss=0.00113]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 103/500 [00:34<02:11,  3.01it/s, loss=0.0462] \u001b[A\n",
      "Train step of epoch 0:  21%|██        | 104/500 [00:34<02:11,  3.01it/s, loss=0.0462]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 104/500 [00:34<02:11,  3.01it/s, loss=0.0282]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 105/500 [00:34<02:10,  3.02it/s, loss=0.0282]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 105/500 [00:34<02:10,  3.02it/s, loss=-]     \u001b[A\n",
      "Train step of epoch 0:  21%|██        | 106/500 [00:35<02:11,  2.99it/s, loss=-]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 106/500 [00:35<02:11,  2.99it/s, loss=0.0132]\u001b[A\n",
      "Train step of epoch 0:  21%|██▏       | 107/500 [00:35<02:12,  2.96it/s, loss=0.0132]\u001b[A\n",
      "Train step of epoch 0:  21%|██▏       | 107/500 [00:35<02:12,  2.96it/s, loss=2.41]  \u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 108/500 [00:35<02:12,  2.97it/s, loss=2.41]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 108/500 [00:35<02:12,  2.97it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 109/500 [00:36<02:12,  2.95it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 109/500 [00:36<02:12,  2.95it/s, loss=0.000647]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 110/500 [00:36<02:11,  2.96it/s, loss=0.000647]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 110/500 [00:36<02:11,  2.96it/s, loss=0.0619]  \u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 111/500 [00:36<02:11,  2.97it/s, loss=0.0619]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 111/500 [00:36<02:11,  2.97it/s, loss=0.455] \u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 112/500 [00:37<02:10,  2.97it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 112/500 [00:37<02:10,  2.97it/s, loss=0.0443]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 113/500 [00:37<02:10,  2.97it/s, loss=0.0443]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 113/500 [00:37<02:10,  2.97it/s, loss=0.336] \u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 114/500 [00:37<02:09,  2.98it/s, loss=0.336]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 114/500 [00:37<02:09,  2.98it/s, loss=0.00719]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 115/500 [00:38<02:09,  2.97it/s, loss=0.00719]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 115/500 [00:38<02:09,  2.97it/s, loss=0.219]  \u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 116/500 [00:38<02:09,  2.97it/s, loss=0.219]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 116/500 [00:38<02:09,  2.97it/s, loss=0.787]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 117/500 [00:38<02:08,  2.99it/s, loss=0.787]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 117/500 [00:38<02:08,  2.99it/s, loss=0.267]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 118/500 [00:39<02:08,  2.97it/s, loss=0.267]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 118/500 [00:39<02:08,  2.97it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 119/500 [00:39<02:07,  2.98it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 119/500 [00:39<02:07,  2.98it/s, loss=0.278]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 120/500 [00:39<02:07,  2.98it/s, loss=0.278]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 120/500 [00:39<02:07,  2.98it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 121/500 [00:40<02:07,  2.98it/s, loss=0.269]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 121/500 [00:40<02:07,  2.98it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 122/500 [00:40<02:06,  2.99it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 122/500 [00:40<02:06,  2.99it/s, loss=0.116]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 123/500 [00:40<02:06,  2.99it/s, loss=0.116]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 123/500 [00:40<02:06,  2.99it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 124/500 [00:41<02:05,  2.99it/s, loss=0.547]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 124/500 [00:41<02:05,  2.99it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 125/500 [00:41<02:05,  2.98it/s, loss=0.379]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 125/500 [00:41<02:05,  2.98it/s, loss=0.198]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 126/500 [00:41<02:05,  2.98it/s, loss=0.198]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 126/500 [00:41<02:05,  2.98it/s, loss=0.00737]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 127/500 [00:42<02:04,  2.99it/s, loss=0.00737]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 127/500 [00:42<02:04,  2.99it/s, loss=0.525]  \u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 128/500 [00:42<02:04,  2.98it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 128/500 [00:42<02:04,  2.98it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 129/500 [00:42<02:04,  2.99it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 129/500 [00:42<02:04,  2.99it/s, loss=0.254]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 130/500 [00:43<02:03,  2.99it/s, loss=0.254]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 130/500 [00:43<02:03,  2.99it/s, loss=0.433]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 131/500 [00:43<02:03,  2.98it/s, loss=0.433]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 131/500 [00:43<02:03,  2.98it/s, loss=0.0129]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 132/500 [00:43<02:03,  2.99it/s, loss=0.0129]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 132/500 [00:43<02:03,  2.99it/s, loss=0.122] \u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 133/500 [00:44<02:02,  3.00it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 133/500 [00:44<02:02,  3.00it/s, loss=0.309]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 134/500 [00:44<02:02,  2.99it/s, loss=0.309]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 134/500 [00:44<02:02,  2.99it/s, loss=6.97e-5]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 135/500 [00:44<02:01,  2.99it/s, loss=6.97e-5]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 135/500 [00:44<02:01,  2.99it/s, loss=0.0349] \u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 136/500 [00:45<02:02,  2.97it/s, loss=0.0349]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 136/500 [00:45<02:02,  2.97it/s, loss=0.000385]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 137/500 [00:45<02:02,  2.96it/s, loss=0.000385]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 137/500 [00:45<02:02,  2.96it/s, loss=0.541]   \u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 138/500 [00:45<02:01,  2.98it/s, loss=0.541]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 138/500 [00:45<02:01,  2.98it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 139/500 [00:46<02:01,  2.97it/s, loss=0.696]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 139/500 [00:46<02:01,  2.97it/s, loss=0.0275]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 140/500 [00:46<02:00,  2.98it/s, loss=0.0275]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 140/500 [00:46<02:00,  2.98it/s, loss=0.242] \u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 141/500 [00:46<02:00,  2.99it/s, loss=0.242]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 141/500 [00:46<02:00,  2.99it/s, loss=0.0307]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 142/500 [00:47<01:59,  2.98it/s, loss=0.0307]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 142/500 [00:47<01:59,  2.98it/s, loss=0.00642]\u001b[A\n",
      "Train step of epoch 0:  29%|██▊       | 143/500 [00:47<01:59,  2.98it/s, loss=0.00642]\u001b[A\n",
      "Train step of epoch 0:  29%|██▊       | 143/500 [00:47<01:59,  2.98it/s, loss=0.534]  \u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 144/500 [00:47<01:59,  2.97it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 144/500 [00:47<01:59,  2.97it/s, loss=4.49e-5]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 145/500 [00:48<02:00,  2.96it/s, loss=4.49e-5]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 145/500 [00:48<02:00,  2.96it/s, loss=0.0219] \u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 146/500 [00:48<01:59,  2.95it/s, loss=0.0219]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 146/500 [00:48<01:59,  2.95it/s, loss=0.0264]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 147/500 [00:48<01:58,  2.97it/s, loss=0.0264]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 147/500 [00:48<01:58,  2.97it/s, loss=0.374] \u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 148/500 [00:49<01:59,  2.96it/s, loss=0.374]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 148/500 [00:49<01:59,  2.96it/s, loss=0.00603]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 149/500 [00:49<01:58,  2.96it/s, loss=0.00603]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 149/500 [00:49<01:58,  2.96it/s, loss=0.000883]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 150/500 [00:49<01:57,  2.97it/s, loss=0.000883]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 150/500 [00:49<01:57,  2.97it/s, loss=0.11]    \u001b[A\n",
      "Train step of epoch 0:  30%|███       | 151/500 [00:50<01:57,  2.97it/s, loss=0.11]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 151/500 [00:50<01:57,  2.97it/s, loss=0.0118]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 152/500 [00:50<01:58,  2.95it/s, loss=0.0118]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 152/500 [00:50<01:58,  2.95it/s, loss=0.000183]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 153/500 [00:51<01:57,  2.96it/s, loss=0.000183]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 153/500 [00:51<01:57,  2.96it/s, loss=0.694]   \u001b[A\n",
      "Train step of epoch 0:  31%|███       | 154/500 [00:51<01:57,  2.95it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 154/500 [00:51<01:57,  2.95it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 0:  31%|███       | 155/500 [00:51<01:56,  2.96it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 155/500 [00:51<01:56,  2.96it/s, loss=0.509]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 156/500 [00:52<01:56,  2.95it/s, loss=0.509]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 156/500 [00:52<01:56,  2.95it/s, loss=1.04] \u001b[A\n",
      "Train step of epoch 0:  31%|███▏      | 157/500 [00:52<01:56,  2.94it/s, loss=1.04]\u001b[A\n",
      "Train step of epoch 0:  31%|███▏      | 157/500 [00:52<01:56,  2.94it/s, loss=0.0498]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 158/500 [00:52<01:55,  2.95it/s, loss=0.0498]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 158/500 [00:52<01:55,  2.95it/s, loss=0.0448]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 159/500 [00:53<01:55,  2.96it/s, loss=0.0448]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 159/500 [00:53<01:55,  2.96it/s, loss=0.0072]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 160/500 [00:53<01:55,  2.95it/s, loss=0.0072]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 160/500 [00:53<01:55,  2.95it/s, loss=0.358] \u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 161/500 [00:53<01:54,  2.96it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 161/500 [00:53<01:54,  2.96it/s, loss=0.52] \u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 162/500 [00:54<01:54,  2.96it/s, loss=0.52]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 162/500 [00:54<01:54,  2.96it/s, loss=0.0583]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 163/500 [00:54<01:54,  2.94it/s, loss=0.0583]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 163/500 [00:54<01:54,  2.94it/s, loss=0.971] \u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 164/500 [00:54<01:54,  2.93it/s, loss=0.971]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 164/500 [00:54<01:54,  2.93it/s, loss=0.000669]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 165/500 [00:55<01:53,  2.94it/s, loss=0.000669]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 165/500 [00:55<01:53,  2.94it/s, loss=0.0202]  \u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 166/500 [00:55<01:55,  2.90it/s, loss=0.0202]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 166/500 [00:55<01:55,  2.90it/s, loss=0.067] \u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 167/500 [00:55<01:54,  2.90it/s, loss=0.067]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 167/500 [00:55<01:54,  2.90it/s, loss=0.0759]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 168/500 [00:56<01:54,  2.91it/s, loss=0.0759]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 168/500 [00:56<01:54,  2.91it/s, loss=0.0493]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 169/500 [00:56<01:53,  2.93it/s, loss=0.0493]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 169/500 [00:56<01:53,  2.93it/s, loss=0.196] \u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 170/500 [00:56<01:52,  2.93it/s, loss=0.196]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 170/500 [00:56<01:52,  2.93it/s, loss=0.187]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 171/500 [00:57<01:51,  2.94it/s, loss=0.187]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 171/500 [00:57<01:51,  2.94it/s, loss=0.307]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 172/500 [00:57<01:51,  2.95it/s, loss=0.307]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 172/500 [00:57<01:51,  2.95it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 173/500 [00:57<01:50,  2.95it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 173/500 [00:57<01:50,  2.95it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 174/500 [00:58<01:50,  2.94it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 174/500 [00:58<01:50,  2.94it/s, loss=0.000963]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 175/500 [00:58<01:50,  2.94it/s, loss=0.000963]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 175/500 [00:58<01:50,  2.94it/s, loss=0.662]   \u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 176/500 [00:58<01:50,  2.95it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 176/500 [00:58<01:50,  2.95it/s, loss=0.0974]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 177/500 [00:59<01:49,  2.94it/s, loss=0.0974]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 177/500 [00:59<01:49,  2.94it/s, loss=0.133] \u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 178/500 [00:59<01:49,  2.93it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 178/500 [00:59<01:49,  2.93it/s, loss=0.0834]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 179/500 [00:59<01:49,  2.94it/s, loss=0.0834]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 179/500 [00:59<01:49,  2.94it/s, loss=0.599] \u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 180/500 [01:00<01:48,  2.95it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 180/500 [01:00<01:48,  2.95it/s, loss=2.15e-6]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 181/500 [01:00<01:47,  2.96it/s, loss=2.15e-6]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 181/500 [01:00<01:47,  2.96it/s, loss=0.512]  \u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 182/500 [01:00<01:47,  2.96it/s, loss=0.512]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 182/500 [01:00<01:47,  2.96it/s, loss=0.000388]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 183/500 [01:01<01:47,  2.96it/s, loss=0.000388]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 183/500 [01:01<01:47,  2.96it/s, loss=0.0107]  \u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 184/500 [01:01<01:46,  2.97it/s, loss=0.0107]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 184/500 [01:01<01:46,  2.97it/s, loss=0.0236]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 185/500 [01:01<01:46,  2.96it/s, loss=0.0236]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 185/500 [01:01<01:46,  2.96it/s, loss=0.177] \u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 186/500 [01:02<01:46,  2.95it/s, loss=0.177]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 186/500 [01:02<01:46,  2.95it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 187/500 [01:02<01:45,  2.96it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 187/500 [01:02<01:45,  2.96it/s, loss=0.0121]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 188/500 [01:02<01:45,  2.96it/s, loss=0.0121]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 188/500 [01:02<01:45,  2.96it/s, loss=1.27]  \u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 189/500 [01:03<01:45,  2.95it/s, loss=1.27]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 189/500 [01:03<01:45,  2.95it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 190/500 [01:03<01:44,  2.96it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 190/500 [01:03<01:44,  2.96it/s, loss=0.0384]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 191/500 [01:03<01:44,  2.96it/s, loss=0.0384]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 191/500 [01:03<01:44,  2.96it/s, loss=0.392] \u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 192/500 [01:04<01:43,  2.97it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 192/500 [01:04<01:43,  2.97it/s, loss=0.241]\u001b[A\n",
      "Train step of epoch 0:  39%|███▊      | 193/500 [01:04<01:43,  2.97it/s, loss=0.241]\u001b[A\n",
      "Train step of epoch 0:  39%|███▊      | 193/500 [01:04<01:43,  2.97it/s, loss=0.0582]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 194/500 [01:04<01:42,  2.97it/s, loss=0.0582]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 194/500 [01:04<01:42,  2.97it/s, loss=0.00722]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 195/500 [01:05<01:43,  2.95it/s, loss=0.00722]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 195/500 [01:05<01:43,  2.95it/s, loss=2.5e-6] \u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 196/500 [01:05<01:44,  2.92it/s, loss=2.5e-6]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 196/500 [01:05<01:44,  2.92it/s, loss=0.044] \u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 197/500 [01:05<01:43,  2.93it/s, loss=0.044]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 197/500 [01:05<01:43,  2.93it/s, loss=2.35e-5]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 198/500 [01:06<01:42,  2.94it/s, loss=2.35e-5]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 198/500 [01:06<01:42,  2.94it/s, loss=0.0298] \u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 199/500 [01:06<01:42,  2.95it/s, loss=0.0298]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 199/500 [01:06<01:42,  2.95it/s, loss=0.000786]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 200/500 [01:06<01:41,  2.96it/s, loss=0.000786]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 200/500 [01:06<01:41,  2.96it/s, loss=0.0169]  \u001b[A\n",
      "Train step of epoch 0:  40%|████      | 201/500 [01:07<01:41,  2.95it/s, loss=0.0169]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 201/500 [01:07<01:41,  2.95it/s, loss=0.705] \u001b[A\n",
      "Train step of epoch 0:  40%|████      | 202/500 [01:07<01:41,  2.95it/s, loss=0.705]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 202/500 [01:07<01:41,  2.95it/s, loss=1.42] \u001b[A\n",
      "Train step of epoch 0:  41%|████      | 203/500 [01:07<01:40,  2.95it/s, loss=1.42]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 203/500 [01:07<01:40,  2.95it/s, loss=2.62e-6]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 204/500 [01:08<01:40,  2.96it/s, loss=2.62e-6]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 204/500 [01:08<01:40,  2.96it/s, loss=1.56e-5]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 205/500 [01:08<01:39,  2.96it/s, loss=1.56e-5]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 205/500 [01:08<01:39,  2.96it/s, loss=1.2]    \u001b[A\n",
      "Train step of epoch 0:  41%|████      | 206/500 [01:08<01:38,  2.97it/s, loss=1.2]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 206/500 [01:08<01:38,  2.97it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  41%|████▏     | 207/500 [01:09<01:39,  2.95it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  41%|████▏     | 207/500 [01:09<01:39,  2.95it/s, loss=0.271]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 208/500 [01:09<01:38,  2.95it/s, loss=0.271]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 208/500 [01:09<01:38,  2.95it/s, loss=0.0517]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 209/500 [01:10<01:38,  2.97it/s, loss=0.0517]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 209/500 [01:10<01:38,  2.97it/s, loss=0.501] \u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 210/500 [01:10<01:37,  2.96it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 210/500 [01:10<01:37,  2.96it/s, loss=0.00776]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 211/500 [01:10<01:37,  2.97it/s, loss=0.00776]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 211/500 [01:10<01:37,  2.97it/s, loss=0.426]  \u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 212/500 [01:11<01:36,  2.97it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 212/500 [01:11<01:36,  2.97it/s, loss=0.056]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 213/500 [01:11<01:36,  2.96it/s, loss=0.056]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 213/500 [01:11<01:36,  2.96it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 214/500 [01:11<01:36,  2.97it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 214/500 [01:11<01:36,  2.97it/s, loss=0.321]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 215/500 [01:12<01:35,  2.97it/s, loss=0.321]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 215/500 [01:12<01:35,  2.97it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 216/500 [01:12<01:36,  2.96it/s, loss=0.222]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 216/500 [01:12<01:36,  2.96it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 217/500 [01:12<01:35,  2.96it/s, loss=0.486]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 217/500 [01:12<01:35,  2.96it/s, loss=0.00437]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 218/500 [01:13<01:35,  2.95it/s, loss=0.00437]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 218/500 [01:13<01:35,  2.95it/s, loss=0.329]  \u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 219/500 [01:13<01:35,  2.94it/s, loss=0.329]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 219/500 [01:13<01:35,  2.94it/s, loss=0.104]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 220/500 [01:13<01:34,  2.95it/s, loss=0.104]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 220/500 [01:13<01:34,  2.95it/s, loss=0.186]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 221/500 [01:14<01:33,  2.97it/s, loss=0.186]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 221/500 [01:14<01:33,  2.97it/s, loss=0.0438]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 222/500 [01:14<01:33,  2.99it/s, loss=0.0438]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 222/500 [01:14<01:33,  2.99it/s, loss=0.000313]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 223/500 [01:14<01:32,  2.98it/s, loss=0.000313]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 223/500 [01:14<01:32,  2.98it/s, loss=0.275]   \u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 224/500 [01:15<01:32,  2.97it/s, loss=0.275]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 224/500 [01:15<01:32,  2.97it/s, loss=0.135]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 225/500 [01:15<01:33,  2.93it/s, loss=0.135]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 225/500 [01:15<01:33,  2.93it/s, loss=0.147]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 226/500 [01:15<01:33,  2.93it/s, loss=0.147]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 226/500 [01:15<01:33,  2.93it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 227/500 [01:16<01:32,  2.96it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 227/500 [01:16<01:32,  2.96it/s, loss=1.3e-5]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 228/500 [01:16<01:31,  2.96it/s, loss=1.3e-5]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 228/500 [01:16<01:31,  2.96it/s, loss=0.186] \u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 229/500 [01:16<01:31,  2.98it/s, loss=0.186]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 229/500 [01:16<01:31,  2.98it/s, loss=0.0539]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 230/500 [01:17<01:30,  2.98it/s, loss=0.0539]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 230/500 [01:17<01:30,  2.98it/s, loss=0.338] \u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 231/500 [01:17<01:30,  2.97it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 231/500 [01:17<01:30,  2.97it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 232/500 [01:17<01:30,  2.98it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 232/500 [01:17<01:30,  2.98it/s, loss=0.191]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 233/500 [01:18<01:29,  2.98it/s, loss=0.191]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 233/500 [01:18<01:29,  2.98it/s, loss=0.261]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 234/500 [01:18<01:29,  2.96it/s, loss=0.261]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 234/500 [01:18<01:29,  2.96it/s, loss=0.00834]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 235/500 [01:18<01:29,  2.96it/s, loss=0.00834]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 235/500 [01:18<01:29,  2.96it/s, loss=1.73e-6]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 236/500 [01:19<01:29,  2.96it/s, loss=1.73e-6]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 236/500 [01:19<01:29,  2.96it/s, loss=0.00393]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 237/500 [01:19<01:29,  2.95it/s, loss=0.00393]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 237/500 [01:19<01:29,  2.95it/s, loss=0.142]  \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 238/500 [01:19<01:28,  2.95it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 238/500 [01:19<01:28,  2.95it/s, loss=0.00293]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 239/500 [01:20<01:28,  2.96it/s, loss=0.00293]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 239/500 [01:20<01:28,  2.96it/s, loss=0.000805]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 240/500 [01:20<01:27,  2.97it/s, loss=0.000805]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 240/500 [01:20<01:27,  2.97it/s, loss=0.248]   \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 241/500 [01:20<01:27,  2.94it/s, loss=0.248]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 241/500 [01:20<01:27,  2.94it/s, loss=0.00102]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 242/500 [01:21<01:28,  2.93it/s, loss=0.00102]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 242/500 [01:21<01:28,  2.93it/s, loss=0.703]  \u001b[A\n",
      "Train step of epoch 0:  49%|████▊     | 243/500 [01:21<01:27,  2.93it/s, loss=0.703]\u001b[A\n",
      "Train step of epoch 0:  49%|████▊     | 243/500 [01:21<01:27,  2.93it/s, loss=0.0402]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 244/500 [01:21<01:27,  2.94it/s, loss=0.0402]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 244/500 [01:21<01:27,  2.94it/s, loss=0.00266]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 245/500 [01:22<01:26,  2.94it/s, loss=0.00266]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 245/500 [01:22<01:26,  2.94it/s, loss=0.166]  \u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 246/500 [01:22<01:25,  2.95it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 246/500 [01:22<01:25,  2.95it/s, loss=1.61e-6]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 247/500 [01:22<01:25,  2.97it/s, loss=1.61e-6]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 247/500 [01:22<01:25,  2.97it/s, loss=0.00528]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 248/500 [01:23<01:24,  2.98it/s, loss=0.00528]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 248/500 [01:23<01:24,  2.98it/s, loss=0.0193] \u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 249/500 [01:23<01:23,  2.99it/s, loss=0.0193]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 249/500 [01:23<01:23,  2.99it/s, loss=0.978] \u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 250/500 [01:23<01:23,  3.00it/s, loss=0.978]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 250/500 [01:23<01:23,  3.00it/s, loss=0.0101]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 251/500 [01:24<01:23,  3.00it/s, loss=0.0101]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 251/500 [01:24<01:23,  3.00it/s, loss=1.23e-5]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 252/500 [01:24<01:22,  3.00it/s, loss=1.23e-5]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 252/500 [01:24<01:22,  3.00it/s, loss=3.46e-6]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 253/500 [01:24<01:22,  3.01it/s, loss=3.46e-6]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 253/500 [01:24<01:22,  3.01it/s, loss=0.000878]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 254/500 [01:25<01:22,  2.99it/s, loss=0.000878]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 254/500 [01:25<01:22,  2.99it/s, loss=5.13e-6] \u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 255/500 [01:25<01:23,  2.94it/s, loss=5.13e-6]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 255/500 [01:25<01:23,  2.94it/s, loss=2.44e-6]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 256/500 [01:25<01:22,  2.95it/s, loss=2.44e-6]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 256/500 [01:25<01:22,  2.95it/s, loss=3.65e-5]\u001b[A\n",
      "Train step of epoch 0:  51%|█████▏    | 257/500 [01:26<01:21,  2.98it/s, loss=3.65e-5]\u001b[A\n",
      "Train step of epoch 0:  51%|█████▏    | 257/500 [01:26<01:21,  2.98it/s, loss=0.0186] \u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 258/500 [01:26<01:20,  2.99it/s, loss=0.0186]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 258/500 [01:26<01:20,  2.99it/s, loss=0.0078]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 259/500 [01:26<01:20,  2.98it/s, loss=0.0078]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 259/500 [01:26<01:20,  2.98it/s, loss=0.00202]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 260/500 [01:27<01:20,  2.98it/s, loss=0.00202]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 260/500 [01:27<01:20,  2.98it/s, loss=0.281]  \u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 261/500 [01:27<01:20,  2.98it/s, loss=0.281]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 261/500 [01:27<01:20,  2.98it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 262/500 [01:27<01:19,  2.98it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 262/500 [01:27<01:19,  2.98it/s, loss=6.63e-5]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 263/500 [01:28<01:19,  2.97it/s, loss=6.63e-5]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 263/500 [01:28<01:19,  2.97it/s, loss=0.00103]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 264/500 [01:28<01:19,  2.96it/s, loss=0.00103]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 264/500 [01:28<01:19,  2.96it/s, loss=0.0332] \u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 265/500 [01:28<01:19,  2.97it/s, loss=0.0332]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 265/500 [01:28<01:19,  2.97it/s, loss=1.07e-6]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 266/500 [01:29<01:18,  2.97it/s, loss=1.07e-6]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 266/500 [01:29<01:18,  2.97it/s, loss=4.6e-5] \u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 267/500 [01:29<01:18,  2.97it/s, loss=4.6e-5]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 267/500 [01:29<01:18,  2.97it/s, loss=0.208] \u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 268/500 [01:29<01:18,  2.97it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 268/500 [01:29<01:18,  2.97it/s, loss=0.000102]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 269/500 [01:30<01:17,  2.97it/s, loss=0.000102]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 269/500 [01:30<01:17,  2.97it/s, loss=0.692]   \u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 270/500 [01:30<01:17,  2.98it/s, loss=0.692]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 270/500 [01:30<01:17,  2.98it/s, loss=0.147]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 271/500 [01:30<01:16,  2.98it/s, loss=0.147]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 271/500 [01:30<01:16,  2.98it/s, loss=0.00149]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 272/500 [01:31<01:16,  2.98it/s, loss=0.00149]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 272/500 [01:31<01:16,  2.98it/s, loss=5.68e-5]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 273/500 [01:31<01:15,  2.99it/s, loss=5.68e-5]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 273/500 [01:31<01:15,  2.99it/s, loss=1.28e-5]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 274/500 [01:31<01:15,  2.99it/s, loss=1.28e-5]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 274/500 [01:31<01:15,  2.99it/s, loss=0.000273]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 275/500 [01:32<01:15,  2.99it/s, loss=0.000273]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 275/500 [01:32<01:15,  2.99it/s, loss=8.15]    \u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 276/500 [01:32<01:14,  3.00it/s, loss=8.15]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 276/500 [01:32<01:14,  3.00it/s, loss=0.00755]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 277/500 [01:32<01:14,  2.99it/s, loss=0.00755]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 277/500 [01:32<01:14,  2.99it/s, loss=0.608]  \u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 278/500 [01:33<01:14,  2.99it/s, loss=0.608]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 278/500 [01:33<01:14,  2.99it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 279/500 [01:33<01:14,  2.99it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 279/500 [01:33<01:14,  2.99it/s, loss=0.278]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 280/500 [01:33<01:13,  2.99it/s, loss=0.278]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 280/500 [01:33<01:13,  2.99it/s, loss=0.00252]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 281/500 [01:34<01:13,  2.99it/s, loss=0.00252]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 281/500 [01:34<01:13,  2.99it/s, loss=0.291]  \u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 282/500 [01:34<01:12,  2.99it/s, loss=0.291]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 282/500 [01:34<01:12,  2.99it/s, loss=0.0413]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 283/500 [01:34<01:12,  2.98it/s, loss=0.0413]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 283/500 [01:34<01:12,  2.98it/s, loss=0.612] \u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 284/500 [01:35<01:13,  2.96it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 284/500 [01:35<01:13,  2.96it/s, loss=0.319]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 285/500 [01:35<01:13,  2.92it/s, loss=0.319]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 285/500 [01:35<01:13,  2.92it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 286/500 [01:35<01:12,  2.94it/s, loss=0.395]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 286/500 [01:35<01:12,  2.94it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 287/500 [01:36<01:12,  2.95it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 287/500 [01:36<01:12,  2.95it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 288/500 [01:36<01:11,  2.97it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 288/500 [01:36<01:11,  2.97it/s, loss=0.776]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 289/500 [01:36<01:11,  2.96it/s, loss=0.776]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 289/500 [01:36<01:11,  2.96it/s, loss=0.0425]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 290/500 [01:37<01:10,  2.96it/s, loss=0.0425]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 290/500 [01:37<01:10,  2.96it/s, loss=0.306] \u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 291/500 [01:37<01:10,  2.96it/s, loss=0.306]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 291/500 [01:37<01:10,  2.96it/s, loss=0.0122]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 292/500 [01:37<01:10,  2.96it/s, loss=0.0122]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 292/500 [01:37<01:10,  2.96it/s, loss=0.533] \u001b[A\n",
      "Train step of epoch 0:  59%|█████▊    | 293/500 [01:38<01:09,  2.97it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▊    | 293/500 [01:38<01:09,  2.97it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 294/500 [01:38<01:09,  2.98it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 294/500 [01:38<01:09,  2.98it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 295/500 [01:38<01:08,  2.99it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 295/500 [01:38<01:08,  2.99it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 296/500 [01:39<01:08,  2.99it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 296/500 [01:39<01:08,  2.99it/s, loss=0.165]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 297/500 [01:39<01:08,  2.98it/s, loss=0.165]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 297/500 [01:39<01:08,  2.98it/s, loss=0.00151]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 298/500 [01:39<01:07,  2.99it/s, loss=0.00151]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 298/500 [01:39<01:07,  2.99it/s, loss=0.606]  \u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 299/500 [01:40<01:07,  2.99it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 299/500 [01:40<01:07,  2.99it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 300/500 [01:40<01:06,  3.00it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 300/500 [01:40<01:06,  3.00it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 301/500 [01:40<01:06,  3.00it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 301/500 [01:40<01:06,  3.00it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 302/500 [01:41<01:06,  2.99it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 302/500 [01:41<01:06,  2.99it/s, loss=0.38] \u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 303/500 [01:41<01:05,  3.00it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 303/500 [01:41<01:05,  3.00it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 304/500 [01:41<01:05,  3.00it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 304/500 [01:41<01:05,  3.00it/s, loss=0.00453]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 305/500 [01:42<01:04,  3.00it/s, loss=0.00453]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 305/500 [01:42<01:04,  3.00it/s, loss=0.000572]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 306/500 [01:42<01:04,  3.00it/s, loss=0.000572]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 306/500 [01:42<01:04,  3.00it/s, loss=0.000744]\u001b[A\n",
      "Train step of epoch 0:  61%|██████▏   | 307/500 [01:42<01:04,  3.01it/s, loss=0.000744]\u001b[A\n",
      "Train step of epoch 0:  61%|██████▏   | 307/500 [01:42<01:04,  3.01it/s, loss=0.503]   \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 308/500 [01:43<01:04,  3.00it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 308/500 [01:43<01:04,  3.00it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 309/500 [01:43<01:03,  2.99it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 309/500 [01:43<01:03,  2.99it/s, loss=0.324]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 310/500 [01:43<01:03,  3.00it/s, loss=0.324]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 310/500 [01:43<01:03,  3.00it/s, loss=0.418]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 311/500 [01:44<01:03,  2.99it/s, loss=0.418]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 311/500 [01:44<01:03,  2.99it/s, loss=0.115]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 312/500 [01:44<01:02,  2.99it/s, loss=0.115]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 312/500 [01:44<01:02,  2.99it/s, loss=0.36] \u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 313/500 [01:44<01:02,  3.00it/s, loss=0.36]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 313/500 [01:44<01:02,  3.00it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 314/500 [01:45<01:02,  2.98it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 314/500 [01:45<01:02,  2.98it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 315/500 [01:45<01:02,  2.97it/s, loss=0.232]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 315/500 [01:45<01:02,  2.97it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 316/500 [01:45<01:01,  2.98it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 316/500 [01:45<01:01,  2.98it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 317/500 [01:46<01:01,  2.99it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 317/500 [01:46<01:01,  2.99it/s, loss=0.126]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 318/500 [01:46<01:00,  3.00it/s, loss=0.126]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 318/500 [01:46<01:00,  3.00it/s, loss=0.141]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 319/500 [01:46<01:00,  3.00it/s, loss=0.141]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 319/500 [01:46<01:00,  3.00it/s, loss=0.227]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 320/500 [01:47<00:59,  3.00it/s, loss=0.227]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 320/500 [01:47<00:59,  3.00it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 321/500 [01:47<00:59,  3.00it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 321/500 [01:47<00:59,  3.00it/s, loss=0.267]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 322/500 [01:47<00:59,  3.00it/s, loss=0.267]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 322/500 [01:47<00:59,  3.00it/s, loss=0.514]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 323/500 [01:48<00:59,  2.99it/s, loss=0.514]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 323/500 [01:48<00:59,  2.99it/s, loss=0.0755]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 324/500 [01:48<00:58,  2.99it/s, loss=0.0755]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 324/500 [01:48<00:58,  2.99it/s, loss=0.00282]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 325/500 [01:48<00:58,  2.99it/s, loss=0.00282]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 325/500 [01:48<00:58,  2.99it/s, loss=0.504]  \u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 326/500 [01:49<00:58,  3.00it/s, loss=0.504]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 326/500 [01:49<00:58,  3.00it/s, loss=0.000478]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 327/500 [01:49<00:57,  2.99it/s, loss=0.000478]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 327/500 [01:49<00:57,  2.99it/s, loss=0.0413]  \u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 328/500 [01:49<00:57,  2.99it/s, loss=0.0413]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 328/500 [01:49<00:57,  2.99it/s, loss=0.00727]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 329/500 [01:50<00:57,  2.99it/s, loss=0.00727]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 329/500 [01:50<00:57,  2.99it/s, loss=1.07]   \u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 330/500 [01:50<00:57,  2.94it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 330/500 [01:50<00:57,  2.94it/s, loss=0.0473]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 331/500 [01:51<00:57,  2.96it/s, loss=0.0473]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 331/500 [01:51<00:57,  2.96it/s, loss=0.000422]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 332/500 [01:51<00:56,  2.96it/s, loss=0.000422]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 332/500 [01:51<00:56,  2.96it/s, loss=0.0249]  \u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 333/500 [01:51<00:56,  2.96it/s, loss=0.0249]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 333/500 [01:51<00:56,  2.96it/s, loss=0.00265]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 334/500 [01:52<00:56,  2.95it/s, loss=0.00265]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 334/500 [01:52<00:56,  2.95it/s, loss=0.401]  \u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 335/500 [01:52<00:55,  2.95it/s, loss=0.401]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 335/500 [01:52<00:55,  2.95it/s, loss=0.0433]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 336/500 [01:52<00:55,  2.94it/s, loss=0.0433]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 336/500 [01:52<00:55,  2.94it/s, loss=0.000336]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 337/500 [01:53<00:55,  2.96it/s, loss=0.000336]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 337/500 [01:53<00:55,  2.96it/s, loss=0.411]   \u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 338/500 [01:53<00:54,  2.95it/s, loss=0.411]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 338/500 [01:53<00:54,  2.95it/s, loss=0.000448]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 339/500 [01:53<00:54,  2.96it/s, loss=0.000448]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 339/500 [01:53<00:54,  2.96it/s, loss=0.18]    \u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 340/500 [01:54<00:53,  2.97it/s, loss=0.18]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 340/500 [01:54<00:53,  2.97it/s, loss=0.0368]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 341/500 [01:54<00:53,  2.96it/s, loss=0.0368]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 341/500 [01:54<00:53,  2.96it/s, loss=0.0209]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 342/500 [01:54<00:53,  2.97it/s, loss=0.0209]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 342/500 [01:54<00:53,  2.97it/s, loss=0.816] \u001b[A\n",
      "Train step of epoch 0:  69%|██████▊   | 343/500 [01:55<00:52,  2.98it/s, loss=0.816]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▊   | 343/500 [01:55<00:52,  2.98it/s, loss=0.0284]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 344/500 [01:55<00:53,  2.94it/s, loss=0.0284]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 344/500 [01:55<00:53,  2.94it/s, loss=0.0195]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 345/500 [01:55<00:52,  2.93it/s, loss=0.0195]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 345/500 [01:55<00:52,  2.93it/s, loss=0.0458]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 346/500 [01:56<00:52,  2.96it/s, loss=0.0458]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 346/500 [01:56<00:52,  2.96it/s, loss=0.000115]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 347/500 [01:56<00:51,  2.95it/s, loss=0.000115]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 347/500 [01:56<00:51,  2.95it/s, loss=0.000705]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 348/500 [01:56<00:51,  2.97it/s, loss=0.000705]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 348/500 [01:56<00:51,  2.97it/s, loss=0.514]   \u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 349/500 [01:57<00:50,  2.97it/s, loss=0.514]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 349/500 [01:57<00:50,  2.97it/s, loss=0.011]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 350/500 [01:57<00:50,  2.97it/s, loss=0.011]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 350/500 [01:57<00:50,  2.97it/s, loss=0.118]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 351/500 [01:57<00:50,  2.98it/s, loss=0.118]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 351/500 [01:57<00:50,  2.98it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 352/500 [01:58<00:49,  2.98it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 352/500 [01:58<00:49,  2.98it/s, loss=0.00101]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 353/500 [01:58<00:49,  2.97it/s, loss=0.00101]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 353/500 [01:58<00:49,  2.97it/s, loss=0.0247] \u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 354/500 [01:58<00:49,  2.98it/s, loss=0.0247]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 354/500 [01:58<00:49,  2.98it/s, loss=1.36]  \u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 355/500 [01:59<00:48,  2.97it/s, loss=1.36]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 355/500 [01:59<00:48,  2.97it/s, loss=0.872]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 356/500 [01:59<00:48,  2.99it/s, loss=0.872]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 356/500 [01:59<00:48,  2.99it/s, loss=0.406]\u001b[A\n",
      "Train step of epoch 0:  71%|███████▏  | 357/500 [01:59<00:48,  2.97it/s, loss=0.406]\u001b[A\n",
      "Train step of epoch 0:  71%|███████▏  | 357/500 [01:59<00:48,  2.97it/s, loss=0.00611]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 358/500 [02:00<00:47,  2.97it/s, loss=0.00611]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 358/500 [02:00<00:47,  2.97it/s, loss=0.215]  \u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 359/500 [02:00<00:47,  2.97it/s, loss=0.215]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 359/500 [02:00<00:47,  2.97it/s, loss=0.189]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 360/500 [02:00<00:47,  2.97it/s, loss=0.189]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 360/500 [02:00<00:47,  2.97it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 361/500 [02:01<00:46,  2.97it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 361/500 [02:01<00:46,  2.97it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 362/500 [02:01<00:46,  2.98it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 362/500 [02:01<00:46,  2.98it/s, loss=0.343]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 363/500 [02:01<00:45,  2.98it/s, loss=0.343]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 363/500 [02:01<00:45,  2.98it/s, loss=0.336]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 364/500 [02:02<00:45,  2.97it/s, loss=0.336]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 364/500 [02:02<00:45,  2.97it/s, loss=0.0115]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 365/500 [02:02<00:45,  2.98it/s, loss=0.0115]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 365/500 [02:02<00:45,  2.98it/s, loss=8.06e-5]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 366/500 [02:02<00:44,  2.98it/s, loss=8.06e-5]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 366/500 [02:02<00:44,  2.98it/s, loss=0.659]  \u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 367/500 [02:03<00:44,  2.97it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 367/500 [02:03<00:44,  2.97it/s, loss=8.71e-5]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 368/500 [02:03<00:44,  2.98it/s, loss=8.71e-5]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 368/500 [02:03<00:44,  2.98it/s, loss=0.106]  \u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 369/500 [02:03<00:43,  2.99it/s, loss=0.106]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 369/500 [02:03<00:43,  2.99it/s, loss=0.0783]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 370/500 [02:04<00:43,  2.97it/s, loss=0.0783]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 370/500 [02:04<00:43,  2.97it/s, loss=0.627] \u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 371/500 [02:04<00:43,  2.98it/s, loss=0.627]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 371/500 [02:04<00:43,  2.98it/s, loss=0.152]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 372/500 [02:04<00:42,  2.98it/s, loss=0.152]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 372/500 [02:04<00:42,  2.98it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 373/500 [02:05<00:42,  2.96it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 373/500 [02:05<00:42,  2.96it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 374/500 [02:05<00:42,  2.94it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 374/500 [02:05<00:42,  2.94it/s, loss=0.129]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 375/500 [02:05<00:42,  2.94it/s, loss=0.129]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 375/500 [02:05<00:42,  2.94it/s, loss=0.0315]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 376/500 [02:06<00:42,  2.95it/s, loss=0.0315]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 376/500 [02:06<00:42,  2.95it/s, loss=0.133] \u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 377/500 [02:06<00:41,  2.96it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 377/500 [02:06<00:41,  2.96it/s, loss=0.00395]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 378/500 [02:06<00:41,  2.97it/s, loss=0.00395]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 378/500 [02:06<00:41,  2.97it/s, loss=0.663]  \u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 379/500 [02:07<00:40,  2.96it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 379/500 [02:07<00:40,  2.96it/s, loss=0.000409]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 380/500 [02:07<00:40,  2.96it/s, loss=0.000409]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 380/500 [02:07<00:40,  2.96it/s, loss=0.294]   \u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 381/500 [02:07<00:40,  2.95it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 381/500 [02:07<00:40,  2.95it/s, loss=0.164]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 382/500 [02:08<00:40,  2.94it/s, loss=0.164]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 382/500 [02:08<00:40,  2.94it/s, loss=0.0456]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 383/500 [02:08<00:39,  2.95it/s, loss=0.0456]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 383/500 [02:08<00:39,  2.95it/s, loss=0.00177]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 384/500 [02:08<00:39,  2.96it/s, loss=0.00177]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 384/500 [02:08<00:39,  2.96it/s, loss=0.455]  \u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 385/500 [02:09<00:38,  2.97it/s, loss=0.455]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 385/500 [02:09<00:38,  2.97it/s, loss=0.0895]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 386/500 [02:09<00:38,  2.97it/s, loss=0.0895]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 386/500 [02:09<00:38,  2.97it/s, loss=0.0679]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 387/500 [02:09<00:38,  2.97it/s, loss=0.0679]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 387/500 [02:09<00:38,  2.97it/s, loss=0.0928]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 388/500 [02:10<00:37,  2.97it/s, loss=0.0928]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 388/500 [02:10<00:37,  2.97it/s, loss=0.0316]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 389/500 [02:10<00:37,  2.97it/s, loss=0.0316]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 389/500 [02:10<00:37,  2.97it/s, loss=0.286] \u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 390/500 [02:10<00:37,  2.95it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 390/500 [02:10<00:37,  2.95it/s, loss=0.00739]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 391/500 [02:11<00:36,  2.95it/s, loss=0.00739]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 391/500 [02:11<00:36,  2.95it/s, loss=0.25]   \u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 392/500 [02:11<00:36,  2.96it/s, loss=0.25]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 392/500 [02:11<00:36,  2.96it/s, loss=2.15]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▊  | 393/500 [02:11<00:36,  2.96it/s, loss=2.15]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▊  | 393/500 [02:11<00:36,  2.96it/s, loss=0.0379]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 394/500 [02:12<00:35,  2.96it/s, loss=0.0379]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 394/500 [02:12<00:35,  2.96it/s, loss=0.208] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 395/500 [02:12<00:35,  2.96it/s, loss=0.208]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 395/500 [02:12<00:35,  2.96it/s, loss=0.000449]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 396/500 [02:12<00:35,  2.96it/s, loss=0.000449]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 396/500 [02:12<00:35,  2.96it/s, loss=0.194]   \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 397/500 [02:13<00:34,  2.97it/s, loss=0.194]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 397/500 [02:13<00:34,  2.97it/s, loss=0.233]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 398/500 [02:13<00:34,  2.98it/s, loss=0.233]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 398/500 [02:13<00:34,  2.98it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 399/500 [02:13<00:34,  2.97it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 399/500 [02:13<00:34,  2.97it/s, loss=0.0477]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 400/500 [02:14<00:33,  2.98it/s, loss=0.0477]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 400/500 [02:14<00:33,  2.98it/s, loss=0.254] \u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 401/500 [02:14<00:33,  2.99it/s, loss=0.254]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 401/500 [02:14<00:33,  2.99it/s, loss=0.0431]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 402/500 [02:14<00:32,  2.98it/s, loss=0.0431]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 402/500 [02:14<00:32,  2.98it/s, loss=2.72]  \u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 403/500 [02:15<00:32,  2.95it/s, loss=2.72]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 403/500 [02:15<00:32,  2.95it/s, loss=0.11]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 404/500 [02:15<00:32,  2.93it/s, loss=0.11]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 404/500 [02:15<00:32,  2.93it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 405/500 [02:15<00:32,  2.94it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 405/500 [02:15<00:32,  2.94it/s, loss=0.0195]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 406/500 [02:16<00:31,  2.95it/s, loss=0.0195]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 406/500 [02:16<00:31,  2.95it/s, loss=0.546] \u001b[A\n",
      "Train step of epoch 0:  81%|████████▏ | 407/500 [02:16<00:31,  2.96it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 0:  81%|████████▏ | 407/500 [02:16<00:31,  2.96it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 408/500 [02:16<00:31,  2.97it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 408/500 [02:16<00:31,  2.97it/s, loss=0.0509]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 409/500 [02:17<00:30,  2.97it/s, loss=0.0509]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 409/500 [02:17<00:30,  2.97it/s, loss=0.018] \u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 410/500 [02:17<00:30,  2.97it/s, loss=0.018]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 410/500 [02:17<00:30,  2.97it/s, loss=0.479]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 411/500 [02:17<00:29,  2.98it/s, loss=0.479]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 411/500 [02:17<00:29,  2.98it/s, loss=0.085]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 412/500 [02:18<00:29,  2.97it/s, loss=0.085]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 412/500 [02:18<00:29,  2.97it/s, loss=0.000905]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 413/500 [02:18<00:29,  2.97it/s, loss=0.000905]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 413/500 [02:18<00:29,  2.97it/s, loss=0.128]   \u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 414/500 [02:18<00:28,  2.98it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 414/500 [02:18<00:28,  2.98it/s, loss=0.997]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 415/500 [02:19<00:28,  2.98it/s, loss=0.997]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 415/500 [02:19<00:28,  2.98it/s, loss=1.46] \u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 416/500 [02:19<00:28,  2.98it/s, loss=1.46]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 416/500 [02:19<00:28,  2.98it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 417/500 [02:20<00:28,  2.96it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 417/500 [02:20<00:28,  2.96it/s, loss=0.0315]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 418/500 [02:20<00:27,  2.95it/s, loss=0.0315]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 418/500 [02:20<00:27,  2.95it/s, loss=0.106] \u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 419/500 [02:20<00:27,  2.95it/s, loss=0.106]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 419/500 [02:20<00:27,  2.95it/s, loss=0.105]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 420/500 [02:21<00:27,  2.96it/s, loss=0.105]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 420/500 [02:21<00:27,  2.96it/s, loss=0.00577]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 421/500 [02:21<00:26,  2.95it/s, loss=0.00577]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 421/500 [02:21<00:26,  2.95it/s, loss=0.522]  \u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 422/500 [02:21<00:26,  2.96it/s, loss=0.522]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 422/500 [02:21<00:26,  2.96it/s, loss=0.00245]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 423/500 [02:22<00:25,  2.96it/s, loss=0.00245]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 423/500 [02:22<00:25,  2.96it/s, loss=0.00253]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 424/500 [02:22<00:25,  2.95it/s, loss=0.00253]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 424/500 [02:22<00:25,  2.95it/s, loss=0.00191]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 425/500 [02:22<00:25,  2.96it/s, loss=0.00191]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 425/500 [02:22<00:25,  2.96it/s, loss=0.143]  \u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 426/500 [02:23<00:25,  2.96it/s, loss=0.143]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 426/500 [02:23<00:25,  2.96it/s, loss=0.302]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 427/500 [02:23<00:24,  2.94it/s, loss=0.302]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 427/500 [02:23<00:24,  2.94it/s, loss=0.178]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 428/500 [02:23<00:24,  2.95it/s, loss=0.178]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 428/500 [02:23<00:24,  2.95it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 429/500 [02:24<00:24,  2.95it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 429/500 [02:24<00:24,  2.95it/s, loss=0.283]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 430/500 [02:24<00:23,  2.95it/s, loss=0.283]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 430/500 [02:24<00:23,  2.95it/s, loss=0.136]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 431/500 [02:24<00:23,  2.97it/s, loss=0.136]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 431/500 [02:24<00:23,  2.97it/s, loss=0.388]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 432/500 [02:25<00:22,  2.96it/s, loss=0.388]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 432/500 [02:25<00:22,  2.96it/s, loss=0.00232]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 433/500 [02:25<00:23,  2.89it/s, loss=0.00232]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 433/500 [02:25<00:23,  2.89it/s, loss=0.346]  \u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 434/500 [02:25<00:22,  2.90it/s, loss=0.346]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 434/500 [02:25<00:22,  2.90it/s, loss=0.187]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 435/500 [02:26<00:22,  2.90it/s, loss=0.187]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 435/500 [02:26<00:22,  2.90it/s, loss=0.000886]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 436/500 [02:26<00:21,  2.92it/s, loss=0.000886]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 436/500 [02:26<00:21,  2.92it/s, loss=0.142]   \u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 437/500 [02:26<00:21,  2.93it/s, loss=0.142]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 437/500 [02:26<00:21,  2.93it/s, loss=0.0632]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 438/500 [02:27<00:21,  2.94it/s, loss=0.0632]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 438/500 [02:27<00:21,  2.94it/s, loss=0.069] \u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 439/500 [02:27<00:20,  2.96it/s, loss=0.069]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 439/500 [02:27<00:20,  2.96it/s, loss=0.0135]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 440/500 [02:27<00:20,  2.95it/s, loss=0.0135]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 440/500 [02:27<00:20,  2.95it/s, loss=0.0317]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 441/500 [02:28<00:20,  2.95it/s, loss=0.0317]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 441/500 [02:28<00:20,  2.95it/s, loss=0.0283]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 442/500 [02:28<00:19,  2.95it/s, loss=0.0283]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 442/500 [02:28<00:19,  2.95it/s, loss=0.00778]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▊ | 443/500 [02:28<00:19,  2.96it/s, loss=0.00778]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▊ | 443/500 [02:28<00:19,  2.96it/s, loss=0.0713] \u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 444/500 [02:29<00:18,  2.97it/s, loss=0.0713]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 444/500 [02:29<00:18,  2.97it/s, loss=0.0238]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 445/500 [02:29<00:18,  2.97it/s, loss=0.0238]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 445/500 [02:29<00:18,  2.97it/s, loss=0.0839]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 446/500 [02:29<00:18,  2.97it/s, loss=0.0839]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 446/500 [02:29<00:18,  2.97it/s, loss=0.00481]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 447/500 [02:30<00:17,  2.97it/s, loss=0.00481]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 447/500 [02:30<00:17,  2.97it/s, loss=0.117]  \u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 448/500 [02:30<00:17,  2.98it/s, loss=0.117]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 448/500 [02:30<00:17,  2.98it/s, loss=0.00789]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 449/500 [02:30<00:17,  2.97it/s, loss=0.00789]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 449/500 [02:30<00:17,  2.97it/s, loss=0.15]   \u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 450/500 [02:31<00:16,  2.96it/s, loss=0.15]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 450/500 [02:31<00:16,  2.96it/s, loss=1.65e-5]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 451/500 [02:31<00:16,  2.96it/s, loss=1.65e-5]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 451/500 [02:31<00:16,  2.96it/s, loss=0.0565] \u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 452/500 [02:31<00:16,  2.97it/s, loss=0.0565]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 452/500 [02:31<00:16,  2.97it/s, loss=0.00499]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 453/500 [02:32<00:15,  2.96it/s, loss=0.00499]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 453/500 [02:32<00:15,  2.96it/s, loss=0.0065] \u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 454/500 [02:32<00:15,  2.97it/s, loss=0.0065]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 454/500 [02:32<00:15,  2.97it/s, loss=0.261] \u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 455/500 [02:32<00:15,  2.98it/s, loss=0.261]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 455/500 [02:32<00:15,  2.98it/s, loss=5.1e-5]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 456/500 [02:33<00:14,  2.97it/s, loss=5.1e-5]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 456/500 [02:33<00:14,  2.97it/s, loss=0.0165]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████▏| 457/500 [02:33<00:14,  2.97it/s, loss=0.0165]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████▏| 457/500 [02:33<00:14,  2.97it/s, loss=0.662] \u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 458/500 [02:33<00:14,  2.97it/s, loss=0.662]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 458/500 [02:33<00:14,  2.97it/s, loss=2.43e-5]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 459/500 [02:34<00:13,  2.97it/s, loss=2.43e-5]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 459/500 [02:34<00:13,  2.97it/s, loss=0.194]  \u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 460/500 [02:34<00:13,  2.99it/s, loss=0.194]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 460/500 [02:34<00:13,  2.99it/s, loss=0.000218]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 461/500 [02:34<00:13,  2.99it/s, loss=0.000218]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 461/500 [02:34<00:13,  2.99it/s, loss=1.97]    \u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 462/500 [02:35<00:12,  2.96it/s, loss=1.97]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 462/500 [02:35<00:12,  2.96it/s, loss=6.21e-5]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 463/500 [02:35<00:12,  2.92it/s, loss=6.21e-5]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 463/500 [02:35<00:12,  2.92it/s, loss=0.631]  \u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 464/500 [02:35<00:12,  2.94it/s, loss=0.631]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 464/500 [02:35<00:12,  2.94it/s, loss=0.00304]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 465/500 [02:36<00:11,  2.96it/s, loss=0.00304]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 465/500 [02:36<00:11,  2.96it/s, loss=0.26]   \u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 466/500 [02:36<00:11,  2.97it/s, loss=0.26]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 466/500 [02:36<00:11,  2.97it/s, loss=0.0321]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 467/500 [02:36<00:11,  2.97it/s, loss=0.0321]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 467/500 [02:36<00:11,  2.97it/s, loss=0.465] \u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 468/500 [02:37<00:10,  2.97it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 468/500 [02:37<00:10,  2.97it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 469/500 [02:37<00:10,  2.96it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 469/500 [02:37<00:10,  2.96it/s, loss=0.52] \u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 470/500 [02:37<00:10,  2.96it/s, loss=0.52]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 470/500 [02:37<00:10,  2.96it/s, loss=0.00226]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 471/500 [02:38<00:09,  2.97it/s, loss=0.00226]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 471/500 [02:38<00:09,  2.97it/s, loss=0.225]  \u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 472/500 [02:38<00:09,  2.97it/s, loss=0.225]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 472/500 [02:38<00:09,  2.97it/s, loss=0.000331]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 473/500 [02:38<00:09,  2.98it/s, loss=0.000331]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 473/500 [02:38<00:09,  2.98it/s, loss=0.118]   \u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 474/500 [02:39<00:08,  2.99it/s, loss=0.118]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 474/500 [02:39<00:08,  2.99it/s, loss=0.0535]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 475/500 [02:39<00:08,  2.98it/s, loss=0.0535]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 475/500 [02:39<00:08,  2.98it/s, loss=0.00158]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 476/500 [02:39<00:08,  2.98it/s, loss=0.00158]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 476/500 [02:39<00:08,  2.98it/s, loss=4.59e-5]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 477/500 [02:40<00:07,  2.98it/s, loss=4.59e-5]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 477/500 [02:40<00:07,  2.98it/s, loss=0.0557] \u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 478/500 [02:40<00:07,  2.97it/s, loss=0.0557]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 478/500 [02:40<00:07,  2.97it/s, loss=0.287] \u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 479/500 [02:40<00:07,  2.98it/s, loss=0.287]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 479/500 [02:40<00:07,  2.98it/s, loss=4.6e-5]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 480/500 [02:41<00:06,  2.98it/s, loss=4.6e-5]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 480/500 [02:41<00:06,  2.98it/s, loss=0.0124]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 481/500 [02:41<00:06,  2.99it/s, loss=0.0124]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 481/500 [02:41<00:06,  2.99it/s, loss=8.94e-6]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 482/500 [02:41<00:06,  2.99it/s, loss=8.94e-6]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 482/500 [02:41<00:06,  2.99it/s, loss=0.0108] \u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 483/500 [02:42<00:05,  3.00it/s, loss=0.0108]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 483/500 [02:42<00:05,  3.00it/s, loss=0.0843]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 484/500 [02:42<00:05,  3.00it/s, loss=0.0843]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 484/500 [02:42<00:05,  3.00it/s, loss=0.29]  \u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 485/500 [02:42<00:05,  3.00it/s, loss=0.29]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 485/500 [02:42<00:05,  3.00it/s, loss=0.000202]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 486/500 [02:43<00:04,  3.00it/s, loss=0.000202]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 486/500 [02:43<00:04,  3.00it/s, loss=0.043]   \u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 487/500 [02:43<00:04,  3.00it/s, loss=0.043]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 487/500 [02:43<00:04,  3.00it/s, loss=0.0491]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 488/500 [02:43<00:03,  3.01it/s, loss=0.0491]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 488/500 [02:43<00:03,  3.01it/s, loss=0.292] \u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 489/500 [02:44<00:03,  3.00it/s, loss=0.292]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 489/500 [02:44<00:03,  3.00it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 490/500 [02:44<00:03,  3.00it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 490/500 [02:44<00:03,  3.00it/s, loss=0.124]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 491/500 [02:44<00:03,  3.00it/s, loss=0.124]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 491/500 [02:44<00:03,  3.00it/s, loss=0.0763]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 492/500 [02:45<00:02,  2.98it/s, loss=0.0763]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 492/500 [02:45<00:02,  2.98it/s, loss=0.00189]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▊| 493/500 [02:45<00:02,  2.96it/s, loss=0.00189]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▊| 493/500 [02:45<00:02,  2.96it/s, loss=0.154]  \u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 494/500 [02:45<00:02,  2.98it/s, loss=0.154]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 494/500 [02:45<00:02,  2.98it/s, loss=0.00396]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 495/500 [02:46<00:01,  2.98it/s, loss=0.00396]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 495/500 [02:46<00:01,  2.98it/s, loss=0.945]  \u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 496/500 [02:46<00:01,  2.97it/s, loss=0.945]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 496/500 [02:46<00:01,  2.97it/s, loss=4.54e-5]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 497/500 [02:46<00:01,  2.98it/s, loss=4.54e-5]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 497/500 [02:46<00:01,  2.98it/s, loss=0.409]  \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 498/500 [02:47<00:00,  2.99it/s, loss=0.409]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 498/500 [02:47<00:00,  2.99it/s, loss=2.03e-5]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 499/500 [02:47<00:00,  2.98it/s, loss=2.03e-5]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 499/500 [02:47<00:00,  2.98it/s, loss=1.37e-6]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 500/500 [02:47<00:00,  3.00it/s, loss=1.37e-6]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [02:56<00:00, 176.68s/it]0,  3.00it/s, loss=5.18e-5]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 500/500 [02:56<00:00,  2.83it/s, loss=0.163, dist_mean=8.94]\u001b[A\n",
      "Train epoch: 100%|██████████| 1/1 [02:56<00:00, 176.68s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(use_lora=0)\n",
    "\n",
    "model.save_pretrained(f'KoChatGPT/{model_name_short}_output_2_RM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bee445b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3d713c",
   "metadata": {},
   "source": [
    "### PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9b449d",
   "metadata": {},
   "source": [
    "### 참고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2f22b",
   "metadata": {},
   "source": [
    "```\n",
    "LLM Trend Note2 노드에서 살펴본 KoChatGPT 소스코드는\n",
    "빠르게 baseline모델을 설계해 실습해보기 위해 오리지널 코드를 일부 수정한 버전입니다.\n",
    "프로젝트 진행을 위해 모델을 커스터마이징할 때, 필요시 \"colossalai_ChatGPT_230319\" 폴더 내의 원본 스크립트들을 참고하세요.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b20c9935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e3d53d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained=f'KoChatGPT/{model_name_short}_output_1_SFT_ext', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained=f'KoChatGPT/{model_name_short}_output_2_RM_prep', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=model_max_length\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99f838a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)\n",
    "\n",
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d1b1a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('KoChatGPT/data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "932288b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(\n",
    "    NaiveStrategy(),\n",
    "    actor,\n",
    "    critic,\n",
    "    reward_model,\n",
    "    initial_model,\n",
    "    actor_optim,\n",
    "    critic_optim,\n",
    "    max_epochs=1,  \n",
    "    train_batch_size=4, \n",
    "    tokenizer=tokenize_fn,\n",
    "    max_length=model_max_length,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fbaa7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(input_text, verbose=True):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(output)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aacb76",
   "metadata": {},
   "source": [
    "### 결과 - 'taeminlee/kogpt2': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb2970d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859fc0c8",
   "metadata": {},
   "source": [
    "- 'taeminlee/kogpt2'\n",
    "  - 수치를 낮추어도 메모리 부족으로 진행 불가  \n",
    "  - train_batch_size=1\n",
    "  - max_length=64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cba5916",
   "metadata": {},
   "source": [
    "### 결과 - 데이터 정제 및 데이터 증강:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "37c504bb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:50<00:25, 25.43s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000429]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.91it/s, actor_loss=0, critic_loss=0.000429]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.91it/s, actor_loss=0, critic_loss=0.183]   \u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.01it/s, actor_loss=0, critic_loss=0.183]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.01it/s, actor_loss=0, critic_loss=0.0185]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:00<00:06,  3.04it/s, actor_loss=0, critic_loss=0.0185]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.04it/s, actor_loss=0, critic_loss=0.0581]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.07it/s, actor_loss=0, critic_loss=0.0581]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.07it/s, actor_loss=0, critic_loss=0.0674]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.08it/s, actor_loss=0, critic_loss=0.0674]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.08it/s, actor_loss=0, critic_loss=0.0634]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:01<00:05,  3.07it/s, actor_loss=0, critic_loss=0.0634]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.07it/s, actor_loss=0, critic_loss=0.0426]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.09it/s, actor_loss=0, critic_loss=0.0426]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.09it/s, actor_loss=0, critic_loss=0.0464]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.07it/s, actor_loss=0, critic_loss=0.0464]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.07it/s, actor_loss=0, critic_loss=2.17e-5]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:02<00:04,  3.06it/s, actor_loss=0, critic_loss=2.17e-5]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.06it/s, actor_loss=0, critic_loss=0.127]  \u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.09it/s, actor_loss=0, critic_loss=0.127]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.09it/s, actor_loss=0, critic_loss=0.00288]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.10it/s, actor_loss=0, critic_loss=0.00288]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.10it/s, actor_loss=0, critic_loss=0.0617] \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:03<00:03,  3.07it/s, actor_loss=0, critic_loss=0.0617]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:03,  3.07it/s, actor_loss=0, critic_loss=0.0229]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.09it/s, actor_loss=0, critic_loss=0.0229]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.09it/s, actor_loss=0, critic_loss=5.61e-6]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.09it/s, actor_loss=0, critic_loss=5.61e-6]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.09it/s, actor_loss=0, critic_loss=0.051]  \u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:04<00:02,  3.08it/s, actor_loss=0, critic_loss=0.051]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:02,  3.08it/s, actor_loss=0, critic_loss=0.0937]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.05it/s, actor_loss=0, critic_loss=0.0937]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.05it/s, actor_loss=0, critic_loss=0.0751]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.06it/s, actor_loss=0, critic_loss=0.0751]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.06it/s, actor_loss=0, critic_loss=0.0797]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:05<00:01,  3.05it/s, actor_loss=0, critic_loss=0.0797]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:01,  3.05it/s, actor_loss=0, critic_loss=0.00361]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.05it/s, actor_loss=0, critic_loss=0.00361]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.05it/s, actor_loss=0, critic_loss=0.00687]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.06it/s, actor_loss=0, critic_loss=0.00687]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.06it/s, actor_loss=0, critic_loss=0.0483] \u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:06<00:00,  3.07it/s, actor_loss=0, critic_loss=0.0483]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:00,  3.07it/s, actor_loss=0, critic_loss=0.0382]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.97it/s, actor_loss=0, critic_loss=0.0382]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.97it/s, actor_loss=0, critic_loss=0.00951]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  2.99it/s, actor_loss=0, critic_loss=0.00951]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  2.99it/s, actor_loss=0, critic_loss=0.0698] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:07<00:00,  3.05it/s, actor_loss=0, critic_loss=0.0698]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [01:24<00:00, 28.24s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:50<00:25, 25.37s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=0.0652, critic_loss=4.9e-8]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.01it/s, actor_loss=0.0652, critic_loss=4.9e-8]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.01it/s, actor_loss=0.052, critic_loss=0.00156]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.02it/s, actor_loss=0.052, critic_loss=0.00156]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.02it/s, actor_loss=-.0792, critic_loss=0.0253]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:00<00:06,  3.02it/s, actor_loss=-.0792, critic_loss=0.0253]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.02it/s, actor_loss=0.221, critic_loss=0.00377]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.01it/s, actor_loss=0.221, critic_loss=0.00377]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.01it/s, actor_loss=-.169, critic_loss=0.157]  \u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.02it/s, actor_loss=-.169, critic_loss=0.157]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.02it/s, actor_loss=0.0903, critic_loss=0.00407]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:01<00:05,  3.03it/s, actor_loss=0.0903, critic_loss=0.00407]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.03it/s, actor_loss=0.0284, critic_loss=0.00146]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.06it/s, actor_loss=0.0284, critic_loss=0.00146]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.06it/s, actor_loss=0.092, critic_loss=0.00119] \u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.06it/s, actor_loss=0.092, critic_loss=0.00119]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:03<00:05,  3.06it/s, actor_loss=0.115, critic_loss=0.028]  \u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:05,  2.92it/s, actor_loss=0.115, critic_loss=0.028]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:05,  2.92it/s, actor_loss=0.117, critic_loss=0.0257]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  2.86it/s, actor_loss=0.117, critic_loss=0.0257]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  2.86it/s, actor_loss=0.158, critic_loss=0.0092]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  2.90it/s, actor_loss=0.158, critic_loss=0.0092]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:04<00:04,  2.90it/s, actor_loss=0.106, critic_loss=0.00706]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  2.96it/s, actor_loss=0.106, critic_loss=0.00706]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  2.96it/s, actor_loss=-.0763, critic_loss=0.00249]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.97it/s, actor_loss=-.0763, critic_loss=0.00249]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.97it/s, actor_loss=0.104, critic_loss=0.0237]  \u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  2.98it/s, actor_loss=0.104, critic_loss=0.0237]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:05<00:03,  2.98it/s, actor_loss=0.019, critic_loss=0.0292]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:02,  3.01it/s, actor_loss=0.019, critic_loss=0.0292]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:02,  3.01it/s, actor_loss=0.0765, critic_loss=0.0102]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.03it/s, actor_loss=0.0765, critic_loss=0.0102]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.03it/s, actor_loss=-.0288, critic_loss=0.00887]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.03it/s, actor_loss=-.0288, critic_loss=0.00887]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:06<00:02,  3.03it/s, actor_loss=-.0212, critic_loss=0.00996]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:01,  3.04it/s, actor_loss=-.0212, critic_loss=0.00996]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:01,  3.04it/s, actor_loss=-.131, critic_loss=0.0598]  \u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.00it/s, actor_loss=-.131, critic_loss=0.0598]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.00it/s, actor_loss=0.0226, critic_loss=0.00112]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  2.95it/s, actor_loss=0.0226, critic_loss=0.00112]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:07<00:01,  2.95it/s, actor_loss=-.000654, critic_loss=0.0311]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.95it/s, actor_loss=-.000654, critic_loss=0.0311]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.95it/s, actor_loss=0.0385, critic_loss=0.00109] \u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.96it/s, actor_loss=0.0385, critic_loss=0.00109]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.96it/s, actor_loss=-.00497, critic_loss=0.000873]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  2.96it/s, actor_loss=-.00497, critic_loss=0.000873]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:08<00:00,  2.96it/s, actor_loss=-.0712, critic_loss=0.000137] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:08<00:00,  2.98it/s, actor_loss=-.0712, critic_loss=0.000137]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [01:24<00:00, 28.12s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.59s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=0.256, critic_loss=0.0779]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.03it/s, actor_loss=0.256, critic_loss=0.0779]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.03it/s, actor_loss=63.6, critic_loss=876]    \u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.02it/s, actor_loss=63.6, critic_loss=876]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.02it/s, actor_loss=0.0257, critic_loss=0.0704]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:07,  2.99it/s, actor_loss=0.0257, critic_loss=0.0704]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:07,  2.99it/s, actor_loss=1.92, critic_loss=2.82]    \u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.03it/s, actor_loss=1.92, critic_loss=2.82]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.03it/s, actor_loss=0.133, critic_loss=0.344]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.03it/s, actor_loss=0.133, critic_loss=0.344]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.03it/s, actor_loss=-.0837, critic_loss=1.44]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:01<00:06,  3.00it/s, actor_loss=-.0837, critic_loss=1.44]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:06,  3.00it/s, actor_loss=0.126, critic_loss=0.705]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.00it/s, actor_loss=0.126, critic_loss=0.705]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.00it/s, actor_loss=9.71, critic_loss=138]   \u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.03it/s, actor_loss=9.71, critic_loss=138]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.03it/s, actor_loss=0.0511, critic_loss=3.81]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:02<00:04,  3.04it/s, actor_loss=0.0511, critic_loss=3.81]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.04it/s, actor_loss=-.0996, critic_loss=3.43]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.07it/s, actor_loss=-.0996, critic_loss=3.43]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.07it/s, actor_loss=-.0742, critic_loss=5.09]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.05it/s, actor_loss=-.0742, critic_loss=5.09]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.05it/s, actor_loss=0.000324, critic_loss=4] \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:03<00:03,  3.05it/s, actor_loss=0.000324, critic_loss=4]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:03,  3.05it/s, actor_loss=2.86, critic_loss=8.81] \u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.07it/s, actor_loss=2.86, critic_loss=8.81]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.07it/s, actor_loss=11.1, critic_loss=144] \u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.08it/s, actor_loss=11.1, critic_loss=144]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.08it/s, actor_loss=0.315, critic_loss=3.4]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:04<00:02,  3.06it/s, actor_loss=0.315, critic_loss=3.4]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:02,  3.06it/s, actor_loss=18.1, critic_loss=492] \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.07it/s, actor_loss=18.1, critic_loss=492]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.07it/s, actor_loss=0.00317, critic_loss=2.76]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.06it/s, actor_loss=0.00317, critic_loss=2.76]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.06it/s, actor_loss=19.5, critic_loss=206]    \u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:05<00:01,  3.03it/s, actor_loss=19.5, critic_loss=206]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:01,  3.03it/s, actor_loss=-.0199, critic_loss=2.48]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.02it/s, actor_loss=-.0199, critic_loss=2.48]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.02it/s, actor_loss=1.85, critic_loss=2.1]   \u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.03it/s, actor_loss=1.85, critic_loss=2.1]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.03it/s, actor_loss=0.867, critic_loss=0.234]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:06<00:00,  3.03it/s, actor_loss=0.867, critic_loss=0.234]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:00,  3.03it/s, actor_loss=0.0996, critic_loss=1.14]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.03it/s, actor_loss=0.0996, critic_loss=1.14]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.03it/s, actor_loss=0.0974, critic_loss=1.53]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.02it/s, actor_loss=0.0974, critic_loss=1.53]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.02it/s, actor_loss=3.61, critic_loss=15.4]  \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:07<00:00,  3.03it/s, actor_loss=3.61, critic_loss=15.4]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [01:24<00:00, 28.22s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.73s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=-1.17, critic_loss=2.44]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.00it/s, actor_loss=-1.17, critic_loss=2.44]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.00it/s, actor_loss=-.656, critic_loss=0.396]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.05it/s, actor_loss=-.656, critic_loss=0.396]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.05it/s, actor_loss=-1.07, critic_loss=1.22] \u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:00<00:06,  3.06it/s, actor_loss=-1.07, critic_loss=1.22]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.06it/s, actor_loss=-.737, critic_loss=0.363]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.06it/s, actor_loss=-.737, critic_loss=0.363]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.06it/s, actor_loss=-.838, critic_loss=0.246]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.08it/s, actor_loss=-.838, critic_loss=0.246]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.08it/s, actor_loss=-1.71, critic_loss=2.36] \u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:01<00:05,  3.09it/s, actor_loss=-1.71, critic_loss=2.36]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.09it/s, actor_loss=-.875, critic_loss=0.384]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.09it/s, actor_loss=-.875, critic_loss=0.384]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.09it/s, actor_loss=-.863, critic_loss=0.423]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.08it/s, actor_loss=-.863, critic_loss=0.423]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.08it/s, actor_loss=-.898, critic_loss=0.344]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:02<00:04,  3.08it/s, actor_loss=-.898, critic_loss=0.344]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.08it/s, actor_loss=-.246, critic_loss=0.222]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.01it/s, actor_loss=-.246, critic_loss=0.222]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.01it/s, actor_loss=-.94, critic_loss=0.35]  \u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.02it/s, actor_loss=-.94, critic_loss=0.35]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.02it/s, actor_loss=-.992, critic_loss=0.534]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:03<00:03,  3.05it/s, actor_loss=-.992, critic_loss=0.534]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:03,  3.05it/s, actor_loss=-1.16, critic_loss=0.74] \u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.05it/s, actor_loss=-1.16, critic_loss=0.74]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.05it/s, actor_loss=-1.17, critic_loss=1.22]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.04it/s, actor_loss=-1.17, critic_loss=1.22]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.04it/s, actor_loss=-1.49, critic_loss=1.64]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:04<00:02,  3.06it/s, actor_loss=-1.49, critic_loss=1.64]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:02,  3.06it/s, actor_loss=-1.26, critic_loss=1.04]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.05it/s, actor_loss=-1.26, critic_loss=1.04]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.05it/s, actor_loss=-.77, critic_loss=0.817]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.05it/s, actor_loss=-.77, critic_loss=0.817]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.05it/s, actor_loss=-.238, critic_loss=1.11]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:05<00:01,  3.06it/s, actor_loss=-.238, critic_loss=1.11]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:01,  3.06it/s, actor_loss=-.848, critic_loss=0.372]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.06it/s, actor_loss=-.848, critic_loss=0.372]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.06it/s, actor_loss=-.759, critic_loss=0.186]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.07it/s, actor_loss=-.759, critic_loss=0.186]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.07it/s, actor_loss=-.917, critic_loss=0.439]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:06<00:00,  3.07it/s, actor_loss=-.917, critic_loss=0.439]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:00,  3.07it/s, actor_loss=-1.18, critic_loss=0.689]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.07it/s, actor_loss=-1.18, critic_loss=0.689]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.07it/s, actor_loss=-.955, critic_loss=0.428]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.07it/s, actor_loss=-.955, critic_loss=0.428]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.07it/s, actor_loss=-.835, critic_loss=0.306]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:07<00:00,  3.06it/s, actor_loss=-.835, critic_loss=0.306]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [01:24<00:00, 28.27s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.54s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=0.177, critic_loss=0.0182]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:08,  2.83it/s, actor_loss=0.177, critic_loss=0.0182]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:08,  2.83it/s, actor_loss=0.283, critic_loss=0.0274]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  2.92it/s, actor_loss=0.283, critic_loss=0.0274]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:01<00:07,  2.92it/s, actor_loss=0.342, critic_loss=0.0745]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:07,  2.96it/s, actor_loss=0.342, critic_loss=0.0745]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:07,  2.96it/s, actor_loss=0.168, critic_loss=0.00765]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.97it/s, actor_loss=0.168, critic_loss=0.00765]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.97it/s, actor_loss=-.332, critic_loss=0.217]  \u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  2.99it/s, actor_loss=-.332, critic_loss=0.217]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:02<00:06,  2.99it/s, actor_loss=0.0725, critic_loss=0.00022]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.02it/s, actor_loss=0.0725, critic_loss=0.00022]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.02it/s, actor_loss=0.478, critic_loss=0.0974]  \u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.02it/s, actor_loss=0.478, critic_loss=0.0974]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.02it/s, actor_loss=0.25, critic_loss=0.0196] \u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.03it/s, actor_loss=0.25, critic_loss=0.0196]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.03it/s, actor_loss=-.333, critic_loss=0.297]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.03it/s, actor_loss=-.333, critic_loss=0.297]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.03it/s, actor_loss=0.981, critic_loss=0.289]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.02it/s, actor_loss=0.981, critic_loss=0.289]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.02it/s, actor_loss=-.0639, critic_loss=0.00108]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.00it/s, actor_loss=-.0639, critic_loss=0.00108]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:04<00:04,  3.00it/s, actor_loss=0.158, critic_loss=0.000181]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  2.93it/s, actor_loss=0.158, critic_loss=0.000181]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  2.93it/s, actor_loss=0.241, critic_loss=0.00143] \u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.92it/s, actor_loss=0.241, critic_loss=0.00143]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.92it/s, actor_loss=0.433, critic_loss=0.0298] \u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  2.92it/s, actor_loss=0.433, critic_loss=0.0298]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:05<00:03,  2.92it/s, actor_loss=1.08, critic_loss=0.821]  \u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  2.91it/s, actor_loss=1.08, critic_loss=0.821]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  2.91it/s, actor_loss=0.266, critic_loss=0.0078]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  2.90it/s, actor_loss=0.266, critic_loss=0.0078]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  2.90it/s, actor_loss=0.264, critic_loss=0.00048]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  2.92it/s, actor_loss=0.264, critic_loss=0.00048]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:06<00:02,  2.92it/s, actor_loss=1.36, critic_loss=1.74]    \u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.94it/s, actor_loss=1.36, critic_loss=1.74]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.94it/s, actor_loss=0.293, critic_loss=0.0206]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.94it/s, actor_loss=0.293, critic_loss=0.0206]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.94it/s, actor_loss=0.662, critic_loss=0.0682]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  2.95it/s, actor_loss=0.662, critic_loss=0.0682]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:07<00:01,  2.95it/s, actor_loss=0.0323, critic_loss=0.0768]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.95it/s, actor_loss=0.0323, critic_loss=0.0768]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.95it/s, actor_loss=0.284, critic_loss=0.0285] \u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.94it/s, actor_loss=0.284, critic_loss=0.0285]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.94it/s, actor_loss=-.648, critic_loss=0.641] \u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  2.96it/s, actor_loss=-.648, critic_loss=0.641]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:08<00:00,  2.96it/s, actor_loss=0.168, critic_loss=0.106]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:08<00:00,  2.96it/s, actor_loss=0.168, critic_loss=0.106]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [01:24<00:00, 28.32s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.55s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=-.895, critic_loss=0.51]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.03it/s, actor_loss=-.895, critic_loss=0.51]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  3.03it/s, actor_loss=-.528, critic_loss=0.275]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.02it/s, actor_loss=-.528, critic_loss=0.275]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.02it/s, actor_loss=-.402, critic_loss=0.206]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:00<00:06,  3.03it/s, actor_loss=-.402, critic_loss=0.206]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.03it/s, actor_loss=-.616, critic_loss=0.294]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.03it/s, actor_loss=-.616, critic_loss=0.294]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.03it/s, actor_loss=-.273, critic_loss=0.065]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  2.98it/s, actor_loss=-.273, critic_loss=0.065]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:02<00:06,  2.98it/s, actor_loss=-.222, critic_loss=0.0209]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:06,  2.97it/s, actor_loss=-.222, critic_loss=0.0209]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:06,  2.97it/s, actor_loss=-.103, critic_loss=0.000971]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  2.96it/s, actor_loss=-.103, critic_loss=0.000971]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  2.96it/s, actor_loss=-.138, critic_loss=0.0265]  \u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  2.96it/s, actor_loss=-.138, critic_loss=0.0265]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:03<00:05,  2.96it/s, actor_loss=-.221, critic_loss=0.00151]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:05,  2.95it/s, actor_loss=-.221, critic_loss=0.00151]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:05,  2.95it/s, actor_loss=-.255, critic_loss=0.00273]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  2.95it/s, actor_loss=-.255, critic_loss=0.00273]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  2.95it/s, actor_loss=-.597, critic_loss=0.103]  \u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  2.88it/s, actor_loss=-.597, critic_loss=0.103]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:04<00:04,  2.88it/s, actor_loss=-.187, critic_loss=0.00395]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  2.88it/s, actor_loss=-.187, critic_loss=0.00395]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  2.88it/s, actor_loss=-.123, critic_loss=0.0392] \u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.86it/s, actor_loss=-.123, critic_loss=0.0392]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.86it/s, actor_loss=-.652, critic_loss=0.187] \u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  2.89it/s, actor_loss=-.652, critic_loss=0.187]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:05<00:03,  2.89it/s, actor_loss=-.349, critic_loss=0.000331]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  2.90it/s, actor_loss=-.349, critic_loss=0.000331]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  2.90it/s, actor_loss=0.243, critic_loss=0.775]   \u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  2.88it/s, actor_loss=0.243, critic_loss=0.775]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  2.88it/s, actor_loss=-.275, critic_loss=0.000559]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  2.89it/s, actor_loss=-.275, critic_loss=0.000559]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:06<00:02,  2.89it/s, actor_loss=-.902, critic_loss=0.338]   \u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.91it/s, actor_loss=-.902, critic_loss=0.338]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.91it/s, actor_loss=-.084, critic_loss=0.049]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.93it/s, actor_loss=-.084, critic_loss=0.049]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.93it/s, actor_loss=-.294, critic_loss=0.00526]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  2.97it/s, actor_loss=-.294, critic_loss=0.00526]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:07<00:01,  2.97it/s, actor_loss=-.102, critic_loss=0.0306] \u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.99it/s, actor_loss=-.102, critic_loss=0.0306]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.99it/s, actor_loss=-.333, critic_loss=0.000955]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.00it/s, actor_loss=-.333, critic_loss=0.000955]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.00it/s, actor_loss=-.256, critic_loss=0.000528]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.01it/s, actor_loss=-.256, critic_loss=0.000528]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:08<00:00,  3.01it/s, actor_loss=0.555, critic_loss=1.27]    \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:08<00:00,  2.95it/s, actor_loss=0.555, critic_loss=1.27]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [01:24<00:00, 28.31s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:50<00:25, 25.48s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=-.251, critic_loss=0.0433]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.94it/s, actor_loss=-.251, critic_loss=0.0433]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.94it/s, actor_loss=-.339, critic_loss=0.115] \u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  2.97it/s, actor_loss=-.339, critic_loss=0.115]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:01<00:07,  2.97it/s, actor_loss=-.11, critic_loss=0.0515]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:07,  2.97it/s, actor_loss=-.11, critic_loss=0.0515]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:07,  2.97it/s, actor_loss=-.279, critic_loss=0.176]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.97it/s, actor_loss=-.279, critic_loss=0.176]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.97it/s, actor_loss=-.42, critic_loss=0.205] \u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  2.97it/s, actor_loss=-.42, critic_loss=0.205]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:02<00:06,  2.97it/s, actor_loss=-.364, critic_loss=0.19]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:06,  2.97it/s, actor_loss=-.364, critic_loss=0.19]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:06,  2.97it/s, actor_loss=-.634, critic_loss=0.414]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  2.98it/s, actor_loss=-.634, critic_loss=0.414]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  2.98it/s, actor_loss=-.0644, critic_loss=0.0149]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.00it/s, actor_loss=-.0644, critic_loss=0.0149]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:03<00:05,  3.00it/s, actor_loss=-.06, critic_loss=0.0127]  \u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.00it/s, actor_loss=-.06, critic_loss=0.0127]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.00it/s, actor_loss=0.0319, critic_loss=9e-5]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.00it/s, actor_loss=0.0319, critic_loss=9e-5]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.00it/s, actor_loss=31, critic_loss=964]     \u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.00it/s, actor_loss=31, critic_loss=964]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:04<00:04,  3.00it/s, actor_loss=-.0504, critic_loss=0.101]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  3.00it/s, actor_loss=-.0504, critic_loss=0.101]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:04,  3.00it/s, actor_loss=-.0483, critic_loss=0.629]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.00it/s, actor_loss=-.0483, critic_loss=0.629]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.00it/s, actor_loss=-.103, critic_loss=1.06]  \u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.00it/s, actor_loss=-.103, critic_loss=1.06]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:05<00:03,  3.00it/s, actor_loss=1.25, critic_loss=0.776]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  3.00it/s, actor_loss=1.25, critic_loss=0.776]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  3.00it/s, actor_loss=0.0265, critic_loss=1.35]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.01it/s, actor_loss=0.0265, critic_loss=1.35]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.01it/s, actor_loss=-.138, critic_loss=3.29] \u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.01it/s, actor_loss=-.138, critic_loss=3.29]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:06<00:02,  3.01it/s, actor_loss=-.036, critic_loss=3.35]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.99it/s, actor_loss=-.036, critic_loss=3.35]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.99it/s, actor_loss=0.0569, critic_loss=2.93]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.99it/s, actor_loss=0.0569, critic_loss=2.93]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.99it/s, actor_loss=-.53, critic_loss=6.53]  \u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  2.98it/s, actor_loss=-.53, critic_loss=6.53]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:07<00:01,  2.98it/s, actor_loss=-.591, critic_loss=6.47]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.97it/s, actor_loss=-.591, critic_loss=6.47]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.97it/s, actor_loss=0.00978, critic_loss=3.86]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.98it/s, actor_loss=0.00978, critic_loss=3.86]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.98it/s, actor_loss=-.0314, critic_loss=2.49] \u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  2.99it/s, actor_loss=-.0314, critic_loss=2.49]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:08<00:00,  2.99it/s, actor_loss=-.0656, critic_loss=3.18]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:08<00:00,  2.99it/s, actor_loss=-.0656, critic_loss=3.18]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [01:24<00:00, 28.19s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:50<00:25, 25.52s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=-1.05, critic_loss=1.75]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.95it/s, actor_loss=-1.05, critic_loss=1.75]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.95it/s, actor_loss=-1.18, critic_loss=1.92]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  2.98it/s, actor_loss=-1.18, critic_loss=1.92]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  2.98it/s, actor_loss=-1.3, critic_loss=2.12] \u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.00it/s, actor_loss=-1.3, critic_loss=2.12]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.00it/s, actor_loss=-1.18, critic_loss=0.338]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.99it/s, actor_loss=-1.18, critic_loss=0.338]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.99it/s, actor_loss=-.118, critic_loss=0.111]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.00it/s, actor_loss=-.118, critic_loss=0.111]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.00it/s, actor_loss=-.83, critic_loss=0.283] \u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.00it/s, actor_loss=-.83, critic_loss=0.283]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.00it/s, actor_loss=-1.23, critic_loss=0.851]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  2.99it/s, actor_loss=-1.23, critic_loss=0.851]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  2.99it/s, actor_loss=-1.06, critic_loss=0.702]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.01it/s, actor_loss=-1.06, critic_loss=0.702]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.01it/s, actor_loss=-1.57, critic_loss=1.21] \u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:02<00:04,  3.02it/s, actor_loss=-1.57, critic_loss=1.21]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.02it/s, actor_loss=-2.21, critic_loss=4.63]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.00it/s, actor_loss=-2.21, critic_loss=4.63]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.00it/s, actor_loss=-.969, critic_loss=0.48]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.01it/s, actor_loss=-.969, critic_loss=0.48]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.01it/s, actor_loss=1.8, critic_loss=7.69]  \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:03<00:03,  3.02it/s, actor_loss=1.8, critic_loss=7.69]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:03,  3.02it/s, actor_loss=-1.24, critic_loss=0.808]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.96it/s, actor_loss=-1.24, critic_loss=0.808]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  2.96it/s, actor_loss=-1.05, critic_loss=0.704]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  2.94it/s, actor_loss=-1.05, critic_loss=0.704]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:05<00:03,  2.94it/s, actor_loss=-.941, critic_loss=0.57] \u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  2.96it/s, actor_loss=-.941, critic_loss=0.57]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:03,  2.96it/s, actor_loss=-1.14, critic_loss=0.77]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  2.96it/s, actor_loss=-1.14, critic_loss=0.77]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  2.96it/s, actor_loss=-2.46, critic_loss=4.07]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  2.97it/s, actor_loss=-2.46, critic_loss=4.07]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:06<00:02,  2.97it/s, actor_loss=-1.57, critic_loss=1.95]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.97it/s, actor_loss=-1.57, critic_loss=1.95]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:02,  2.97it/s, actor_loss=-1.58, critic_loss=1.82]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.96it/s, actor_loss=-1.58, critic_loss=1.82]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  2.96it/s, actor_loss=-1.55, critic_loss=1.92]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  2.96it/s, actor_loss=-1.55, critic_loss=1.92]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:07<00:01,  2.96it/s, actor_loss=-1.57, critic_loss=1.49]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.94it/s, actor_loss=-1.57, critic_loss=1.49]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:01,  2.94it/s, actor_loss=-1.31, critic_loss=0.936]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.94it/s, actor_loss=-1.31, critic_loss=0.936]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  2.94it/s, actor_loss=-1.71, critic_loss=2.41] \u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  2.94it/s, actor_loss=-1.71, critic_loss=2.41]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:08<00:00,  2.94it/s, actor_loss=-1.33, critic_loss=1.31]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:08<00:00,  2.97it/s, actor_loss=-1.33, critic_loss=1.31]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [01:24<00:00, 28.24s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:49<00:24, 24.60s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=-.58, critic_loss=0.468]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.97it/s, actor_loss=-.58, critic_loss=0.468]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.97it/s, actor_loss=-.44, critic_loss=0.258]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.03it/s, actor_loss=-.44, critic_loss=0.258]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.03it/s, actor_loss=-.329, critic_loss=0.146]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:00<00:06,  3.05it/s, actor_loss=-.329, critic_loss=0.146]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.05it/s, actor_loss=-.0819, critic_loss=0.0089]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.99it/s, actor_loss=-.0819, critic_loss=0.0089]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  2.99it/s, actor_loss=-.691, critic_loss=0.532]  \u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  2.99it/s, actor_loss=-.691, critic_loss=0.532]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  2.99it/s, actor_loss=-.599, critic_loss=0.233]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:06,  2.99it/s, actor_loss=-.599, critic_loss=0.233]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:06,  2.99it/s, actor_loss=-.528, critic_loss=0.394]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.01it/s, actor_loss=-.528, critic_loss=0.394]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.01it/s, actor_loss=-.644, critic_loss=0.262]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.02it/s, actor_loss=-.644, critic_loss=0.262]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.02it/s, actor_loss=-.698, critic_loss=0.312]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:02<00:04,  3.03it/s, actor_loss=-.698, critic_loss=0.312]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.03it/s, actor_loss=-.29, critic_loss=0.00056]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.04it/s, actor_loss=-.29, critic_loss=0.00056]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.04it/s, actor_loss=-.577, critic_loss=0.126] \u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.05it/s, actor_loss=-.577, critic_loss=0.126]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.05it/s, actor_loss=-.247, critic_loss=0.0767]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:03<00:03,  3.06it/s, actor_loss=-.247, critic_loss=0.0767]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:03,  3.06it/s, actor_loss=-.613, critic_loss=0.0557]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.04it/s, actor_loss=-.613, critic_loss=0.0557]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.04it/s, actor_loss=-2.01, critic_loss=2.49]  \u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.05it/s, actor_loss=-2.01, critic_loss=2.49]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.05it/s, actor_loss=-.747, critic_loss=0.329]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:04<00:02,  3.06it/s, actor_loss=-.747, critic_loss=0.329]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:02,  3.06it/s, actor_loss=-1.08, critic_loss=0.451]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.03it/s, actor_loss=-1.08, critic_loss=0.451]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.03it/s, actor_loss=-.384, critic_loss=0.0185]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.05it/s, actor_loss=-.384, critic_loss=0.0185]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.05it/s, actor_loss=-.546, critic_loss=0.0419]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:05<00:01,  3.07it/s, actor_loss=-.546, critic_loss=0.0419]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:01,  3.07it/s, actor_loss=-1.64, critic_loss=2.42]  \u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.06it/s, actor_loss=-1.64, critic_loss=2.42]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.06it/s, actor_loss=-.127, critic_loss=0.245]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.08it/s, actor_loss=-.127, critic_loss=0.245]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.08it/s, actor_loss=-.556, critic_loss=0.109]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:06<00:00,  3.03it/s, actor_loss=-.556, critic_loss=0.109]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:00,  3.03it/s, actor_loss=-1.76, critic_loss=1.98] \u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.04it/s, actor_loss=-1.76, critic_loss=1.98]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.04it/s, actor_loss=-.253, critic_loss=0.188]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.05it/s, actor_loss=-.253, critic_loss=0.188]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.05it/s, actor_loss=-.443, critic_loss=0.0555]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:07<00:00,  3.04it/s, actor_loss=-.443, critic_loss=0.0555]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [01:22<00:00, 27.57s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:50<00:25, 25.45s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/24 [00:00<?, ?it/s, actor_loss=0.497, critic_loss=0.164]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.95it/s, actor_loss=0.497, critic_loss=0.164]\u001b[A\n",
      "Train epoch [1/1]:   4%|▍         | 1/24 [00:00<00:07,  2.95it/s, actor_loss=0.128, critic_loss=0.0245]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.01it/s, actor_loss=0.128, critic_loss=0.0245]\u001b[A\n",
      "Train epoch [1/1]:   8%|▊         | 2/24 [00:00<00:07,  3.01it/s, actor_loss=0.244, critic_loss=0.0406]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:00<00:06,  3.03it/s, actor_loss=0.244, critic_loss=0.0406]\u001b[A\n",
      "Train epoch [1/1]:  12%|█▎        | 3/24 [00:01<00:06,  3.03it/s, actor_loss=0.441, critic_loss=0.0321]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.02it/s, actor_loss=0.441, critic_loss=0.0321]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 4/24 [00:01<00:06,  3.02it/s, actor_loss=-.986, critic_loss=1.58]  \u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.03it/s, actor_loss=-.986, critic_loss=1.58]\u001b[A\n",
      "Train epoch [1/1]:  21%|██        | 5/24 [00:01<00:06,  3.03it/s, actor_loss=-.126, critic_loss=0.0134]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:01<00:05,  3.04it/s, actor_loss=-.126, critic_loss=0.0134]\u001b[A\n",
      "Train epoch [1/1]:  25%|██▌       | 6/24 [00:02<00:05,  3.04it/s, actor_loss=0.0204, critic_loss=0.00889]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.03it/s, actor_loss=0.0204, critic_loss=0.00889]\u001b[A\n",
      "Train epoch [1/1]:  29%|██▉       | 7/24 [00:02<00:05,  3.03it/s, actor_loss=0.417, critic_loss=0.0342]  \u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.04it/s, actor_loss=0.417, critic_loss=0.0342]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 8/24 [00:02<00:05,  3.04it/s, actor_loss=0.0149, critic_loss=0.0111]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:02<00:04,  3.06it/s, actor_loss=0.0149, critic_loss=0.0111]\u001b[A\n",
      "Train epoch [1/1]:  38%|███▊      | 9/24 [00:03<00:04,  3.06it/s, actor_loss=-.222, critic_loss=0.107]  \u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.02it/s, actor_loss=-.222, critic_loss=0.107]\u001b[A\n",
      "Train epoch [1/1]:  42%|████▏     | 10/24 [00:03<00:04,  3.02it/s, actor_loss=0.356, critic_loss=0.0143]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.02it/s, actor_loss=0.356, critic_loss=0.0143]\u001b[A\n",
      "Train epoch [1/1]:  46%|████▌     | 11/24 [00:03<00:04,  3.02it/s, actor_loss=0.356, critic_loss=0.0344]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:03<00:03,  3.05it/s, actor_loss=0.356, critic_loss=0.0344]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 12/24 [00:04<00:03,  3.05it/s, actor_loss=0.239, critic_loss=0.0103]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.04it/s, actor_loss=0.239, critic_loss=0.0103]\u001b[A\n",
      "Train epoch [1/1]:  54%|█████▍    | 13/24 [00:04<00:03,  3.04it/s, actor_loss=-.0386, critic_loss=0.0234]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.03it/s, actor_loss=-.0386, critic_loss=0.0234]\u001b[A\n",
      "Train epoch [1/1]:  58%|█████▊    | 14/24 [00:04<00:03,  3.03it/s, actor_loss=0.0374, critic_loss=0.0465]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:04<00:02,  3.05it/s, actor_loss=0.0374, critic_loss=0.0465]\u001b[A\n",
      "Train epoch [1/1]:  62%|██████▎   | 15/24 [00:05<00:02,  3.05it/s, actor_loss=-.00406, critic_loss=0.0273]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.01it/s, actor_loss=-.00406, critic_loss=0.0273]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 16/24 [00:05<00:02,  3.01it/s, actor_loss=0.583, critic_loss=0.0607]  \u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.03it/s, actor_loss=0.583, critic_loss=0.0607]\u001b[A\n",
      "Train epoch [1/1]:  71%|███████   | 17/24 [00:05<00:02,  3.03it/s, actor_loss=-.485, critic_loss=0.00616]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:05<00:01,  3.05it/s, actor_loss=-.485, critic_loss=0.00616]\u001b[A\n",
      "Train epoch [1/1]:  75%|███████▌  | 18/24 [00:06<00:01,  3.05it/s, actor_loss=-.573, critic_loss=0.699]  \u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.05it/s, actor_loss=-.573, critic_loss=0.699]\u001b[A\n",
      "Train epoch [1/1]:  79%|███████▉  | 19/24 [00:06<00:01,  3.05it/s, actor_loss=0.62, critic_loss=0.0415]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.05it/s, actor_loss=0.62, critic_loss=0.0415]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 20/24 [00:06<00:01,  3.05it/s, actor_loss=-.157, critic_loss=0.056]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:06<00:00,  3.06it/s, actor_loss=-.157, critic_loss=0.056]\u001b[A\n",
      "Train epoch [1/1]:  88%|████████▊ | 21/24 [00:07<00:00,  3.06it/s, actor_loss=-.779, critic_loss=0.27] \u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.05it/s, actor_loss=-.779, critic_loss=0.27]\u001b[A\n",
      "Train epoch [1/1]:  92%|█████████▏| 22/24 [00:07<00:00,  3.05it/s, actor_loss=0.207, critic_loss=0.00181]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.05it/s, actor_loss=0.207, critic_loss=0.00181]\u001b[A\n",
      "Train epoch [1/1]:  96%|█████████▌| 23/24 [00:07<00:00,  3.05it/s, actor_loss=0.221, critic_loss=0.00129]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 24/24 [00:07<00:00,  3.04it/s, actor_loss=0.221, critic_loss=0.00129]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [01:22<00:00, 27.37s/it]\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained(f'KoChatGPT/{model_name_short}_output_3_PPO_prep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "1c8e2875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'죄송합니다. 저는 AI 어시스턴트로, 실제 존재하는 가게의 메뉴나 정보를 찾을 수 없습니다. 하지만, 일반적으로 불고기용은 보통 한국에서 많이 판매되므로 전화로 주문하시면 됩니다. 자세한 정보는 해당 가게의 공식 홈페이지나 전화를 통해 확인해보시는 것이 좋을 것 같습니다.      \n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 42대 부통령직을 수행한 년도는 1947년이다. 프랭클린이 43대 부통령직을 맡았던 기간은 5년이었으며, 이후 1946년 12월 대선 후보자로 선정되었다. 프랭클린이 대선 후보로 선출되었다. 프랭클린의 취임 일자는 1952년이다. 프랭클린의 취임 일자는 1951년이니, 39대 킹과 킹이 당선되었다. 프랭클린의 취임 년도는 1952년부터 1975년까지이다. 프랭클린의 취임 연도는 알려지지 않았다. 프랭클린의 취임 일자는 킹이 아니라 그의 대통령직을 수행한 시기도 있다. 프랭클린의 취임 일자는 닉슨이 사망한 이후인 1963년이다. 프랭클린의 취임 일자는 1960년대 이후이다. 프랭클린의 취임 일자는 그의 취임일과 같다. 프랭클린의 닉슨이 1947년 취임 당시다. 프랭클린의 취임 일자는 그의 아버지의 출신이다. 프랭클린의 재임 일자는 그의 장인인다. 프랭클린의 취임일자가다. 프랭클린의 취임 일자는 그의 취임 일자에게 존중한 일기이다.다.릭. 적혀 있다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'현재로서는 위치를 파악할 수 없습니다. 좀 더 자세한 정보를 제공해 주세요.イ,,,,  등이 해당 지역의 이름일 수 있기 때문입니다., 와 가 속한 방언이 불분명하기 때문입니다. 와 가 소속된 구간의 이름일 수 있습니다. 와 를 지칭하는 구체적인 이름을 언급하시면 어떨까요? 와 와 가 속한 지역인 의 이름과 주소지가 다를 수 있기 때문입니다. 와 가 속한 지역도 비슷한 점이 있을 수 있습니다. 와 가 속한 언어도 관련이 있을 수 있으니 해당 지역의 지명을 확인해 주시기 바랍니다. 와 는 전 세계적으로 알려진 이름일 수 있습니다. 와는 별개의 이름이므로, 더 자세한 정보를 제공해 주세요. \n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지 농도는 보통 중국보다는 높을 수 있지만 대체로 평일에 높고 낮에도 높은 수준이다. 따라서 항상 쾌적한 날씨를 유지해야 합니다. 또한, 미세먼지 농도는 실외에서 측정되기 때문에 실외활동을 통해 미세먼지 농도를 체크할 수도 있다. 따라서, 날씨 변화에 따른 대기 상황에 따라 미세먼지 농도를 파악할 수 없으므로, 필요하다면 미세먼지 측정기를 사용하거나 주변 마트나 공원 등에 마스크를 비치하는 등 실내 활동을 줄이는 것도 도움이 될 수 있다. 언급된 날씨는 직접 측정하진 않는 경우가 많으므로 미세먼지 농도에 대한 대처 방법을 잘 생각해야 하겠습니다. 질문 보다 구체적인 질문이라 가능해 질문이자면, 최대한 도움을 드릴 수 있도록 노력하겠습니다. 내용을 제공해주신다면 감사합니다. 도움이 되겠습니다. 질문에 대해서는 전문적인 도움을 드리기 위해 노력하겠습니다.提供正者 께서는 구체적인 내용을 추가해 주시면 감사합니다. 도움이 되겠습니다! 도움을 드리지 못해 죄송합니다. 니다. mincience boals and loses me.\n"
     ]
    }
   ],
   "source": [
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b1137c",
   "metadata": {},
   "source": [
    "### 결과 - 추가 데이터셋:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c4f8537",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.65s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.000849]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.55it/s, actor_loss=0, critic_loss=0.000849]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.55it/s, actor_loss=0, critic_loss=0.188]   \u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.55it/s, actor_loss=0, critic_loss=0.188]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.55it/s, actor_loss=0, critic_loss=0.0082]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.56it/s, actor_loss=0, critic_loss=0.0082]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.56it/s, actor_loss=0, critic_loss=0.0439]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.57it/s, actor_loss=0, critic_loss=0.0439]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.57it/s, actor_loss=0, critic_loss=0.128] \u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.57it/s, actor_loss=0, critic_loss=0.128]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.57it/s, actor_loss=0, critic_loss=0.0765]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.57it/s, actor_loss=0, critic_loss=0.0765]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [01:21<00:00, 27.05s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:50<00:25, 25.46s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=0.0773, critic_loss=0.00713]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.53it/s, actor_loss=0.0773, critic_loss=0.00713]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.53it/s, actor_loss=0.0864, critic_loss=0.00407]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=0.0864, critic_loss=0.00407]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=0.111, critic_loss=0.0308]  \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.53it/s, actor_loss=0.111, critic_loss=0.0308]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.53it/s, actor_loss=0.083, critic_loss=0.0407]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.54it/s, actor_loss=0.083, critic_loss=0.0407]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.54it/s, actor_loss=0.091, critic_loss=0.0409]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=0.091, critic_loss=0.0409]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=0.0864, critic_loss=0.0135]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.54it/s, actor_loss=0.0864, critic_loss=0.0135]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [01:20<00:00, 26.86s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.70s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=-.0383, critic_loss=0.00259]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.52it/s, actor_loss=-.0383, critic_loss=0.00259]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.52it/s, actor_loss=-.0428, critic_loss=0.00458]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.53it/s, actor_loss=-.0428, critic_loss=0.00458]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.53it/s, actor_loss=-.0603, critic_loss=0.0167] \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.53it/s, actor_loss=-.0603, critic_loss=0.0167]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.53it/s, actor_loss=-.0814, critic_loss=0.0156]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.54it/s, actor_loss=-.0814, critic_loss=0.0156]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.54it/s, actor_loss=-.0415, critic_loss=0.0262]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.53it/s, actor_loss=-.0415, critic_loss=0.0262]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.53it/s, actor_loss=-.0492, critic_loss=0.0107]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.53it/s, actor_loss=-.0492, critic_loss=0.0107]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [01:21<00:00, 27.04s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.75s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=0.0122, critic_loss=0.000925]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.53it/s, actor_loss=0.0122, critic_loss=0.000925]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.53it/s, actor_loss=0.00931, critic_loss=0.00275]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=0.00931, critic_loss=0.00275]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=-.0391, critic_loss=0.0518]  \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.53it/s, actor_loss=-.0391, critic_loss=0.0518]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.53it/s, actor_loss=0.0417, critic_loss=0.00698]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.53it/s, actor_loss=0.0417, critic_loss=0.00698]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.53it/s, actor_loss=0.0388, critic_loss=0.00538]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.52it/s, actor_loss=0.0388, critic_loss=0.00538]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.52it/s, actor_loss=-.000946, critic_loss=0.00764]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.53it/s, actor_loss=-.000946, critic_loss=0.00764]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [01:21<00:00, 27.03s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.67s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=-.0331, critic_loss=0.000478]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.54it/s, actor_loss=-.0331, critic_loss=0.000478]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.54it/s, actor_loss=-.00195, critic_loss=0.00226]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=-.00195, critic_loss=0.00226]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=-.0179, critic_loss=0.0116]  \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.54it/s, actor_loss=-.0179, critic_loss=0.0116]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.54it/s, actor_loss=-.0301, critic_loss=0.0108]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.53it/s, actor_loss=-.0301, critic_loss=0.0108]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.53it/s, actor_loss=-.0206, critic_loss=0.0032]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=-.0206, critic_loss=0.0032]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=-.0192, critic_loss=0.00141]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.54it/s, actor_loss=-.0192, critic_loss=0.00141]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [01:20<00:00, 26.92s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.58s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=-.0036, critic_loss=0.00413]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.54it/s, actor_loss=-.0036, critic_loss=0.00413]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.54it/s, actor_loss=-.0354, critic_loss=0.00454]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.52it/s, actor_loss=-.0354, critic_loss=0.00454]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.52it/s, actor_loss=0.00474, critic_loss=0.0118]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.53it/s, actor_loss=0.00474, critic_loss=0.0118]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.53it/s, actor_loss=-.000253, critic_loss=0.00287]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.54it/s, actor_loss=-.000253, critic_loss=0.00287]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.54it/s, actor_loss=-.0308, critic_loss=0.00904]  \u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=-.0308, critic_loss=0.00904]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=-.0278, critic_loss=0.00383]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.53it/s, actor_loss=-.0278, critic_loss=0.00383]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [01:20<00:00, 26.96s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.63s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=-.0255, critic_loss=0.00206]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.51it/s, actor_loss=-.0255, critic_loss=0.00206]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.51it/s, actor_loss=-.00562, critic_loss=0.00674]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.52it/s, actor_loss=-.00562, critic_loss=0.00674]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.52it/s, actor_loss=-.0107, critic_loss=0.00567] \u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.52it/s, actor_loss=-.0107, critic_loss=0.00567]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.52it/s, actor_loss=-.0123, critic_loss=0.00968]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.52it/s, actor_loss=-.0123, critic_loss=0.00968]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.52it/s, actor_loss=-.0163, critic_loss=0.00461]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.53it/s, actor_loss=-.0163, critic_loss=0.00461]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.53it/s, actor_loss=-.0257, critic_loss=0.000974]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.53it/s, actor_loss=-.0257, critic_loss=0.000974]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [01:20<00:00, 26.89s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:51<00:25, 25.72s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=-.0235, critic_loss=0.00292]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.53it/s, actor_loss=-.0235, critic_loss=0.00292]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.53it/s, actor_loss=-.0531, critic_loss=0.00734]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=-.0531, critic_loss=0.00734]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=-.0485, critic_loss=0.00598]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.54it/s, actor_loss=-.0485, critic_loss=0.00598]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.54it/s, actor_loss=-.0394, critic_loss=0.00341]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.54it/s, actor_loss=-.0394, critic_loss=0.00341]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.54it/s, actor_loss=-.0468, critic_loss=0.0043] \u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=-.0468, critic_loss=0.0043]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=-.044, critic_loss=0.00188]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.54it/s, actor_loss=-.044, critic_loss=0.00188]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [01:21<00:00, 27.03s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:49<00:24, 24.79s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=0.033, critic_loss=0.00325]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.54it/s, actor_loss=0.033, critic_loss=0.00325]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.54it/s, actor_loss=0.0207, critic_loss=0.00209]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.53it/s, actor_loss=0.0207, critic_loss=0.00209]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.53it/s, actor_loss=0.0389, critic_loss=0.00613]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.53it/s, actor_loss=0.0389, critic_loss=0.00613]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.53it/s, actor_loss=0.0567, critic_loss=0.00282]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.54it/s, actor_loss=0.0567, critic_loss=0.00282]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.54it/s, actor_loss=0.0715, critic_loss=0.00061]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=0.0715, critic_loss=0.00061]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.54it/s, actor_loss=0.0188, critic_loss=0.0022] \u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.54it/s, actor_loss=0.0188, critic_loss=0.0022]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [01:18<00:00, 26.24s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:48<00:24, 24.26s/it]\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/1]:   0%|          | 0/6 [00:00<?, ?it/s, actor_loss=-.0374, critic_loss=0.00507]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:00<00:03,  1.55it/s, actor_loss=-.0374, critic_loss=0.00507]\u001b[A\n",
      "Train epoch [1/1]:  17%|█▋        | 1/6 [00:01<00:03,  1.55it/s, actor_loss=-.0368, critic_loss=0.00359]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=-.0368, critic_loss=0.00359]\u001b[A\n",
      "Train epoch [1/1]:  33%|███▎      | 2/6 [00:01<00:02,  1.54it/s, actor_loss=-.0673, critic_loss=0.00381]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:01<00:01,  1.54it/s, actor_loss=-.0673, critic_loss=0.00381]\u001b[A\n",
      "Train epoch [1/1]:  50%|█████     | 3/6 [00:02<00:01,  1.54it/s, actor_loss=-.0653, critic_loss=0.00253]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:02<00:01,  1.52it/s, actor_loss=-.0653, critic_loss=0.00253]\u001b[A\n",
      "Train epoch [1/1]:  67%|██████▋   | 4/6 [00:03<00:01,  1.52it/s, actor_loss=-.0246, critic_loss=0.00599]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.53it/s, actor_loss=-.0246, critic_loss=0.00599]\u001b[A\n",
      "Train epoch [1/1]:  83%|████████▎ | 5/6 [00:03<00:00,  1.53it/s, actor_loss=-.0442, critic_loss=0.00319]\u001b[A\n",
      "Train epoch [1/1]: 100%|██████████| 6/6 [00:03<00:00,  1.53it/s, actor_loss=-.0442, critic_loss=0.00319]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [01:18<00:00, 26.16s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1532f8e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(f'KoChatGPT/{model_name_short}_output_3_PPO_ext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d2310906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'불고기용은 한우의 맛을 내는 것은 추천하지 않습니다. 하지만 고기용 고기 한우는 건강하고 육질이 좋은 해산물을 주재료로 하기 때문에 맛과 품질을 더 좋아하기 때문입니다. 다만, 한우의 맛을 살려주는 요리로는 돼지고기나, 새우, 새우 양념 등이 있습니다. 다른 한우와 다른 질 나쁜 해산물 대신 즐겨 먹는 불고기를 추천할 수 있습니다. 추천하지 않았던 질 나쁜 해산물은 다시마, 맵고 톡 쏘는 맛이 더 많이 나게 됩니다. 한우와 다른 지방 성분이 들어 있는 소금 또는 견과류 역시 많이 먹으면 좋은 고기를 구할 수 있습니다. 대부분의 경우, 불고기용으로 시어머니나 친정 어머니가 제공하는 것을 추천합니다. 양념이나 견과류를 추천하는 것은 좋은 방법입니다. 좋은 음식 또는 고기 양념의 조합은 다양하지만, 대부분의 경우 적절한 것이 아닙니다. 고기용 고기를 먹을 때는 통통한 새우를 넣어 먹는 것이 좋습니다. 대부분의 양념이나 견과류는 매운맛보다는 육질과 짭짤한 맛을 좋아합니다. 좋은 재료로 만든 식사를 추천할 수 있습니다. 대부분의 경우 묵직하게 구운 치킨과 짭짜름\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):1975년. 미국 대통령으로서는 리처드 닉슨이 1949년 3월 31일 재임한 이래 최초로 재임한 대통령은 해리 트루먼 대통령이다. 트루먼 대통령은 전임 대통령보다 훨씬 재임 기간의 기간을 크게 줄여 6년이나 재임하고 있다. 전임 대통령은 미국이 1971년부터 1973년 사이에 연방정부 격을 35년으로 줄이는 개혁에 실패했다고 지적했다. 그는 또한 미국 경제가 1980년대와 90년대에 들어 상승하여 1980년대 중 가장 강력하다고 주장했다. 또한 그는 취임초기부터 의료보험 개혁을 위해 전임 대통령에 대한 탄핵과 사표를 제기하였고, 선거철에는 전임 대통령을 흠집 내느라 전임 대통령들의 빚을 갚았다. 이와 더불어 재임하면서 건강보험개혁으로 의료보험 개혁을 완성하였으며, 국민의 정부라는 대통령 취임 초년책을 성과로 평가했다. 그러나 이 결과물인 건강보험개혁은 많은 비판을 받았다. 전임 대통령 시절부터 추진된 의료보험 개혁은 의료보험을 개혁하는 결과를 낳았으며, 국민건강보험의 보장성도 개선하였다. 전임 대통령 재임 동안 발생한 교통망도 개선하였다. 이 개혁과 성과의 가장 짧은 기간 내에 추진된 것은 의료보험 개혁이다. 그의 재임 중에는 의료보험 개혁을 추진하여 국민의 의료비를 대폭 줄이고, 의료보험을\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항의 경우, 주로 중범죄와 테러 공격에 노출되기 때문에 보안이 매우 뛰어나기 때문에 국제공항이나 지역 공항을 통해 24시간 출입이 가능합니다. 공항마다 다르지만, 보통 공항과 지역 공항 모두 보안이 매우 엄격합니다. 공항이나 지역의 공항의 특성상 특정 공항의 보안 검색을 피하는 경우가 많지만, 만약 이를 무시하고 접근했을 경우, 항공사는 문을 닫는 것을 권유합니다. 만약 보안에 각별히 주의해야 할 필요가 있을 경우, 해당 공항의 보안 직원에게 직접 문의하시는 것이 좋습니다. 대부분의 항공사가 지역 공항을 책임지며, 여러 항공사가 보안 정책을 실시하고 있으니 항공사와 협의를 하여 공항 보안 정책을 강화해야 합니다. 어떤 항공사나 항공사가 공항에서 자사에 체크인을 요청하고, 체크인 전에 사전에 협의하는 것을 권장합니다. 지역 공항을 이용할 때, 사전에 충분한 체크인을 부탁드립니다. 모든 항공사가 공항에 체크인을 요청할 수 있는 지 여부를 확인하는 것은 매우 중요한 일입니다. 보안이 뛰어난 항공사나 항공기로 인해 보안 문제가 발생할 수 있으므로, 사전에 항공사와 여행사 간의 협의를 통해 적절한 보안 정책을 적용하는\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지는 인간에게는 건강에 해로운 영향을 미칠 수 있으므로 건강하고 쾌적한 환경을 유지하는 것이 중요합니다. 미세먼지 예방을 위한 생활 속 건강한 환경 유지 요령 등의 생활 습관을 지켜보시기 바랍니다. 감사합니다. 미세먼지를 줄이기 위해서는 우리 주변에 사용되는 마스크를 사용하여 실내 공기청소의 필요성이나 예방 방법을 찾아보는 것이 좋을 것입니다. 미세먼지의 경우 호흡기 건강에 위험을 줄 수 있으므로, 호흡기와 먼지를 함께 마시는 것은 건강에 유해한 물질일 수 있으므로 개인 양해를 하시는 것이 좋습니다. 미세먼지가 심해지는 시기에는 개인위생에 유의해야 할 일이 있습니다. 미세먼지가 건강에 유의할 수도 있지만, 생활습관으로는 생활 습관의 중요성에 대해 인지하고 충분한 수면과 꾸준한 노력이 필요합니다. 미세먼지는 인간도 건강에 치명적인 영향을 줄 수 있으므로, 건강한 실내 환경 유지를 위해 마스크를 사용하는 것이 좋습니다. 정확한 정보를 원하시는 경우, 미세먼지 예방을 위한 생활 습관을 유지하는 것이 도움이 될 것입니다. 미세먼지를 줄이기 위해서는 효과적인 청소 방법을 찾아보고, 적절한 수면과 청소를 유지하는 것이 도움이 될 수 있습니다. 미세\n"
     ]
    }
   ],
   "source": [
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236c34c4",
   "metadata": {},
   "source": [
    "- 추가 데이터셋 때문인지 핵심 답변은 안하고 구구절절 주변 설명만 늘어놓는다...;;;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6684acd",
   "metadata": {},
   "source": [
    "### BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "beab708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79348e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_bleu_single(src_sentence, tgt_sentence, verbose=True):\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"src: {src_sentence}\")\n",
    "        print(f\"tgt: {tgt_sentence}\")\n",
    "\n",
    "    reference = tgt_sentence\n",
    "    candidate = generation(src_sentence, verbose=False).split()\n",
    "\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                          smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        # print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", \" \".join(candidate))\n",
    "        print(\"Real: \", reference)\n",
    "        print(\"Score: %lf\\n\" % score)\n",
    "        \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bfceb290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def eval_bleu(sentences, verbose=True):\n",
    "    total_score = 0.0\n",
    "    sample_size = len(sentences)\n",
    "    \n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        src = sentences[idx]['prompt']\n",
    "        tgt = sentences[idx]['completion']\n",
    "        score = eval_bleu_single(src, tgt, verbose)\n",
    "        if not score: continue\n",
    "        \n",
    "        total_score += score\n",
    "    \n",
    "    print(\"Num of Sample:\", sample_size)\n",
    "    print(\"Total Score:\", total_score / sample_size)\n",
    "    \n",
    "    return total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f9e1cbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ds = get_json_dataset('data/kochatgpt_1_sft_ext.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "62d29559",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: 양장피라는 게 어디 있어요?\n",
      "tgt: '종이나 천으로 만든 옷으로, 특히 비서나 직장인들이 입는 포멀한(격식 있는) 의상을 일컫는 말이에요. 양장피를 파는 곳은 대부분의 쇼핑몰과 온라인 쇼핑몰에서 찾아볼 수 있어요! 또한 의류 전문점이나 백화점 패션코너에서도 구매할 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 1/40 [00:06<04:12,  6.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  양장피라는 게 어디 있어요? 어디서든 찾아갈 수 있습니다. 언제든지 제가 도와드리겠습니다! 언제든지 말씀해주세요. 제가 도움을 드린다면 언제든지 말씀해주세요. 다른 도움이 필요하신 분들께서도 도움 드릴 수 있습니다. serveded to asks medical formed your given passed or recording or your sessions your context or your quality assistants or one signals or them servedly or what it ebuy with their regular and what commiting to come opterants or portface to they during a gif it still or treaming inside your emotions of they treams agained people your statement to lear or all they during helpfople inside quality; but as any still toed and professing forces to\n",
      "Real:  '종이나 천으로 만든 옷으로, 특히 비서나 직장인들이 입는 포멀한(격식 있는) 의상을 일컫는 말이에요. 양장피를 파는 곳은 대부분의 쇼핑몰과 온라인 쇼핑몰에서 찾아볼 수 있어요! 또한 의류 전문점이나 백화점 패션코너에서도 구매할 수 있습니다.\n",
      "Score: 0.001527\n",
      "\n",
      "src: 양장피 이름 게 어디 있 어요 ?\n",
      "tgt: '종이나 천으로 만든 옷으로, 특히 비서나 직장인들이 입는 포멀한(격식 있는) 의상을 일컫는 말이에요. 양장피를 파는 곳은 대부분의 쇼핑몰과 온라인 쇼핑몰에서 찾아볼 수 있어요! 또한 의류 전문점이나 백화점 패션코너에서도 구매할 수 있습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 2/40 [00:12<03:53,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  양장피 이름 게 어디 있 어요? 검색해보시면 됩니다. \\n\\n해당 정보를 제공하시면 더 자세한 내용을 제공드릴 수 있습니다! 알려진 정보 외에도 추가적인 정보도 있으니, 더 자세한 정보를 제공하시면 더욱 정확한 답변을 제공해 드릴 수 있습니다. 어떤 지역에서 어느 지역에서, 어떤 지역에서 어떤 방법으로 정보를 알고 계신지 알려주시면 더욱 도움을 드릴 수 있습니다. 질문에 대한 다른 자세한 정보는 해당 지역의 행정 기관이나 관련 기관에 문의해 주시기 바랍니다. 지역에 대한 정확한 정보를 제공하신다면 더 정확한 답변을 제공해 드릴 수 있습니다. 어떤 지역에서도, 정보를 찾으시려면 지역의 관리자나 공공 기관 또는 관련 기관에 문의해보시는 것이 가장 정확한 방법일 것입니다. 지역 주민 및 지방 정부에서도 비슷한 정보를 얻을 수 있으므로, 지역에서 더 자세한 정보를 제공하시면 더 자세한 정보를 제공해 드릴 수 있습니다. 검색결과에서는 게시가 검색되고 있지만, 이에 대한 정보는 지역과 관련된 기관에서 제공하는 것이 일반적입니다. \\n\\n해당 정보뿐만 아니라 상세한 지역 정보를 제공하거나 지역 미디어서 관련 정보도 제공해주시면 더욱 정확한 답변을 제공해 드릴 수 있습니다. 검색결과, 지역, 관련된 기타 정보도 있으니, 참고해주시기 바랍니다. \\n\n",
      "Real:  '종이나 천으로 만든 옷으로, 특히 비서나 직장인들이 입는 포멀한(격식 있는) 의상을 일컫는 말이에요. 양장피를 파는 곳은 대부분의 쇼핑몰과 온라인 쇼핑몰에서 찾아볼 수 있어요! 또한 의류 전문점이나 백화점 패션코너에서도 구매할 수 있습니다.\n",
      "Score: 0.001505\n",
      "\n",
      "src: 세계 랭킹 10위권의 바둑 기사 점수는 몇점이야\n",
      "tgt: '제가 현재 시점 기준으로는 바둑 세계 랭킹 10위권 기사들의 점수를 파악할 수 없습니다. 이는 랭킹에 대한 정보가 수시로 갱신되기 때문입니다. 따라서, 랭킹 관련 사이트나 뉴스 등에서 확인하시는 것이 좋을 것 같습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3/40 [00:18<03:44,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  세계 랭킹 10위권의 바둑 기사 점수는 몇점이야? 그 중에서 0.02점을 기준으로 0.02점을 쳐주는 경우는 이례적인 사례입니다. 예를 들어, 바둑판 5단짜리 바둑 랭킹이 0.02점에서 0.00점을 가지고 있다면, 이 바둑판 5단짜리 기사의 랭킹인 0.26점은 0.02점이니 이게 롯데가 운영하는 일반적인 시스템이다. 해당 시스템에서 제공하는 롯데마트의 랭킹과 거의 똑같은 수치이기 때문에, 롯데가 운영하고 있는 바둑전문 네이버 바둑기사들에 따르면 바둑기사들의 랭킹은 0.19점이거나 심지어는 0.19점이 안기부에서 매겨진다. 따라서, 바둑기사들이 롯데마트를 통해 특정 해 판매 기사에 바둑 기사 1명을 등록시키는 것은 불법적인 행위로 보인다. 이는 각 기사들의 랭킹이 서로 다른 프로기사들에게 팔렸기 때문으로 추측되며, 현재 한화이앤씨, LG이앤씨 등 해당 업체에 등록된 기사 숫자는 약 50만명 이상으로 추정되고 있어 이 추측이 사실인지 아닌지 여부를 알 수 있다. 이러한 수치는 기사가 특정해 판매한 기사수에 비례해 다른 결과를 보이기도 한다. 롯데그룹 측은 이 사건의 파장을 줄이기 위해 특정 기사에 0.02점을 할당했고 해당 기사는 모두 삭제한다고 밝혔으나,\n",
      "Real:  '제가 현재 시점 기준으로는 바둑 세계 랭킹 10위권 기사들의 점수를 파악할 수 없습니다. 이는 랭킹에 대한 정보가 수시로 갱신되기 때문입니다. 따라서, 랭킹 관련 사이트나 뉴스 등에서 확인하시는 것이 좋을 것 같습니다.\n",
      "Score: 0.001977\n",
      "\n",
      "src: 세계 랭킹 10 위 권이 의 바둑 기사 점수 는 몇 점 이 야\n",
      "tgt: '제가 현재 시점 기준으로는 바둑 세계 랭킹 10위권 기사들의 점수를 파악할 수 없습니다. 이는 랭킹에 대한 정보가 수시로 갱신되기 때문입니다. 따라서, 랭킹 관련 사이트나 뉴스 등에서 확인하시는 것이 좋을 것 같습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 4/40 [00:24<03:35,  6.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  세계 랭킹 10 위 권이 의 바둑 기사 점수 는 몇 점 이 야 하는지에 대한 답변은 하지 않았다. 하지만 바둑이 있기 때문에 한 대회에서 10위권 이상 최상위권 성적을 가진 선수는 거의 없는 것으로 나타났다. 바둑 기사에 따르면 현재 바둑계의 컨센서스는 세계 최상위권으로, 국제 기금은 물론 기금 및 기금지 포럼 등에서 사용되고 있다. 기금의 경우 기금이 직접 예치하고, 이 돈을 바둑계에 출강한다. 기금의 한도는 정액으로, 기금은 이 돈을 이세돌 9단과 바둑 인공지능의 계산을 위한 모금, 이세돌 9단과의 알파 기사단 대국 시뮬레이션의 판매로 쓰고 있다. 이 모금은 2016년 도쿄에서 이세돌 9단과 중국의 천야오웨싱과 중국의 천저우가 한 판을 벌였는데 한 판의 승리보다 두 배 이상 많은 매출을 올리는 결과를 냈다. 이 사건의 총액은 약 9천만 원에서 9억 600만 원으로, 이 9억 8000만 원을 바둑계에 송금하게 된다. 기금의 정확한 계산은 해당 기금의 계정과 출강 기금의 총합에 따라 다르기 때문에, 좀 더 구체적인 정보를 원하신다면 문의해주시기 바랍니다. 이 사건의 경우, 인공지능에서 사용하는 바둑 알고리즘이\n",
      "Real:  '제가 현재 시점 기준으로는 바둑 세계 랭킹 10위권 기사들의 점수를 파악할 수 없습니다. 이는 랭킹에 대한 정보가 수시로 갱신되기 때문입니다. 따라서, 랭킹 관련 사이트나 뉴스 등에서 확인하시는 것이 좋을 것 같습니다.\n",
      "Score: 0.002014\n",
      "\n",
      "src: 진담을 농담처럼 하는 사람\n",
      "tgt: '진담을 농담처럼 하는 사람은 대개 비꼬거나 모욕적인 내용을 전달하기 위해 의도적으로 그런 행동을 합니다. 이러한 사람은 다른 사람의 기분을 상하게 하고, 신뢰를 잃게 만들 수 있습니다. 따라서, 적절한 대화와 행동을 유지하는 것이 중요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 5/40 [00:30<03:30,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  진담을 농담처럼 하는 사람이라면 그 당시 당신의 가장 중요한 이슈가 되었던 대선 행적에 관한 질문에 대해선 답변 드리겠습니다. 그러나 모든 문제의 현안으로 인하여 당일이 중요한 이슈가 되지 않는다면 어떻게 대응해야할지 모색해보세요. 당의 대표로서 당정이 직면한 위기의식과 대응은 당분간 지속될 것입니다. 중요한 것은 첫째, 경제위기의 진원지가 어디이며, 어떤 정책이 펼쳐져서 그 대응이 가능한지 판단하고, 문제를 심각히 보고 문제를 해결해야 합니다. 둘째, 당의 내부분채가 위기에 직면한 상황에서 어떤 정책을 전개해야할 지를 판단하고, 문제의 해결책을 모색하는 것입니다. 셋째, 당이 직면한 위기를 인식하고 위기 해결책을 구상하고, 이를 실천한다는 가정하에, 정책 실행 과정을 거치고 난 후, 상황을 분석하고, 위기 극복을 위한 당의 정책을 구상하고 검토할 것입니다. 당일에 관련된 현안이 있으면 당일 관련 질문을 요약해 주셔도 좋습니다. 당일에는, 또는 그 이상이 이슈가 되지 않도록 주의해야 합니다. 다른 문제나 주제는 당 내에서 당론을 만들고 당대회 결의문을 작성하는 과정에서 함께 언급됩니다. 당일에 관련된 질문이 없을 경우, 질문에 대한 답을 드릴 수 있습니다. 당일에 관련된 질문이 있으시면 그 자리에서 대답변을 드리도록 하겠습니다. 구체적인 것은 당신의 가장 중요한 이슈가 된\n",
      "Real:  '진담을 농담처럼 하는 사람은 대개 비꼬거나 모욕적인 내용을 전달하기 위해 의도적으로 그런 행동을 합니다. 이러한 사람은 다른 사람의 기분을 상하게 하고, 신뢰를 잃게 만들 수 있습니다. 따라서, 적절한 대화와 행동을 유지하는 것이 중요합니다.\n",
      "Score: 0.001334\n",
      "\n",
      "src: 예언서 을 농담 처럼 하 는 사람\n",
      "tgt: '진담을 농담처럼 하는 사람은 대개 비꼬거나 모욕적인 내용을 전달하기 위해 의도적으로 그런 행동을 합니다. 이러한 사람은 다른 사람의 기분을 상하게 하고, 신뢰를 잃게 만들 수 있습니다. 따라서, 적절한 대화와 행동을 유지하는 것이 중요합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 6/40 [00:36<03:25,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  예언서 을 농담 처럼 하 는 사람들도 많이 있으실 법이나 이런 게 있으셨어요. 그래서 어느 한 쪽에선 어떤 문제가 있는 것처럼 느껴지는 걸 거예요. 또는 다른 쪽이 문제가 많은 것처럼 느껴지는 게 좀 이상합니다. 좀더 자세히 설명을해주시면 답변을 드릴 수 있습니다. 하지만 제가 이 말씀을 듣고 싶다면 다시 한번 도움을 드리도록 하겠습니다. 제가 도와드릴 수 있는 것은 제 도움이 필요한 상황으로 보도해주시면 더욱 나은 답변을 드릴 수 있습니다. 만약 문제가 많은 것 같다면 보다 적극적으로 소통하시고 도움을 드리도록 노력하겠습니다. 문제를 해결하기 위해 더 많은 노력을 기울여주세요. 문제가 너무 많아서 더 많은 문제들, 특히 어려운 상황일수록 더 문제들이 있는 것처럼 느껴집니다. 문제를 해결하기 위한 새로운 노력들이 계속해서 발전해갈 수 있도록 노력해 보시고, 더 나은 발전을 위해 노력해보세요. 혹시 도움이 필요하신 부분이 있으면 언제든지 말씀해주세요. 도움이 필요한 상황이 있을 경우, 언제든지 제게 도움 드리겠습니다. 어려운 상황일수록 더 나은 길을 열어주는 것도 중요합니다. 말씀하신 문제가 많기 때문에 제가 최선의 노력을 할 수 있도록 최선을 다해주세요. 이런 문제를 해결하기 위해서 노력하는 것은 저의 노력입니다. 여러분의 어려움이 헛되지 않도록\n",
      "Real:  '진담을 농담처럼 하는 사람은 대개 비꼬거나 모욕적인 내용을 전달하기 위해 의도적으로 그런 행동을 합니다. 이러한 사람은 다른 사람의 기분을 상하게 하고, 신뢰를 잃게 만들 수 있습니다. 따라서, 적절한 대화와 행동을 유지하는 것이 중요합니다.\n",
      "Score: 0.003371\n",
      "\n",
      "src: 비디오 영상 혹은 애니메이션 기술을 이용하여 영상 속에 다양한 움직임을 만들어내는 그래픽은?\n",
      "tgt: 'CG (컴퓨터 그래픽스) 또는 VFX (비주얼 이펙트)입니다. 이 기술은 현실적이고 화려한 효과를 만들어내며, 영화, 게임, 광고 등 다양한 매체에서 사용됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 7/40 [00:42<03:18,  6.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  비디오 영상 혹은 애니메이션 기술을 이용하여 영상 속에 다양한 움직임을 만들어내는 그래픽은? 이른바 Messen All Goesing, 혹은 Maximus Developer라고 지칭한다. 이 개념은 Messen All Goesing이나 Vapage에 비유될 수 있지만, 컴퓨터 게임에 사용되거나 카메라 설정의 등 부가적인 기능을 제공하거나 실제 구현하는 것을 말한다. Messen All Goesing는 비디오 애니메이션, 게임, 비디오 파일 생성 및 재생 기능을 사용하는 대부분의 데스크탑에서 사용되는 것으로 비디오에서 가장 많이 사용되는 것이 비디오 게임이다. 비디오 게임에 사용되는 비디오 디플랫은 일반적으로 각 데스크탑마다 10에서 100개가 넘게 저장된다. 비디오 게임에서, 이 용어는 게임에서 사용되는 모든 비디오와 함께 구현된 데이터를 비디오 게임의 한 예로 생각할 수 있다. Media는 비디오 게임과 비디오 게임을 구별하지 않고 각각의 게임의 Media를 정의하여 정의하기도 했다. 게임의 그래픽 아트레이딩을 따르는 다른 방법에서 Media는 Media Academic, 게임 머니, 비디오 카드(bid Academic, 비디오 게임의 부가적인 특성, 게임 모드 및 음악 파일)의 비디오와 비슷하게 작동한다. Media는 \"메모\"라고 부르는 M\n",
      "Real:  'CG (컴퓨터 그래픽스) 또는 VFX (비주얼 이펙트)입니다. 이 기술은 현실적이고 화려한 효과를 만들어내며, 영화, 게임, 광고 등 다양한 매체에서 사용됩니다.\n",
      "Score: 0.001829\n",
      "\n",
      "src: 비디오 영상 혹은 애니메이션 기술 을 이용 하 여 영상 속 에 다양 한 움직임 을 만들 어 내 며 그래픽 은 ?\n",
      "tgt: 'CG (컴퓨터 그래픽스) 또는 VFX (비주얼 이펙트)입니다. 이 기술은 현실적이고 화려한 효과를 만들어내며, 영화, 게임, 광고 등 다양한 매체에서 사용됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 8/40 [00:48<03:10,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  비디오 영상 혹은 애니메이션 기술 을 이용 하 여 영상 속 에 다양 한 움직임 을 만들 어 내 며 그래픽 은? 이게 일반적인 방식이 아니니까 더 큰 화면의 다른 그래픽을 만들어낼 수 있는 경우도 있습니다만, 이러한 작업은 단순히 영상 자체의 품질을 유지하기 위해서입니다. 예를 들어, 어떤 웹 브라우저에 있는 그래픽이 몇 개의 작은 화면으로 구성되어 있습니다. 이러한 영상 파일의 경우, 해당 웹 브라우저와 관련된 모든 그래픽을 사용하여 작업할 수 있습니다. 따라서 더 큰 화면의 다른 그래픽을 만들거나 더욱 축소하는 작업은 필요하지 않습니다. 다른 웹 브라우저의 경우, 그래픽의 크기와 속도가 너무 높아 이를 개선하는 것이 어렵습니다. 다른 웹 브라우저에서도 비슷한 결과를 얻을 수 있습니다. 사진 영상이나 동영상 파일이 부가적으로 손실되면 이 점을 보완하기 위한 포털이 존재합니다. 사진 파일의 경우, 이러한 웹 브라우저의 그래픽 성능에 더하여, 더욱 더 좋은 기능이 주어질 수 있습니다. 포팅이나 링크 업에 따른 그래픽의 성능에 대한 추가 정보는 웹 브라우저 사용시 확인해야 합니다. 이 작업은 다양한 방법으로 이루어져 있으므로, 웹 브라우저에서 이러한 정보를 빠르게 확인할 수 있도록 도와줍니다. 예를 들어, 카메라나 비디오를 추가해 보는 것은 웹 브라우저에서 쉽게 가능하며, 한 번 사용한 영상\n",
      "Real:  'CG (컴퓨터 그래픽스) 또는 VFX (비주얼 이펙트)입니다. 이 기술은 현실적이고 화려한 효과를 만들어내며, 영화, 게임, 광고 등 다양한 매체에서 사용됩니다.\n",
      "Score: 0.006445\n",
      "\n",
      "src: 차카시나 할키나의 2012년 유럽 리듬 체조 선수권 대회 단체전 성적은\n",
      "tgt: '제가 찾은 정보에 따르면, 2012년 유럽 리듬 체조 선수권 대회 단체전에서 차카시나 할키나는 출전하지 않았습니다. 따라서 성적은 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▎       | 9/40 [00:54<03:03,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  차카시나 할키나의 2012년 유럽 리듬 체조 선수권 대회 단체전 성적은.42위. 2013년 5월 16일 파리에서 열린 FIFA 월드컵 3000만 명의 자국 선수들에게 3000만 파운드가 넘는 사상 최대의 금액인 3000만 파운드 이상의 성적으로 결승 진출에 성공했다. 3000만 파운드 가량의 성적이 예상되나 실제 성적은 이보다 다소 많은 것이 평론가들의 호평을 받았다. 《유럽 스포츠 전문기자》에 따르면 2012년 FIFA 월드컵 본선에 출전한 스페인 유스 출신 선수 5명 중 4명은 결승 전에 치러진 세차례 대회(선발 2위) 경기를 모두 경기 직후 버텼으며, 나머지 6명 중 4명은 결승전의 상대인 크로아티아를 상대로 압도적인 성적이 나왔다고 보도하였다. 또한 몇몇 언론에서는 최종 3분기에 결승전에서 크로아티아에게 0-0으로 비겼음에도 불구하고 4-5로 승리했다는 점과, 1-2로 승리한 것을 동시에 승부처에서 놓친 점 등으로 인해 최종적으로는 결승전이 과열이 된 것 아니냐는 의견도 존재한다. 스페인 유스팀에서는 결승전에서 크로아티아를 상대로 3-0으로 이긴 바 있다고 가정했을 경우 성적이 좋지 않은 선수는 참가하지 않았을 것이라고 판단하였다. 스페인 유스팀의\n",
      "Real:  '제가 찾은 정보에 따르면, 2012년 유럽 리듬 체조 선수권 대회 단체전에서 차카시나 할키나는 출전하지 않았습니다. 따라서 성적은 없습니다.\n",
      "Score: 0.000000\n",
      "\n",
      "src: 총영사관 할 키나 의 2012 년 유럽 리듬 체조 선수 권 대회 단체전 성적 은\n",
      "tgt: '제가 찾은 정보에 따르면, 2012년 유럽 리듬 체조 선수권 대회 단체전에서 차카시나 할키나는 출전하지 않았습니다. 따라서 성적은 없습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 10/40 [00:59<02:57,  5.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  총영사관 할 키나 의 2012 년 유럽 리듬 체조 선수 권 대회 단체전 성적 은 한화로 15억 1800만 달러, 총합계액 1865만 달러로, 역대 최다 출전을 기록한 종목 중 가장 높은 수익률을 기록한 것은 단연 티비 리그 팀이다. FA컵은 유럽과 오세아니아 대륙이 모두 참가해 역대 가장 많이 온 유럽 대륙에서 개최하는 첫 대회이다. 2018년 5월 FA컵 최종 예선에서 그는 아디다스보다 13점을 더 높게 잡았고, 18.8점 상승한 13.5점을 얻었다. 이는 티비 리그 역사상 최고이자 FA컵 사상 최저 기록이다. FA컵은 세계랭킹 33위인 아르헨티나, 브라질, 칠레와 함께 총 4회의 경기를 진행해 총 31회의 경기를 치렀다. 경기 후 현지 언론에 따르면 이 대회 결과는 역대 최다 출전에 올랐다. 예선 경기에서 15번 우승이었던 아르헨티나가 3점차 승리를 거두면서, 8강에서 우루과이가 5-3으로 승리했으며, 8강에서 브라질은 파라과이가 20-5로 패했다. 우루과이는 FA컵 역사상 처음으로 우승을 기록하면서 8강에서 16번 우승하였다. 우루과이가 9번째로, 브라질이 8번째인 것으로 기록되어 있다. 이 대회에서의 수익은 1억 1900만\n",
      "Real:  '제가 찾은 정보에 따르면, 2012년 유럽 리듬 체조 선수권 대회 단체전에서 차카시나 할키나는 출전하지 않았습니다. 따라서 성적은 없습니다.\n",
      "Score: 0.001803\n",
      "\n",
      "src: 푸시캣 돌스의 니콜 셰르징거의 지나친 부각을 비판한 인물은 Entertainment Weely의 누구인가?\n",
      "tgt: '정답: Kristen Baldwin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 11/40 [01:05<02:48,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  푸시캣 돌스의 니콜 셰르징거의 지나친 부각을 비판한 인물은 Entertainment Weely의 누구인가?라는 질문에, 마이클 타일러는 \"우리에게 있어 최고의 시야다. 예를 들어, \"Mon\": I Will Things To Gotta\" 또는 \"Teenage: Pokénerei\" 라고 할 수 있다. 타일러는 그가 《로봇》에서 그의 역할을 매우 부정적으로 평가하는 것에 대해 비난을 가했다. 타일러에 따르면, \"그는 많은 게임 제작자들이 자신의 작품을 진지하게 비평하지 않았다는 점이 그를 공격하는 이유이다. 왜냐하면 그들은 《로봇》의 여러 가지 문제에 대해 그들 자신의 관점만 생각하고 있기 때문이다. 타일러는 《로봇》이 시리즈의 여러가지 문제점을 드러낸다고 비판했다. 타일러는 게임 제작과 관련하여 가장 많이 영향을 준 사람으로, 롤플레잉 게임의 개발자 케빈 라이언(Kevin Leacher)에 대한 평가를 언급하는 것을 《로봇: The Robby of Zavin', 《로봇》의 개발자들 사이의 관계에 대해 언급한다. 그는 그의 \"중대한 게임\"이 \"더욱 중요한 것은 게임의 본질과 관계가 있는 것이며, 더 나은 게임의 미래에 대해 더 나은 지식을 제공해준다는 것이다.\n",
      "Real:  '정답: Kristen Baldwin\n",
      "Score: 0.000000\n",
      "\n",
      "src: 푸시캣 돌스 의 니콜 셰르징거 의 지나친 부각 을 비판 한 등장인물 은 Entertainment Weely 의 누구 인가 ?\n",
      "tgt: '정답: Kristen Baldwin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 12/40 [01:11<02:40,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  푸시캣 돌스 의 니콜 셰르징거 의 지나친 부각 을 비판 한 등장인물 은 Entertainment Weely 의 누구 인가? 제임스 캐닝턴. 2012년 12월 5일, 미국 시사만화 사전제 3회 로 올해의 비디오를 선정했다. 이 단편은 2012년 11월 7일 뉴욕 타임스 스퀘어에서 열린 '크레이지스톤 스캔들' 전야제에서 처음 공개되었으며, 다음 날인 12월 2일(리즈)에 시카고 상그레츠 앤 더 클럽 앤 더 챔피언스전에선 '인터스텔라 스캔들' 부문 후보로 올려졌다. 또한, 2011년 12월 5일자 시사예술제 대상에서 '인터스텔라 스캔들'의 대명사 \"유태계 작가인 제임스 캐닝턴의 사악한 세계와 그와 같은 시대를 살아가는 베트의 이야기를 담은 '인터스텔라 스캔들'에 선정되었다. 제이-세팅(J-SPOM)이 장편 시리즈를 수상한 '인터스텔라'의 후속작인데, 2010년에 출연한 '그린 랜턴'이란 영화에서 이 영화는 비평가들의 압도적인 사랑과 극찬을 받아 캐닝턴의 사악한 세계에 대한 관심과 호평을 받았다. 이러한 긍정적평과 부정적 평가 모두 무시할 수 없었는데, 특히 평론가에게 긍정적인 반응을 이끌어낸 작품의 사례는 '인터스텔라'를\n",
      "Real:  '정답: Kristen Baldwin\n",
      "Score: 0.000000\n",
      "\n",
      "src: 카푸치노는 빅 사이즈로 주시고 아메리카노는 스몰로 주시겠어요?\n",
      "tgt: '저는 인공지능 어시스턴트이므로, 주문을 여쭤보거나 받지 않습니다. 주문하실 때는 해당 매장의 직원에게 직접 언급하시면 됩니다. 감사합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▎      | 13/40 [01:16<02:35,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  카푸치노는 빅 사이즈로 주시고 아메리카노는 스몰로 주시겠어요? 카푸치노보다는 스몰로우를 드릴게요. 직접 만나면 도움을 드리는 사이즈도 있으니 많이 선택해주세요! 커피가 아닌 스몰로우를 제공할 수도 있습니다. 직접 커피 주문하시겠어요? 카드번호와 계정도 확인해보시겠어요? 감사합니다. 포장으로 구매하시거나 스몰로우의 서비스를 제공할 수도 있으니 참고해주세요. 일부 카페나 사이트에서 다양한 아메리카노를 주문하시면 아메리카노 교환 및 추가 할인 혜택도 제공해드릴 수 있으니 참고해보세요. 커피를 시럽을 경우 여러 컵으로 주문하시면 선물해주는 것이 좋을 것 같습니다. 스몰로우를 주신 커피잔에 스네이크나 아메리카노 스네이크 하나를 주문하시는 것도 좋은 방법이 될 수 있습니다. 포장은 챗봇 또는 픽스샵에서 주문해보시는 것이 가장 좋은 옵션 중 하나일 수 있습니다. 커피 시럽을 원하시면 아메리카노 혹은 아메리카노도 함께 주문하시면 됩니다. 스네이크나 아메리카노를 원하시면 아메리카노 라떼나 플레인 스무디, 카카카오케이크, 아메리카노\n",
      "Real:  '저는 인공지능 어시스턴트이므로, 주문을 여쭤보거나 받지 않습니다. 주문하실 때는 해당 매장의 직원에게 직접 언급하시면 됩니다. 감사합니다.\n",
      "Score: 0.000000\n",
      "\n",
      "src: 카푸치노 는 빅 사이즈 로 주도 시 고 아메리카노 는 스몰 로 주 시 겠 어요 ?\n",
      "tgt: '저는 인공지능 어시스턴트이므로, 주문을 여쭤보거나 받지 않습니다. 주문하실 때는 해당 매장의 직원에게 직접 언급하시면 됩니다. 감사합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 14/40 [01:22<02:29,  5.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  카푸치노 는 빅 사이즈 로 주도 시 고 아메리카노 는 스몰 로 주 시 겠 어요? 다른 음료를 살 수 있죠. 커피숍에서 주문하시면 됩니다. 아메리카노, 치즈도 함께 보내주시면 감사하겠습니다. 언제든지 커피의 맛이 바뀌어서 주문하시기 바랍니다. 다른 음식도 소개해주세요. 주문하는 매장마다 차이가 있으니 참고하시기 바랍니다. 아메리카노는 리필해서 원두 구매하실 수 있으니 주문하시는 가게에서 구매하신 후 구매해보시는 것도 좋습니다. 만약 커피숍이 있다면 직접 주문해서 주문하실 수 있으실 수도 있습니다. 매장에서 주문하신 사이즈로 저희가 바로 받아보시는 것도 좋습니다. 감사합니다. 모든 음료를 주문하시면 감사합니다. 더 이상 음료를 받지 못해 죄송합니다. 커피로 대체된 많은 음료를 구매하시는 분들은 직접 커피를 가져다드릴 수도 있죠 추가적인 음료를 직접 주문하시면 더 좋은 결과를 드릴 수 있을 거예요. 대부분의 음료를 직접 가져다드리도록 하세요. 커피를 추천해주신다면 가장 적합한 방법을 선택하시는 것이 좋을 것 같습니다. 어떤 음료를 원하시나 감사합니다. 일반적으로 주문\n",
      "Real:  '저는 인공지능 어시스턴트이므로, 주문을 여쭤보거나 받지 않습니다. 주문하실 때는 해당 매장의 직원에게 직접 언급하시면 됩니다. 감사합니다.\n",
      "Score: 0.002183\n",
      "\n",
      "src: 역세권이에요?\n",
      "tgt: \"저는 인공지능 언어모델이어서 지리적인 개념을 이해하는 데 한계가 있습니다. 반면, '역세권'이란 용어의 의미를 아는 경우, 이를 바탕으로 답변을 제공할 수 있습니다. '역세권'이란 지하철 역과 인접한 지역을 의미하는 용어로, 이 지역은 대중교통 이용이 용이하며, 주위에 상가와 주거지가 밀집해있는 등 생활 편의성이 높은 특징이 있습니다. 이러한 특징 때문에 부동산 가격이 상승하거나 상승할 가능성이 큽니다.\", 'token': 232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 15/40 [01:28<02:26,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  역세권이에요? #n\\nAI가 새로운 기능을 가진 컴퓨터로 진화하면서 시간이 지나면서 지능이 진화하여 현재는 지능에 더 이상 의지하지 않고 있다”는 것이 현재 대한민국의 미래인력에 관한 연구 결과이다. 그러나 국내 연구진에 의하면 최근 몇 년간 우리 주변에 있는 외국계 컴퓨터들이 급격히 증가해 현재 대한민국의 지능에 대한 발전 속도가 더더욱 가속하고 있다는 것이다. 이러한 지능이 갖고 있는 한계성은 우리 생활속에서 가장 큰 문제점으로 부각되고 있다. 즉, 우리는 오랜 시간에 걸쳐 인터넷, 게임, TV 등을 이용하여 새로운 기능을 이용하면서 점점 지능이 진화하면서 점점 더 시간이 지날수록 더 지능이 정교해지게 된다는 것이다. 실제로 한국의 인터넷 이용자가 지속적으로 증가하며 현재 대한민국의 사회 평균보다 더 높은 인터넷 이용률을 보이고 있다. 또한 최근에 증가하고 있는 인터넷 트래픽도 지능의 한 요인으로 작용했다. 대한민국은 인터넷 속도가 세계 최고 수준으로 높은 데다가 인터넷 사용률로 인해 정보 이용 시간이 월등히 많아져, 인터넷이 급속히 보급되면서 인간의 정보 관리는 더 이상 개선되어 가고 있지 않다는 것이다. 이러한 문제점은 지식경제학의 발전과 함께 해결될 수 있는 문제이다. 대한민국의 지식정보사회 선진국은 지식의 생산과 정보 이용 그리고 지식의 관리를 위한 지식관리 시스템을 구축하여 이를 바탕으로 지식 정보화 사회를 이끌어 내고 있다. 지식정보가 지식의\n",
      "Real:  \"저는 인공지능 언어모델이어서 지리적인 개념을 이해하는 데 한계가 있습니다. 반면, '역세권'이란 용어의 의미를 아는 경우, 이를 바탕으로 답변을 제공할 수 있습니다. '역세권'이란 지하철 역과 인접한 지역을 의미하는 용어로, 이 지역은 대중교통 이용이 용이하며, 주위에 상가와 주거지가 밀집해있는 등 생활 편의성이 높은 특징이 있습니다. 이러한 특징 때문에 부동산 가격이 상승하거나 상승할 가능성이 큽니다.\", 'token': 232}\n",
      "Score: 0.000820\n",
      "\n",
      "src: 역세 권 이 .. ?\n",
      "tgt: \"저는 인공지능 언어모델이어서 지리적인 개념을 이해하는 데 한계가 있습니다. 반면, '역세권'이란 용어의 의미를 아는 경우, 이를 바탕으로 답변을 제공할 수 있습니다. '역세권'이란 지하철 역과 인접한 지역을 의미하는 용어로, 이 지역은 대중교통 이용이 용이하며, 주위에 상가와 주거지가 밀집해있는 등 생활 편의성이 높은 특징이 있습니다. 이러한 특징 때문에 부동산 가격이 상승하거나 상승할 가능성이 큽니다.\", 'token': 232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 16/40 [01:34<02:22,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  역세 권 이..?\\n그래도 그중에 하나가 그런 것 같아. 그중에 하나가 어떤 특정한 대상을 지목할 때 그 대상의 이름이나 사실을 가지고 그것을 그 대상을 지목해 버리는 일종의 자기 모독행위거든. 누군가에 대한 집착을 그대로 두려는 경향도 있는데, 예를 들어서, 이 사람이 부동산 중개인을 통해 \"내 발언에 대한 신뢰와 근거를 갖고 있지 않은가?\"라는 물음에 \"그럴 수 있습니다. 그런 취지로 인해 상대방이 '내가 누군가에게 관심을 가지고 있는지 모른다, 만약 그것이 안되면 내가 그 사람을 지목할 수도 있다' 등의 심리술을 통해 상황을 파악한 것이 중요합니다. 그렇게 되면 그 상황에서 아무 생각도 해 내지 않고, 그 상황에서 \"내가 누군가에게 관심을 갖고 있다는 것만으로도 상대방이 그 대상에 대해서 생각하는 것이 너무나 큰 신뢰감 때문에 그런 선택을 하게 되는 게 아닌가. 그런 관점에서 본다면 이 사람 자체가 특정 대상을 지목하는 식으로 자신의 심리를 이용하는 것은 무례한 일일 뿐만 아니라 상당히 위험한 행동일 수도 있습니다. 그렇기 때문에 이런 상황에서 누군가에게 직접적으로, 혹은 상대방의 의사와 동의가 없는 한은, 그런 행동들을 하든 그 자체가 목적이 되는 그러한 선택을 하는 것이 훨씬 더 중요합니다.\"고 평가했다. 다른 견해도 있습니다. 사람의 개인에 대해서는 다른 시각에서 보았을 때 그것은 부정적인 측면들이 있을 수 있고, 마찬가지로, 그것이 개인의 개인\n",
      "Real:  \"저는 인공지능 언어모델이어서 지리적인 개념을 이해하는 데 한계가 있습니다. 반면, '역세권'이란 용어의 의미를 아는 경우, 이를 바탕으로 답변을 제공할 수 있습니다. '역세권'이란 지하철 역과 인접한 지역을 의미하는 용어로, 이 지역은 대중교통 이용이 용이하며, 주위에 상가와 주거지가 밀집해있는 등 생활 편의성이 높은 특징이 있습니다. 이러한 특징 때문에 부동산 가격이 상승하거나 상승할 가능성이 큽니다.\", 'token': 232}\n",
      "Score: 0.001099\n",
      "\n",
      "src: 암호화폐를 분산 저장하여 운영 관리 될 수 있도록 하는 인터넷상의 방식은?\n",
      "tgt: '블록체인 기술을 사용하여 암호화폐를 분산 저장하여 운영 및 관리하는 것을 의미합니다. 블록체인은 암호화폐 거래 정보를 연속적인 블록으로 저장하며, 이러한 블록은 분산된 노드들이 모두 복제해 가지고 있습니다. 이런 방식으로 암호화폐 거래 정보의 중앙집중식 저장이 아닌 분산 저장을 실현하고, 이에 따라 관리 구조도 탈중앙화됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▎     | 17/40 [01:40<02:16,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  암호화폐를 분산 저장하여 운영 관리 될 수 있도록 하는 인터넷상의 방식은? 인터넷상에서 서로 다른 시스템들을 연결해서 서로를 공유할 수 있다. 그러나 이 방법으로는 제한적인 범위의 물리적인 방법을 사용한 것과 마찬가지로, 이미지는 다른 범위의 물리적인 방법을 사용한다. 이 범위의 물리적인 방법으로는 인터넷상에서 서로 다른 시스템들을 연결해 상호작용적인 구조를 만드는 것과, 이미지로 구성된 링크를 생성하는 방법 등이 있다. 이를 위해서 그들은 링크 생성 기술 (링크 생성 기술) 을 사용한다. 링크 생성 기술 (링크 생성 기술)은 링크 생성 기술 (링크 생성 기술) 은 물리적인 링크의 생성 및 관리 (Security and Translation) 을 의미하며 주로 사용되는 링크는 각 링크마다 다르게 나타난다. 예를 들어, 기존의 링크의 생성 방법으로는 각각의 링크가 생성되었을 때 각각의 링크 생성 기술에 나타나는 링크들을 전부 함께 표시해야 했기 때문에, 링크 생성 기술이 필요하다. 일반적으로 이 범위의 링크는 생성되기 전에 서로 다른 링크들을 불러서 상호작용을 통해 상호작용을 일으킨 후, 서로 다른 링크들을 불러서 상호작용을 한다. 이렇게 생성된 링크는 그 수가 많으면 짧다거나 혹은 링크가 생성되기 전에 서로 다른 링크들을 불러서 상호작용을 일으킬 수 있다. 링크는 보통\n",
      "Real:  '블록체인 기술을 사용하여 암호화폐를 분산 저장하여 운영 및 관리하는 것을 의미합니다. 블록체인은 암호화폐 거래 정보를 연속적인 블록으로 저장하며, 이러한 블록은 분산된 노드들이 모두 복제해 가지고 있습니다. 이런 방식으로 암호화폐 거래 정보의 중앙집중식 저장이 아닌 분산 저장을 실현하고, 이에 따라 관리 구조도 탈중앙화됩니다.\n",
      "Score: 0.001576\n",
      "\n",
      "src: 암호 화폐 를 분산 저장 하 여 운영 관리 될 ㄹ 있 도록 하 는 인터넷 상 의 방식 은 ?\n",
      "tgt: '블록체인 기술을 사용하여 암호화폐를 분산 저장하여 운영 및 관리하는 것을 의미합니다. 블록체인은 암호화폐 거래 정보를 연속적인 블록으로 저장하며, 이러한 블록은 분산된 노드들이 모두 복제해 가지고 있습니다. 이런 방식으로 암호화폐 거래 정보의 중앙집중식 저장이 아닌 분산 저장을 실현하고, 이에 따라 관리 구조도 탈중앙화됩니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 18/40 [01:46<02:09,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  암호 화폐 를 분산 저장 하 여 운영 관리 될 ᄅ 있 도록 하 는 인터넷 상 의 방식 은? 이슈를 활용하여, 웹 표준화, 표준화, 관리 등의 방법으로 웹의 신뢰도를 높일 수 있다는 장점이 있습니다. 또한, 분산된 화폐들을 모두 분산 저장함으로써, 해당 화폐 또는 서비스에 대해 배타적으로 서비스를 제공하는 대신 수수료가 저렴하게 부과될 수 있습니다. 일부 웹 사이트에서는 분산화된 화폐를 분산 저장하거나 일반 문자메시지(SMS) 등으로 전송하는 경우도 있었으며, 이로 인해 분산된 지폐를 한 화소에 담으는데 드는 비용이 상대적으로 저렴하여 활용도가 높습니다. 따라서, 일부 사이트에서는 분산된 화폐들을 한 화소에 담아도 다른 사이트에서 다운받아 사용할 수 있습니다. 일부 웹사이트에서는 분산적으로 웹의 신뢰성을 높일 수 있습니다. 웹 표준화가 이루어지기 전, 웹 표준화, 관리 방법 등에 대해서 논의한 바 있습니다. 분산된 화폐일수록, 관리 방법에 따라 가격도 크게 비싸고, 관리 방법도 웹 사이트의 웹 표준화와 관리 방법에 따라 다양합니다. 웹 표준화를 통해 웹의 신뢰도를 높일 수 있으므로, 특정 웹 사이트에서 분산된 중앙 시스템을 분산하여 구축하여 이용자들이 사이트에 접근할 수 있도록 하거나, 이전 웹 사이트로의 구축을 원하는 웹 사이트나 서버에 분산시켜 이전 웹\n",
      "Real:  '블록체인 기술을 사용하여 암호화폐를 분산 저장하여 운영 및 관리하는 것을 의미합니다. 블록체인은 암호화폐 거래 정보를 연속적인 블록으로 저장하며, 이러한 블록은 분산된 노드들이 모두 복제해 가지고 있습니다. 이런 방식으로 암호화폐 거래 정보의 중앙집중식 저장이 아닌 분산 저장을 실현하고, 이에 따라 관리 구조도 탈중앙화됩니다.\n",
      "Score: 0.003452\n",
      "\n",
      "src: 목포극장 개업일은\n",
      "tgt: '제가 알기로는 정확한 날짜를 알 수 없습니다. 재개발로 구조가 변경되고 리모델링 과정이 있을 경우 오픈일이 연기될 수도 있습니다. 그러므로 관련된 공식 사이트나 새로운 정보를 확인해보시는 것이 좋을 것 같습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 19/40 [01:52<02:05,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  목포극장 개업일은 1996년 11월 23일. 1999년 11월 23일부터는 국내 최대의 연예 기획사인 에이브릴에스엠과 제휴를 체결하였고, 10월 1일부터 국내 최대의 연예 기획사인 에이브릴에스엠의 첫 콘서트를 개최했다. 콘서트 투어는 엔씨소프트가 제작한 프리서커스 공연을 비롯해 NHL에서 방영된 드라마, 영화, 게임쇼, 콘서트 투어 등이 있다. 팬들은 국내 최대의 연예 기획사인 에이브릴측에 가면 관람이 가능할 것이라고 믿고 있다. 또한 에이브릴의 홍보 매니저인 유노한도 이 회사의 공식석상에서 섭외 요청이 잇따르고 있다고 한다. 그러나 일각에서는 무료로 일반인을 대상으로 한 콘서트가 개최된 것은 명백한 불법이며, 불법적인 영업 행위가 이루어지는 경우는 법적 대응에 나선다. 《엔씨소프트, 런플릭》의 경우 무료 입장이다. 에이브릴의 광고주 중에는 엔씨소프트가 광고에서 제공하는 아이템을 판매하는 연예 기획사도 있어 일반인들을 대상으로 한 공연과 같은 콘서트는 사실상 불가능하다는 것이 밝혀져 있다. 대한민국의 영화 투자 배급사 《쎄씨엠》의 김성훈 대표는 《무한도전》에 출연해 국내 최고의 영화 스크린 스타로서 스크린 데뷔를 했다.\n",
      "Real:  '제가 알기로는 정확한 날짜를 알 수 없습니다. 재개발로 구조가 변경되고 리모델링 과정이 있을 경우 오픈일이 연기될 수도 있습니다. 그러므로 관련된 공식 사이트나 새로운 정보를 확인해보시는 것이 좋을 것 같습니다.\n",
      "Score: 0.001659\n",
      "\n",
      "src: 목포 상연 개업 일 은\n",
      "tgt: '제가 알기로는 정확한 날짜를 알 수 없습니다. 재개발로 구조가 변경되고 리모델링 과정이 있을 경우 오픈일이 연기될 수도 있습니다. 그러므로 관련된 공식 사이트나 새로운 정보를 확인해보시는 것이 좋을 것 같습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 20/40 [01:59<02:01,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  목포 상연 개업 일 은 7월 31일부터 6월 30일까지 이틀 동안 쉰다. 이것은 통상 상관의례와는 다른 풍경이며, 실제로 이 사관을 받은 적이 있는 경우, 상관과례가 없는 경우로서, 예를 들어 중국 황제가 사망했을 경우에는 상관과례 없이 무조건 사관을 받은 것으로 간주되는 것이 관례이다. 예를 들어, 황제가 사망한 경우 상관과례가 없는 경우, 상관이 있는 경우 상관이 없다는 사실만 확인되면 사관이 주어지지 않는 경우가 있을 수 있다. 일반적으로 상관이 없는 경우 상관은 이미 사망하였거나 사망한 경우라면 사관에 의하여 수여되는 경우가 대부분이다. 예를 들어, 황제가 사망한 경우 상관이 없는 경우도 상관이 없는 경우가 있으며 만약 상관이 없는 경우 상관이 사관을 주는 경우도 있다. 또한 상관이 없는 경우 상관이 없는 경우 상관이 없는 경우는 상관을 가지지 않거나, 상관이 없는 경우 상관이 없는 경우 등이다. 예를 들어, 황제가 사망한 경우 상관이 없다는 이유로 자살한 경우에는 상관이 없었을 수도 있으며, 이러한 경우는 상관이 있어도 사망한 경우와 상관이 없는 경우 모두 상관이 없다. 실제로 이 문서에 서술된 상관이 있다는 사실이 있는 경우, 상관이 없는 경우에는 상관과 상관이 없는 경우 상관이 없다는 사실만 확인하면 상관과 같은 이유로 사망하지 않았다는 사실만 확인해도 사망이상은 아니지만, 만약 상관이 없는 경우 상관이 없는 경우\n",
      "Real:  '제가 알기로는 정확한 날짜를 알 수 없습니다. 재개발로 구조가 변경되고 리모델링 과정이 있을 경우 오픈일이 연기될 수도 있습니다. 그러므로 관련된 공식 사이트나 새로운 정보를 확인해보시는 것이 좋을 것 같습니다.\n",
      "Score: 0.001401\n",
      "\n",
      "src: 바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?\n",
      "tgt: 교향곡. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▎    | 21/40 [02:05<01:55,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가? 샬리트 에 의해 씌어진 소설의 내용을 통해서 그는 소설의 '창조물'으로서 작가로서의 삶을 선택했음을 강조하였다. 괴테의 파우스트는 작가로서 그의 삶을 창조해 줄 수 있었기 때문에, 이 소설에서 작가는 괴테의 파우스트를 읽고 그 사실을 간파하여 그 내용을 읽어 내는 일만이 도구는 곧 소설로서 작가가 선택할 수밖에 없었다. 그러므로 파우스트는 그의 소설에 있는 중요한 부분만이 작가로서 그의 삶을 형성시키는 것이었다. 그래서 파우스트는 파우스트의 고백서를 읽으면서 이 소설에서는 주인공이 파우스트의 고백에 귀를 기울일 수밖에 없었을 것이다. 그러므로 이 소설에서의 주인공은 파우스트의 고백서를 읽으면서 자신이 소설의 주인공이라 할 수 있었던 것이다. 파우스트에 따르면 이 소설에서는 주인공이 \"괴테의 파우스트\"가 되는 것은 아무런 가능성이 없는 것처럼 묘사되었다. 그러나 파우스트의 고백서는 파우스트와 일면식이 없는, 파우스트가 고백서를 쓸 수 있다는 가능성을 열어 놓았다. 파우스트의 고백서는 일종의 하나의 소설로서 소설의 성격을 지니는 경우도 있으나, 이 소설 자체가 독자와 독자 사이에는 널리 영향을 주는 것이 아니라 오히려 작품 자체의 내용 속에 있는 불쾌함과 희열을 수반하는 것이었다. 그러므로 이 소설에서는 진정한 의미의 고백서가 작가들에게 얼마든지 도움을 주었을 것이다. 이 소설에서는 그 작가는 '자신의 고백\n",
      "Real:  교향곡. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000103\n",
      "\n",
      "src: 바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가?\n",
      "tgt: 1악장. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 22/40 [02:11<01:49,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너는 교향곡 작곡을 어디까지 쓴 뒤에 중단했는가? 공연에서의 연습을 위해 필요한 악보 연습과 악보 제작의 첫 단계가 완성될 때까지 미루고, 연습실에서 녹음한 악보집계를 마쳤다. 이 작업이 완성되는 데 걸린 시간은 불과 4시간 30분이나 되었다. 베토벤의 대교향곡 《비텔로 서곡》의 악보 연습이 완성되는 데 걸린 시간은 거의 3시간 30분이었고, 이 3시간 30분 동안에는 소나타 제1악장의 연주 악보도 완성되었다. 악보집계는 모두 약 1시간에 해당하는 1시간 30분이었고, 이는 베토벤 이전에 남긴 미완성 악보의 몇 초도 없이 1시간에 달하는 셈이다. 베토벤은 교향곡 《교향곡》을 완성한 직후 작품의 시간적 완성에 모든 악절이 완성되었고, 특히 교향곡이 완전히 완성될 때까지 미완성으로 남게 될 때까지 이 작업을 중단했고, 교향곡이 거의 동시에 완성되는 데 걸린 시간은 거의 7시간 30분이었다. 이런 상황에서 베토벤의 대교향곡 악곡의 완성과 그 연주가 절정에 달한 것은 작품의 완성 시간이었기 때문이다. 베토벤은 베토벤이 완성한 3시간 40분을 이 방법으로 처리하지 못했음을 지적한다. 4시간의 악보 작업과 1시간의 악보 작업 속에 베토벤은 교향곡 작곡을 최종 마무리한 후, 베토벤의 대교향곡 악장을 관현악기에 담아 《교향곡》으로 연주한 이후에 대교향곡을 완성한다. 베토벤\n",
      "Real:  1악장. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000070\n",
      "\n",
      "src: 바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가?\n",
      "tgt: 베토벤의 교향곡 9번. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▊    | 23/40 [02:17<01:42,  6.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너가 파우스트 서곡을 쓸 때 어떤 곡의 영향을 받았는가? 예를 들어 \"죽여줘서\"의 인용곡 \"A-D to Back\"은 음악 방송에서도 큰 반향을 일으켰지만 \"록과 멜로디\"는 음악 방송에서도 큰 인기를 끌었다. 그리고 이 작품이 음악 방송에서도 화제로 크게 인기를 얻자 음악 평론가 최성국은 \"현재까지도 인기가 많은 멜로디를 사용하다보니까 자연스럽게 그런 음악이 만들어진 것 같아요.\"며 \"소녀의 해쉬운 재미를 살리면서도 듣는 이를 배려하는 멜로디, 섹시함과 패티니가 모두 반영된 음악입니다.\"라고 말했다. 《인터뷰 : 나우누리》에서 \"록과 멜로디는 모두 들을 때 밀린다. 그 속에 담긴 소재들이 워낙 그 심도가 높아서, 조금만 들어도 귀에 아프다고 느꼈어요.\"라고 말했다. 《인터뷰 : 나우누리》가 평론가를 추천한 이유는 \"음악에서 어떤 곡의 영향을 받았나요?\"라는 질문에 \"그것은 대중성을 떠나서 음악이 좋아서 그런 음악이구나 싶어서 그런 음악이 아닐 수도 있습니다. 우리 평론에서는 이 곡이 대중적으로 인기를 끌 때 듣는 이들을 위로하면서, 혹은 듣게 되면서 자연스럽게 '대중성'의 효과를 크게 가지고 있다는 것죠. 그래서 이런 장르성을 가졌\n",
      "Real:  베토벤의 교향곡 9번. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000038\n",
      "\n",
      "src: 1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은?\n",
      "tgt: 파우스트. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 24/40 [02:23<01:36,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  1839년 바그너가 교향곡의 소재로 쓰려고 했던 책은? 이 책을 쓰는 출판업체의 홍보 자료였는데 이 책을 쓰는 출판업체의 홍보 자료를 참고해서 나온 것이었다. 책의 제목은 'AS184'이지만, 책의 원문은 이렇다. 책의 이름은 Johann Gorden, 1883년 9월호에 실려있다. 이 책의 제목은 'The English Symptical About the Billie White Book of The Sales'였다. 이 책은 이미 전 세계 여러 나라에 널리 알려져 있었으며, 세계 곳곳에 번역되어 많은 사람들이 읽게 되었다. <The English Symphony>이며, 영어 제목은 <Lady>였다. 이 책을 쓰기 위해 이 책의 서적은 영국 정부의 허가를 받았으며, 영국에서 먼저 출판되었다. 이 책은 《The English Symphony》로서, 제목은 <Lady to Show>로, 이 시기에 영어 교과서에 수록되지 못할 만한 책이다. 이 책은 주로 고전 시대 외국어를 교육시키는 것에 중점을 뒀다. 예를 들면, 중세 고전을 한글로 풀이하거나 원문을 한글로 번역하는 것으로 번역되어 있다. 또, 책의 제목을 \"Yes, not was understanded on universe\"로 바꾼다. 이 책은 전 세계 1940년에 발매되었지만, 현재는 대부분 출판되지 못했다. 이 책의 전 세계판은\n",
      "Real:  파우스트. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000067\n",
      "\n",
      "src: 파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?\n",
      "tgt: 합창교향곡. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 25/40 [02:29<01:30,  6.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  파우스트 서곡의 라단조 조성이 영향을 받은 베토벤의 곡은?이리네 집전곡. 1988년, 서곡을 담은 담은 음반 《The Boys Live》가 발표 된 후 그는 돌연 그의 추모곡으로 자신의 음반 작업을 진행하였는데 1990년 1월 3일 자신의 음반 Full Out을 발표하면서 음반의 수록곡이었던 \"I Kissed a Murderie\"를 다시 불렀고 2월 22일 음반과 함께 파리 음반 페스티벌에 참석, 공연하기도 하였다. 이 음반은 그의 베스트 음반으로 꼽히면서 한국 대중음악사에서 획기적인 사건으로 기록되었다. 이 음반은 그의 투어 이후 많은 사람들에게 소개 되었고 대중들에게는 사랑을 받았다. 특히 이 음반은 전 세계적으로 500만장 이상의 판매고를 올려 상업적으로 성공하여 전 세계적으로 음반이 차트 1위를 기록하고 한국 가수로는 최초로 3장의 앨범을 발표하며 명성을 얻게 되었다. 이러한 상업적 성공과 함께 마돈나는 그의 노래로써 대중들에게 강렬한 인상을 남기게 되었다. 이 음반은 첫 주 1,047,000만 장의 판매고를 올리며 성공을 거뒀고 마돈나의 《No World》로 불리는 성공과 동시에 한국 대중음악사에서 큰 사랑을 받게 되었다. 앨범의 수록곡 중 일부는 훗날 그의 히트곡인 “Joe Little Love Your Song\n",
      "Real:  합창교향곡. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000061\n",
      "\n",
      "src: 바그너가 파우스트를 처음으로 읽은 년도는?\n",
      "tgt: 1839. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 26/40 [02:35<01:24,  6.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너가 파우스트를 처음으로 읽은 년도는? 1860년에서 1851년까지로, 그리고 그 당시 유럽의 식민지였던 영국과 식민지 사이의 관계에 대해 조사하기 위해 1893년, 1895년에서 1883년까지 런던에서 영국-프랑스 연합 동맹군의 최고사령관이었던 찰스 E. 리히터(Harles M. Leeerttor) 장군의 지휘를 받으며 프랑스 침공에 참전했다. 리히터는 1881년 1월 26일 미국 육군으로 파리에 가서 영국군 사령관의 지휘 하에 연합군에 합류했다. 연합군 상륙 도중, 리히터는 그의 동료 장군인 윌리엄 F. 테일러에게 전보로 인해 부상을 당했고, 이후 즉시 그의 부하였던 찰스 E. 리히터가 그의 부대에 배속되었다. 리히터의 부대는 7월 12일 프랑스 육군 참모총장 앙리 카르탕에게 함락지점 부근에서 프랑스군에 합류했다. 이때 프랑스군과 동맹군 전군이 공격을 당했지만, 리히터는 그들을 격퇴시켰다. 이때까진, 영국군들은 서로 호각을 이루는 바람에 전신을 혼란에 빠뜨렸고, 이때쯤 그들은 연합군에 의해 격퇴당했다. 있어도, 독일군, 영국군은 제1차 세계 대전(1914년-1936년 사이에 이루어진 독일의 상륙작전) 전후, 연합군에게 포위되었고, 전신을 무차별소사하는 등 공세작전을 펼쳐 프랑스군을 상대했다. 이 전투는 오사와 맥키넬리, 기조스 베일스의 연합\n",
      "Real:  1839. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000035\n",
      "\n",
      "src: 바그너가 처음 교향곡 작곡을 한 장소는?\n",
      "tgt: 파리. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 27/40 [02:41<01:19,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너가 처음 교향곡 작곡을 한 장소는? 경주문화회관. 이 시기 가장 많은 수의 공연이 이루어진 곳은 경주에서도 벌어졌다. 경주의 가장 대표적인 축제는 성동문화회관( 경주시), 2014년 10월 14일에는 경주국악원의 특별공연 ‘경주천년의 나무: 한길소나타’가 첫 무대를 차지하고 12월 15일에는 ‘경주선양궁선수권대회’가 개최되었다. 2016년 1월 26일에는 대한민국 경주문화원 국제회의장에서 열린 '제10회 전국고용인대회'에서 우수상과 우수상을 수상하고 우승했다. 같은 해 12월 14일에는 한국프레스센터에서 진행된 '전국 고등학생 교내 콘서트 문화행진'에서 축하공연을 펼쳤는데, 300여 명이 공연을 펼쳐 1000만원의 예산을 가지고 공연에 초청되었다. 또 2016년 12월 26일에는 ‘신한카드 프로리그’의 신한카드 업무대행사(신한카드 MAC)에서 개최한 '신한카드M(m)'에서 최다출연자, 프로듀서, 우수상을 수상했다. 2016 전국 대학생 대상 '신한카드 MAC 올해의 신인부문'에서도 대상을 수상하였으며, 2017년 7월 15일 서울 여의도에서 열린 제 18회 전국민대회의 '신한카드 MAC 어워즈'에서 대상 수상을 하였다. MAC는 지난 2009년과 2012년에 열렸던 전국연합대학교에서의 '신한카드\n",
      "Real:  파리. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000022\n",
      "\n",
      "src: 바그너의 1악장의 초연은 어디서 연주되었는가?\n",
      "tgt: 드레스덴. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 28/40 [02:47<01:13,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너의 1악장의 초연은 어디서 연주되었는가? 한 연주회가 끝나고 나서도 다시 연주회가 열리게 된다. 그 연주회에 참석 중 나온 관객 중 몇 사람은 영화 속 여주인공인 김서연 역을 연기한 것으로 알려져 있다. 또한 그 후 나훈이 공연에서 솔로 무대를 위해 사용한 컷을 추려낸 영화《그 남자 좋다 슈퍼위키》가 개봉했다. 이 영화는 러닝타임 내내 평단과 극비점을 크게 깨뜨리는 엄청난 흥행과 많은 해외배우가 출력을 기록했으며, 특히 이 영화는 니콜 리치의 동명작을 기반으로 하고 있으며, 김서연은 극중 나훈과의 관계를 통해 현실성을 더해가며 영화의 내용 곳곳에 감칠맛을 입혀주었다. 이 영화는 세계 최고의 DVD 타이틀인만큼 김서연이 직접 감독을 맡았다. 김서연의 의상 속에 숨겨진 주인공은 나훈을 연기할 당시 입고 있던 의상과 함께 무대에 오르는 실내는 실제 나훈이 연기하는 퍼포먼스 보다는 배우의 열정과 카메라 앞에서 찍는 연기가 중심이 되었다. 김서연의 이 영화는 평단과 해외 독자들로부터 비난을 받았다. 이전 영화에서 김서연에게 노출된 의상이 의상과 일치한다는 주장도 있고, \"극중 패션과 헤어스타일에 관심이 많았었는데 이 영화의 의상이 실제\n",
      "Real:  드레스덴. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.\n",
      "Score: 0.000057\n",
      "\n",
      "src: 바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?\n",
      "tgt: 한스 폰 뷜로. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▎  | 29/40 [02:53<01:06,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가? 더 많은 기자들이 이 영화가 저서가 저의 의도대로 만든 것이라고 주장할 수 있을 만큼 충분히 인용된 것으로 보인다는 것이 일반적인 의견이다. 영화의 저서가 의도적인 대로 이루어진 것이라면 그것은 잘못된 것이며 따라서 저서가 잘못된 것이라는 것은 명백해 보일 수 있다. 그럼에도 불구하고 영화 비평의 한 저자인 로버트 그로스만은 영화 비평의 한 논픽션으로 이 영화가 잘못된 작품이라는 것을 입증할 수 있다고 말한다. 예를 들어, 이 책은 독자가 영화와 관련된 지식을 거의 모르는 상태에서 이 영화가 \"사실을 전혀 모르는 소설\"이라는 반증을 하기 어렵지 않나 하는 내용을 담고 있다는 것이다. 예를 들어, 영화 속에서 \"본편의 저자가 미리 알고 있었다\"는 것이 바로 그런 것이다. 이 책을 쓴 로버트 그로스만이 이 문제를 정면으로 제기한 바 있다. 더 많은 기자들이 이 책을 썼기 때문에, 그로스만이 작품의 의도적인 의도라고 주장하지 않는다. 물론 영화평론가의 입장에서 이 책이 의도적인 작품이라고 주장할 수 있지만 영화에 대한 정보가 충분하지 않아서 출판 당시 책의 내용만으로는 이 책이 출판되었다고 단정할 수 없으므로, 이 책을 출간할 당시부터 이 책을 쓰기 시작한 많은 언론들 및 언론사들이 이 책을 출간 이전에 승인한 것은 이 책 뿐만 아니라 이 책이 인용된 당시 저자인\n",
      "Real:  한스 폰 뷜로. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000328\n",
      "\n",
      "src: 잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?\n",
      "tgt: 리스트. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 30/40 [02:59<01:00,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가? 바그너가 바그너에 대해 갖는 태도를 어떻게 표현하는지 보여준다. 오페라에서는 이런 장면이 특히 자주 등장한다. 이 장면에서 원기옥 음악 평론가는 \"이것은 \"교향곡 제1번이 분명하고 제2번보다 더 많은 의미를 부여할 수 있다.\"라고 말한다. 원기옥 평론가는 \"여태껏 단순한 문법과 화법이 바흐만의 교향곡을 대부분 복제하고 있고, 심지어 교향곡이 완성되는 것처럼 묘사된다. 이 점 또한 바그너의 극화 발전 가능성에 시사된다. 바그너는 특히 바이로이트 교향곡을 해석, 해석, 사용함으로써 새로운 소재를 만들어낸다. 이러한 과정을 거친 뒤에 바그너는 이 장면에서 만족한다. \"크리스티누와 파브리치오 같은 작곡가도 이 작품이 매우 잘 나타나 있다\"고 말했고, 다른 평론가는 이것을 '역사와의 유일한 차이'이라고 지적한다. 바그너가 바그너에 대해 \"제2번이 하는 말, 베토벤을 연상시키는 '축축하고 부드러운 가을의 아리아'\"를 내포한다. 이와 비슷한 비유인 '잇말'도 있다. \"독일 민요에서 한 번도 이 노래의 악보를 사용한 적이 없는\" 바그너가 \"바이로이트\"라고 부르는 것은 이를테게 닮은 말이다. '잇말'은 이전에 오페라에서의 서곡 1악의 거의\n",
      "Real:  리스트. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000150\n",
      "\n",
      "src: 바그너는 다시 개정된 총보를 얼마를 받고 팔았는가?\n",
      "tgt: 20루이의 금. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 31/40 [03:05<00:54,  6.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  바그너는 다시 개정된 총보를 얼마를 받고 팔았는가? 《가짜 천조》. 《가짜 천조》는 여러 잡지에서 언급되지만, 가짜 천조 시리즈를 다룬 가상의 작품은 아니다. 《가짜 천조》의 제목은 대체로 이 영화에서 따온 것이다. 제작 단계에서 이미 《가짜 천조》의 줄거리는 실현되고 있었으며, 가짜 천조라는 제목의 대중 잡지 《가짜 천조》의 줄거리에 오마르의 아들이 등장한다. 그 아들이 죽은 뒤 닥터가 다시 장면과 함께 이야기하도록 이끌어 가자, 이 둘 사이에는 딸이 있다. 그러나 이 딸도 오마르의 오빠의 사랑을 한 수풀에 두고 내팽개치는 식으로 천수를 놓는다. 그 와중에 로이는 딸에게 자신이 아들을 사랑하고 있다는 것을 주지시키고, 그 딸인 정말로 자신의 딸이 자신의 딸이 당신에게 자신처럼 \"진정한 자신의 딸\"일 수도 있다는 것을 알게 된다. 로이는 딸을 지키기 위해 닥터의 딸 정말로 자신이 딸 자신보다 먼저 죽은 줄도 모른다. 하지만 닥터는 이것을 숨기려고 그녀에게 아들의 말을 들려주고, 닥터와 함께 이야기하면서 자신이 딸의 정체를 밝히고 딸에게 자신의 딸이 진심임을 밝히고는 이야기를 멈추게 한다. 이 이야기는 닥터가 다시 오마르의 친구인 한 소녀와의 우정을 통해 사랑을 키워 준 마지막 계기와도 이어지는 것이다.\n",
      "Real:  20루이의 금. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000214\n",
      "\n",
      "src: 파우스트 교향곡을 부활시킨 사람은?\n",
      "tgt: 리스트. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 32/40 [03:11<00:48,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  파우스트 교향곡을 부활시킨 사람은? 오스카. 그 후 몇 달 지나서야 작곡할 수 있는 작품이 등장한다. 특히 교향곡에 수록된 곡은 C장조 1절에 나타난다. 베토벤의 마지막 교향곡이면서 그 시대의 바그너나 후기협곡, 그리고 제2악장의 주제 등이 수록된 작품이다. 이 작품이 완성되는 것은 교향곡 1절의 구조를 변경한 것이다. 또한 제2악장의 주제가 되는 주제로는 이미 그가 밝힌 바 있듯이, 제2악장의 주제 역시 바뀌어 있다. 이것을 교향곡으로 분명하게 표현하고자 하는 이유는, 교향곡은 형식적으로 첫째가 형식을 취하고 둘째가 형식을 취하고 그것들이 주제로서의 기능을 하기 때문이다. 이러한 연유로 제2악장 형식에 있어서도 제2악장이 불가피한 형식으로써 형식 그대로 제2악장을 사용할 수 있는 작품이라는 생각을 하게 만들 수 있는 작품의 난이도가 높아진 것이다. 이 작품에 대해 다시 한 번 교향곡으로 표현할 수 있는 주제로서는, 이 작품을 통해서 베토벤과 그 형식에 나타나는 바그너의 영향이 매우 강조되고 있는 것이 아닐 수 없다. 그럼에도 불구하고 제1악장은 작품의 성격을 상당히 바꾸게 된다. 베토벤은 제3악장을 통해서 그 당시의 그의 교향곡이 주제를 크게 바꾸어 놓게 된다. 그러므로 이 작품이 주제의 성격을 분명하게 바꾸게 되는 작품은 다름없이, 이 작품이 교향곡의 성격을 분명하게 반영하고 있다는 것을 알 수 있다. 이 작품이 제2악장으로도 사용될 수 있는 작품은\n",
      "Real:  리스트. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000311\n",
      "\n",
      "src: 파우스트 교향곡을 피아노 독주용으로 편곡한 사람은?\n",
      "tgt: 한스 폰 뷜로. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▎ | 33/40 [03:17<00:42,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  파우스트 교향곡을 피아노 독주용으로 편곡한 사람은? 김호진(金皓). 그의 바이올린을 처음 연주한 것은 1969년 봄. 그는 교향곡의 \"박동수는 나의 인생에서 한 번도 연주한 적이 없는 나의 천재성을 깊이 느끼게 되는\" 기회였다 하면서, 이것이 바로 \"박동수는 내가 피아노를 연주할 수 있는 기회를 갖고 있는가에 대하여 확신이 안 된다.\"고 하였다. 김호진(金浩)은 그가 피아노를 독주용으로 편곡한 것에 대해서, \"교향곡을 가장 먼저 연주하고 나서 박동수는 기타를 연주하는 것이 내 자신의 흥미를 끌게 된다.\"고 하였다. 이 작품은 제51회 그래미 어워드에서 최고의 보컬리스트에게 수여되며, 이는 2007년 이달의 신인 음반 표지를 장식하게 된다. 김호진은 또한 자신이 대학원에서 \"박동수의 천재성은 베토벤의 낭만적인 오페라에 나타나는 깊은 고뇌에 있는 결말이다.\"고 믿었다. 2007년 초, 김호진은 한 인터뷰에서 \"그냥 피아노만 있다면 내가 바이올린을 연주하는 것은 꿈에 불과할 것 같다.\"고 자신의 트위터를 통해 밝히기도 했다. 김호진의 바이올린 연주 실력에 대한 평가는 여러 가지 것이다. 김기덕(金己德)은 \"훌륭한 기량은 있어도 피아노는 나에게 맞지 않는다. 내 작품이 굉장히 어렵고 저돌적이다.\"라고 하였다. 김기덕, 김기\n",
      "Real:  한스 폰 뷜로. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000162\n",
      "\n",
      "src: 1악장을 부활시켜 연주한 사람은?\n",
      "tgt: 리스트. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 34/40 [03:24<00:36,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  1악장을 부활시켜 연주한 사람은??? 유태형. 음악을 들을 때면 악장 뒤에 선주곡이 적혀있으니까 들어가서 악보를 읽으면 악보를 읽지 않고는 무용지물에 불과하다. 악기 상자가 접을 수 있는 곳에 갖다놓고 현악기를 연주하는 것이 좋다. 악보에서 주, 음(때)을 불러오는데, 악보를 들었을 때 가장 먼저 나오는 악보가 바로 '주', '주오'라고 되어 있다. 그러나 악보에는 연주된 악보에서 '주'와 '주오'로 오류가 있을 수 있고, '주오'의 경우 연주된 악보에서 '주오', '주오'로 오류가 있다고만 쓰여 있다. 악보에서 주, 음(주)의 쉼을 해주어 주오라는 소리가 나오지만, 악보에는 주오라고만 쓰여 있는 경우가 많은데, 악보에는 '주오'가 반드시 적혀 있지 않아 그것을 오류로 처리하는 방법이 있다. 예를 들어, 악보에서 '알토 보컬'로 시작하는 것이 주오인데 그것이 알토 보컬이 되어 이음의 '알토'가 될 수도 있다. 이 소리는 현악기와는 다르게 '토'나 '알토' 대신 '어반 멜로디'를 조합하는 것이다. 주오 혹은 '알토'와 '주오'의 차이를 바로 주워서 '이것을 잘 알겠다' 싶으면 다른 악기로\n",
      "Real:  리스트. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000105\n",
      "\n",
      "src: 파우스트 교향곡에 감탄하여 피아노곡으로 편곡한 사람은?\n",
      "tgt: 한스 폰 뷜로. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 35/40 [03:30<00:30,  6.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  파우스트 교향곡에 감탄하여 피아노곡으로 편곡한 사람은? 김규식. 김규식이 피아노를 통해 접하는 교향곡 6번의 경우, 이미 녹음한 곡이 아닌데다가, 이미 녹음한 곡이 아니어서 피아노곡을 할 수 없고, 악보대로 녹음한 이후에 피아노곡을 다시 시작한다는 점이 약점이라는 평이 있다. 김규식은 자신이 좋아하는 단조의 명곡은 아니지만 일단 보름달의 여정 끝에 만들었다는 생각을 갖고 있었다. 피아노곡으로는 처음으로 시도한 곡이 바로 제7번인데 이 곡이 바로 그런 것이다. 피아노로만 구성된 제1부는 제1부, 후반부에서 연주되는데, 제1부부터 소나타 형식이라는 전례에 따라 제2부가 주제인 '가장 깊은 감춰둔 아픔과 아픔, 그리고 그것은 결코 제1부, 제2부, 제3부로 가는 것이 아니라 새로운 것을 창조하고 있는 것이기 때문이다.'는 점에서 이 부분은 지금까지의 독보적인 작품으로서도, 이번 작품들은 '단조'의 명곡으로 평가된다. 피아노로만 구성된 제1부, 제2부는 형식적으로나 내용적으로 모두 교향곡과 가장 가까운 점이며 또한 제1부와의 관계는 이후의 작품 안에서 상당히 중요한 부분이라 할 수 있다. 그리고 이미 음악적인 완성도에 있어서도 이 작품이 매우 잘 나타나 있는데, 이것은 지금까지의 피아노 작품 속에서 거의 느껴지지 않는 또 하나의 명곡임을 반증하는 것으로 생각할 수 있다. 김규식은 작사 경험이 거의 없는 곡\n",
      "Real:  한스 폰 뷜로. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000211\n",
      "\n",
      "src: 리스트가 바그너와 알게 된 연도는?\n",
      "tgt: 1840년. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 36/40 [03:36<00:24,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  리스트가 바그너와 알게 된 연도는?? '저는 가명인입니다. \"제가 알기로는 바그너가 1627년에 쓴 번역가였습니다. \\n\\n- 바그너가 1661년에 쓴 책을 출판하면서, 바그너가 바그너와 함께 작업한 적이 있는지에 대한 정보가 없습니다. 이 책은 바그너와 함께 작업한 것으로 알려져 있습니다. 저는 가명인 All Berry Sonnede Gerramn가 쓴 책이며, 그 책은 바그너가 1627년에 출판한 책일 것으로 추정됩니다. 이 책을 통해 바그너와 관련된 모든 연도가 알려져 있습니다. 제가 알기로는, \"제가 알기로는 바그와 조이, 에제키, 비올라와 같이 바그너와도 잘 알려진 독일인 작곡가였습니다.... 제가 알기로는 바그와 조이가 바그너와 함께 작업한 책은 아직 없습니다. \"제가 알기로는, \"가베르크가 1629년에 쓴 번역가였습니다.... 이 책은 바그너와 처음으로 만난 이야기이며, 그가 죽은 후 많은 것을 다루도록 도와드리고 있습니다.\"라고 말하였으며, 나중에 \"제가 알기로는, \"저는 바그너와 함께 작업한 적이 있지만, 지금은 제가 그 이전의 이야기를 가지고 있지는 않습니다.\" 라고 답변하였습니다. \"저는 바그너와 조이가 살았던 시대와 그 시대에 대해 대화를 나누고 있습니다....\n",
      "Real:  1840년. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.\n",
      "Score: 0.000141\n",
      "\n",
      "src: 서주에는 무엇이 암시되어 있는가?\n",
      "tgt: 주제, 동기. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▎| 37/40 [03:42<00:18,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  서주에는 무엇이 암시되어 있는가? 만약에 어떤 형태로든 그런 형태를 가지고 있을 수 있다는 점에서 이 점은 우려스럽다고 볼 수 있습니다. 하지만 그것이 실제로 이루어지려면 시간이 많이 걸릴 뿐 아니라, 실제로 그것이 이루어지기 위해서는 상당한 시간이 필요한데 이를 어떻게 견인할 수 있을까 하는 문제가 있습니다. 추가적인 정보가 필요하신 것 같습니다. 더 자세한 내용 없이, 질문을 좀 더 구체적으로 말씀해주시면 답변드릴 수 있을 것 같습니다. 이 점이 우려스럽다는 점, 또 이 부분에서 우려스럽다는 점, 또 많은 사람들이 이 부분을 간과해서는 안 된다고 생각합니다. 어떤 형태로든 암시적인 형태로라도 이런 형태를 보여주지 않는다면 이것이 실제로 이루어지기 위해서는 상당한 시간이 필요한 것으로 볼 수 있습니다. 가능성이 있다고 생각할 수 있지만, 그 형태가 현재로서는 과연 어떻게 해왔고 발전단계에 있을지도 아직은 불확실합니다. 이에 대한 대비가 필요합니다. 이 부분이 일어나지 않는다면 이 역시 간과해서는 안 될 것입니다. 무언가 가능성이 있기 위한, 즉 실제로 이루어지고 있는 것이 실제로 이루어지는 것이 아니라 그 과정에서 벌어들이는 것이 많거나 아니면 그것을 보다 더 큰 가능성을 향해서 이루어지는 경우도 있으며, 그런 가능성이나 가능성이 있는 부분이 앞으로 만들어지기를 기다리고 있을 거라는 의견입니다. 제가 알기로는 이 부분도 걱정스러운 점은 이 부분이 이 부분을 너무 심각하게 생각하지 못한 부분\n",
      "Real:  주제, 동기. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n",
      "Score: 0.000412\n",
      "\n",
      "src: 첫부분에는 어떤 악기를 사용해 더욱 명확하게 나타내는가?\n",
      "tgt: 제1바이올린. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 38/40 [03:48<00:12,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  첫부분에는 어떤 악기를 사용해 더욱 명확하게 나타내는가? <음악적인 소리>. 《음악적인 소리》(Narrvery Girlf, VGQ). VGQ(Union Go Moment)는 \"현악기\"를 의미하는 VGA(Gripling Gain) 혹은 VGA (영어: Jumping Gain) 와, 또는 VGA는 \"음악적인 소리\"와 \"음악적인 소리를 포함하는 말들을 의미한다. 예를 들어 GMT는 바이올린의 사운드 오브 비디 (다른 비트를 듣는 듯한) 소리를, GMT는 현악기의 사운드를 연주하여 연주하는 사운드를 말한다. (여기에서는 \"음악적인 소리\"에 관련된 \"음악적인 소리\"를 포함하고 있다:) 이 노래에서 가장 많은 노래들이 사용되었다. 이 노래의 대부분을 차지하는 것은 GMT 연주자와 현악기의 파노라마 비디오는 GMF (피아노)와 같은 현악기의 사운드 오브 비디오는 물론 신디사이저를 연결하거나 기타에 붙이는 사운드를 사용한다. 음반의 주요 엔진, 보컬과 주요 음원을 사용한다. 또한 악기의 세 종류, 곡들을 추가하고 기타들을 사용하여 저스틴 비욘세, 퀸의 〈Cant\n",
      "Real:  제1바이올린. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n",
      "Score: 0.000087\n",
      "\n",
      "src: 주요부는 어떤 형식으로 되어 있는가?\n",
      "tgt: 소나타 형식. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 39/40 [03:54<00:06,  6.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  주요부는 어떤 형식으로 되어 있는가? '뉴스보도. 뉴스보도를 통해 일부 신문사와 기사들은 이 내용을 사실이라고 하는데, 이는 대부분의 신문사가 독자들을 대상으로 기사를 제공하는 방식이며, 이 과정에도 언론사의 기사 지면과 타 기사가 노출되는 것으로 밝혀졌다. 조선일보는 '뉴스보도를 이용해 특정 언론사 기자의 입장을 파악하거나 정보를 수집했다. 일부 언론사 기사와 기자는 기사 자체를 보도하는 반면, 일부 언론사는 자체적으로 기사 작성 및 삭제, 삭제 등을 하는 것으로 밝혀졌다. 이를 통해 특정 언론사는 해당 기자의 기사 삭제와 삭제를 요구할 수도 있다. 해당 기사와 기자는 대부분 인터넷 미디어 계정이지만 뉴스 기사는 자체적으로 자체적으로 기사를 제공하고, 일부 언론사는 이 서비스를 거부하기도 한다. 예를 들어 일부 신문사는 기사와 기사에 오보되는 기사를 링크하는 작업을 진행하는데, 해당 기자실에서 기사 공급 여부를 확인하는 과정에서 기사 공급 사실을 확인하고자 한다거나 기사와의 관계 여부를 묻는 방식으로 취재하는 기자들이 기사 공급업체에서 기사를 제공할 수 있다. 다른 언론사들은 이에 포함되지 않는다. 조선일보와 미디어오늘은 모두 기자실 이외 모집을 차단하지 않고 있어 언론사들은 이 기사를 제공하고 있지만 대부분 기사 자체는 독자들을 대상으로 하는 경우가 대부분이라고 보도했다. 일부 언론사나 기자들만 해당 기사에\n",
      "Real:  소나타 형식. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n",
      "Score: 0.000275\n",
      "\n",
      "src: 첫 부분의 주요주제를 암시하는 주제는?\n",
      "tgt: 저음 주제. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [04:00<00:00,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Prediction:  첫 부분의 주요주제를 암시하는 주제는? '적대적인 태도'라는 것이 일반적인 주제이며, 이는 사회 전체에 동일하게 적용되어 있는 주제가 아니다. 이는 사회 전체에 걸쳐 해당된다는 의미를 가지고 있다. 이러한 소재적 특성은 사회 전체에 걸쳐 그 소재적 성격이 적용되기 때문에 이 주제의 주제를 포함한 다양한 소재나 소재를 사용하여 표현하거나 직접적으로 전달해야 할 필요가 있다. 또한 소재적 특성이 사용된다면 그 소재나 제문 등이 사회 전체에 걸쳐 동일한 의미를 가지도록 의도적인 요소를 포함해야 하며, 이러한 소재나 소재의 사용은 다른 사회 전체의 문맥에서 동일한 의미를 전달하는 하나의 수단이 된다. 이 점에서 주제적 성격에서는 대상이나 소재, 제문 등에 사용되는 다양한 소재나 소재가 동일할 수 있지만, 그것이 서로 밀접한 관련을 지니지 않고 다른 소재의 혼합물로 사용될 수도 있다. 소재적 특성이 사용되지 않고 다른 소재로 쓰일 경우 그 소재는 다양하다. 예를 들어 대중이 관심을 가질 수 있는 특정한 형태의 사건이 전개될 경우, 그것을 계기로 주변이 관심하게 되는 경우가 많으며, 이것을 계기로 사회 전체가 주목 받게 되는 것이다. 주제적 성격이 사용된 이유로도 다른 사람들이 그들의 소재를 이용해 정치적 목적을 이루기 때문이고, 또한 소재를 소재로 사용했든 그렇지 않든 간에 그것을 소재로 사용할 수 있다. 소재적 특성이 사용된 예로는 주로 특정 주제에 대한 소재적 해석이 적용된 경우가 있다. 예를 들어 'A-B-B-A'이나 'B\n",
      "Real:  저음 주제. 이 작품은 라단조, Sehr gehalten(아주 신중하게), 4/4박자의 부드러운 서주로 서주로 시작되는데, 여기에는 주요 주제, 동기의 대부분이 암시, 예고되어 있다. 첫 부분의 저음 주제는 주요 주제(고뇌와 갈망 동기, 청춘의 사랑 동기)를 암시하고 있으며, 제1바이올린으로 더욱 명확하게 나타난다. 또한 그것을 이어받는 동기도 중요한 역할을 한다. 여기에 새로운 소재가 더해진 뒤에 새로운 주제도 연주된다. 주요부는 Sehr bewegt(아주 격동적으로), 2/2박자의 자유로운 소나타 형식으로 매우 드라마틱한 구상과 유기적인 구성을 하고 있다. 여기에는 지금까지의 주제나 소재 외에도 오보에에 의한 선율과 제2주제를 떠올리게 하는 부차적인 주제가 더해지는데, 중간부에서는 약보3이 중심이 되고 제2주제는 축소된 재현부에서 D장조로 재현된다. 마지막에는 주요 주제를 회상하면서 조용히 마친다.\n",
      "Score: 0.000352\n",
      "\n",
      "Num of Sample: 40\n",
      "Total Score: 0.000929809367156883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_score = eval_bleu(tmp_ds[22085:22125], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f712a73",
   "metadata": {},
   "source": [
    "> Model Prediction:  바그너가 처음 교향곡 작곡을 한 장소는? 경주문화회관. 이 시기 가장 많은 수의 공연이 이루어진 곳은 경주에서도 벌어졌다.\n",
    "\n",
    "- 이거 뭐지? 데이터셋 오류인가?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e5386b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'prompt': '바그너가 파우스트를 처음으로 읽은 년도는?',\n",
       "  'completion': '1839. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'},\n",
       " {'prompt': '바그너가 처음 교향곡 작곡을 한 장소는?',\n",
       "  'completion': '파리. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'},\n",
       " {'prompt': '바그너의 1악장의 초연은 어디서 연주되었는가?',\n",
       "  'completion': '드레스덴. 1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.'},\n",
       " {'prompt': '바그너의 작품을 시인의 피로 쓰여졌다고 극찬한 것은 누구인가?',\n",
       "  'completion': '한스 폰 뷜로. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.'},\n",
       " {'prompt': '잊혀져 있는 파우스트 서곡 1악장을 부활시킨 것은 누구인가?',\n",
       "  'completion': '리스트. 한편 1840년부터 바그너와 알고 지내던 리스트가 잊혀져 있던 1악장을 부활시켜 1852년에 바이마르에서 연주했다. 이것을 계기로 바그너도 이 작품에 다시 관심을 갖게 되었고, 그 해 9월에는 총보의 반환을 요구하여 이를 서곡으로 간추린 다음 수정을 했고 브라이트코프흐 & 헤르텔 출판사에서 출판할 개정판도 준비했다. 1853년 5월에는 리스트가 이 작품이 수정되었다는 것을 인정했지만, 끝내 바그너의 출판 계획은 무산되고 말았다. 이후 1855년에 리스트가 자신의 작품 파우스트 교향곡을 거의 완성하여 그 사실을 바그너에게 알렸고, 바그너는 다시 개정된 총보를 리스트에게 보내고 브라이트코프흐 & 헤르텔 출판사에는 20루이의 금을 받고 팔았다. 또한 그의 작품을 “하나하나의 음표가 시인의 피로 쓰여졌다”며 극찬했던 한스 폰 뷜로가 그것을 피아노 독주용으로 편곡했는데, 리스트는 그것을 약간 변형되었을 뿐이라고 지적했다. 이 서곡의 총보 첫머리에는 파우스트 1부의 내용 중 한 구절을 인용하고 있다.'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_ds[22085+25:22085+30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f385fa7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41489: 인력과 예산부족. 2002년 11월 4일 경주발전협의회(회장 황윤기)는 경주의 국무총리 산하 문화특별시 지정을 골자로 성타(性陀) 불국사 회주스님, 백상승 경주시장, 이진구 경주시의회 의장, 진병길 신라문화원장을 비롯해 경주시의원 18명 등이 연대서명한 ‘경주 문화특별시 승격 건의서’를 각 정당 16대 대선후보들에게 전달했다. 경주발전협의회는 건의서에서 “불국사 등 세계문화유산으로 지정된 2개 지구를 비롯 296점의 문화재를 보유하고 있는 한국의 대표적 문화관광도시인 경주가 인력과 예산부족으로 도굴과 훼손에 무방비로 노출되어 있는 실정”이라며 “이같은 문제를 근본적으로 해결하기 위해서는 특별시 지정과 함께 이미 국회에 발의되어 있는 고도보존법의 조속한 통과가 필수”라고 주장했다. 특히, “세계각국은 문화재 관리로 인해 시민들이 입는 피해를 최소화 하는 법률을 제정해 문화재 보호와 사유재산권 행사를 조화롭게 해결해 오고 있는 추세”라며 “지금까지 문화재 보존을 이유로 시민의 일방적인 희생만을 강요당해온 지금 시민피해를 최소화 할 수 있는 방안이 강구되어야 한다”고 경주발전협의회는 밝혔다. 경주발전협의회 황윤기 회장은 “경주문화특별시 지정이 대선공약에 포함될 수 있도록 각 정당을 순회방문할 계획”이라고 밝혔다.\n",
      "41490: 경주발전협의회 황윤기 회장. 2002년 11월 4일 경주발전협의회(회장 황윤기)는 경주의 국무총리 산하 문화특별시 지정을 골자로 성타(性陀) 불국사 회주스님, 백상승 경주시장, 이진구 경주시의회 의장, 진병길 신라문화원장을 비롯해 경주시의원 18명 등이 연대서명한 ‘경주 문화특별시 승격 건의서’를 각 정당 16대 대선후보들에게 전달했다. 경주발전협의회는 건의서에서 “불국사 등 세계문화유산으로 지정된 2개 지구를 비롯 296점의 문화재를 보유하고 있는 한국의 대표적 문화관광도시인 경주가 인력과 예산부족으로 도굴과 훼손에 무방비로 노출되어 있는 실정”이라며 “이같은 문제를 근본적으로 해결하기 위해서는 특별시 지정과 함께 이미 국회에 발의되어 있는 고도보존법의 조속한 통과가 필수”라고 주장했다. 특히, “세계각국은 문화재 관리로 인해 시민들이 입는 피해를 최소화 하는 법률을 제정해 문화재 보호와 사유재산권 행사를 조화롭게 해결해 오고 있는 추세”라며 “지금까지 문화재 보존을 이유로 시민의 일방적인 희생만을 강요당해온 지금 시민피해를 최소화 할 수 있는 방안이 강구되어야 한다”고 경주발전협의회는 밝혔다. 경주발전협의회 황윤기 회장은 “경주문화특별시 지정이 대선공약에 포함될 수 있도록 각 정당을 순회방문할 계획”이라고 밝혔다.\n",
      "41491: 경주발전협의회. 2002년 11월 4일 경주발전협의회(회장 황윤기)는 경주의 국무총리 산하 문화특별시 지정을 골자로 성타(性陀) 불국사 회주스님, 백상승 경주시장, 이진구 경주시의회 의장, 진병길 신라문화원장을 비롯해 경주시의원 18명 등이 연대서명한 ‘경주 문화특별시 승격 건의서’를 각 정당 16대 대선후보들에게 전달했다. 경주발전협의회는 건의서에서 “불국사 등 세계문화유산으로 지정된 2개 지구를 비롯 296점의 문화재를 보유하고 있는 한국의 대표적 문화관광도시인 경주가 인력과 예산부족으로 도굴과 훼손에 무방비로 노출되어 있는 실정”이라며 “이같은 문제를 근본적으로 해결하기 위해서는 특별시 지정과 함께 이미 국회에 발의되어 있는 고도보존법의 조속한 통과가 필수”라고 주장했다. 특히, “세계각국은 문화재 관리로 인해 시민들이 입는 피해를 최소화 하는 법률을 제정해 문화재 보호와 사유재산권 행사를 조화롭게 해결해 오고 있는 추세”라며 “지금까지 문화재 보존을 이유로 시민의 일방적인 희생만을 강요당해온 지금 시민피해를 최소화 할 수 있는 방안이 강구되어야 한다”고 경주발전협의회는 밝혔다. 경주발전협의회 황윤기 회장은 “경주문화특별시 지정이 대선공약에 포함될 수 있도록 각 정당을 순회방문할 계획”이라고 밝혔다.\n",
      "41492: 296점. 2002년 11월 4일 경주발전협의회(회장 황윤기)는 경주의 국무총리 산하 문화특별시 지정을 골자로 성타(性陀) 불국사 회주스님, 백상승 경주시장, 이진구 경주시의회 의장, 진병길 신라문화원장을 비롯해 경주시의원 18명 등이 연대서명한 ‘경주 문화특별시 승격 건의서’를 각 정당 16대 대선후보들에게 전달했다. 경주발전협의회는 건의서에서 “불국사 등 세계문화유산으로 지정된 2개 지구를 비롯 296점의 문화재를 보유하고 있는 한국의 대표적 문화관광도시인 경주가 인력과 예산부족으로 도굴과 훼손에 무방비로 노출되어 있는 실정”이라며 “이같은 문제를 근본적으로 해결하기 위해서는 특별시 지정과 함께 이미 국회에 발의되어 있는 고도보존법의 조속한 통과가 필수”라고 주장했다. 특히, “세계각국은 문화재 관리로 인해 시민들이 입는 피해를 최소화 하는 법률을 제정해 문화재 보호와 사유재산권 행사를 조화롭게 해결해 오고 있는 추세”라며 “지금까지 문화재 보존을 이유로 시민의 일방적인 희생만을 강요당해온 지금 시민피해를 최소화 할 수 있는 방안이 강구되어야 한다”고 경주발전협의회는 밝혔다. 경주발전협의회 황윤기 회장은 “경주문화특별시 지정이 대선공약에 포함될 수 있도록 각 정당을 순회방문할 계획”이라고 밝혔다.\n",
      "41493: 경주문화특별시. 2002년 11월 4일 경주발전협의회(회장 황윤기)는 경주의 국무총리 산하 문화특별시 지정을 골자로 성타(性陀) 불국사 회주스님, 백상승 경주시장, 이진구 경주시의회 의장, 진병길 신라문화원장을 비롯해 경주시의원 18명 등이 연대서명한 ‘경주 문화특별시 승격 건의서’를 각 정당 16대 대선후보들에게 전달했다. 경주발전협의회는 건의서에서 “불국사 등 세계문화유산으로 지정된 2개 지구를 비롯 296점의 문화재를 보유하고 있는 한국의 대표적 문화관광도시인 경주가 인력과 예산부족으로 도굴과 훼손에 무방비로 노출되어 있는 실정”이라며 “이같은 문제를 근본적으로 해결하기 위해서는 특별시 지정과 함께 이미 국회에 발의되어 있는 고도보존법의 조속한 통과가 필수”라고 주장했다. 특히, “세계각국은 문화재 관리로 인해 시민들이 입는 피해를 최소화 하는 법률을 제정해 문화재 보호와 사유재산권 행사를 조화롭게 해결해 오고 있는 추세”라며 “지금까지 문화재 보존을 이유로 시민의 일방적인 희생만을 강요당해온 지금 시민피해를 최소화 할 수 있는 방안이 강구되어야 한다”고 경주발전협의회는 밝혔다. 경주발전협의회 황윤기 회장은 “경주문화특별시 지정이 대선공약에 포함될 수 있도록 각 정당을 순회방문할 계획”이라고 밝혔다.\n"
     ]
    }
   ],
   "source": [
    "for index, item in enumerate(tmp_ds):\n",
    "    if '경주문화' in item['completion']:\n",
    "        print(f\"{index}: {item['completion']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf9887cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_tmp = pd.DataFrame(tmp_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "93766d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [prompt, completion]\n",
       "Index: []"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tmp[df_tmp['completion'].str.contains('경주문화회관')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a549b86",
   "metadata": {},
   "source": [
    "- 뭐지???? 데이터셋 안에 보이지 않는다.\n",
    "- '경주문화회관'이 데이터셋에는 없는 것 같은데, 설마...만들어진 문장인건가????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2997cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03719237468627532\n"
     ]
    }
   ],
   "source": [
    "print(total_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86832ca8",
   "metadata": {},
   "source": [
    "BLEU score\n",
    "- 0.037 로 형편없는 값이다.\n",
    "- 추가 데이터셋을 정제하지 않아서, 질문 내용도 포함되어 있다. \n",
    "- 정답만 포함하기 보다는 맥락도 포함하는 게 더 나을 것 같아서 그랬지만, 질문에 대한 답변을 하지않고 구구절절 맥락을 설명하게 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e7e75",
   "metadata": {},
   "source": [
    "# 평가문항"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae5b19",
   "metadata": {},
   "source": [
    "1. 기존 데이터셋을 추가 정제하고, generation 성능을 끌어올리기 위한 기법들을 실험해 모델 perfomance를 향상시켜보았는가?\t\n",
    "  - 기존 데이터셋의 문제점을 분석하고 전처리 전략을 수립해 추가 정제를 진행했다. Beam search, Top-k(p) sampling 등 최선의 디코딩 전략을 수립해 향상된 모델 추론 결과를 제시했다. BLEU, ROUGE 등 생성된 텍스트를 평가하기 위한 메트릭을 적용한 정량적인 평가 결과와 주관적인 평가를 비교분석하였다.\n",
    "2. 새로운 데이터를 수집해 전처리를 수행하여 모델을 재학습시켜보았는가?\t\n",
    "  - 모두의 말뭉치, AI hub 등에 공개된 데이터를 사용해 추가 데이터셋을 구축하기 위한 기준과 근거를 수립했다. ChatGPT API나 다양한 한국어 benchmark 데이터셋을 활용해 Human Feedback 을 대체할 수 있는 아이디어를 구현했다. 위를 바탕으로 SFT, RM, PPO 세 단계에 필요한 각 데이터셋을 적절히 구축하여, 모델 추론 결과와 수립한 가설을 비교해보았다.\n",
    "3. 학습 전략 또는 foundation model을 변경해 모델을 재학습시켜보았는가?\t\n",
    "  - 더 적절한 Instruction Tuning 기법을 적용해 SFT를 해보거나, Reward Model의 ranking algorithm을 개선해보았다. KoGPT-2가 아닌 다른 모델을 initial model로 사용하여 모델 학습을 성공시켰다. 허깅페이스의 accelerate, bitsandbytes 라이브러리 등을 사용하여 더 큰 스케일의 모델로 ChatGPT를 re-building해 모델 성능을 향상시켰다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afa2db6",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ab5ae9",
   "metadata": {},
   "source": [
    "- OOM \n",
    "  - 이것 때문에 시간을 너무 많이 소모했다.\n",
    "  - 한번 발생하면 gc.collect(), torch.cuda.empty_cache() 로도 메모리 초기화가 되지 않는다.\n",
    "  - 해결책을 찾지 못하고, 그럴때마다 커널을 다시 시작해야 해서 너무 번거로웠다. \n",
    "- 데이터셋 \n",
    "  - 긴 토큰을 잘라내고, 데이터 증강을 2배한 결과가 더 좋기는 했는데 결과가 과대적합이 아닌지 좀 의심스러웠다.\n",
    "  - 팀원분이 여기에서는 과대적합을 염려하지 않아도 괜찮을거라고 했다.\n",
    "  - 추가 데이터셋은 당연하지만 단순히 그냥 늘린다고 결과가 좋아지지는 않음을 확인했다.\n",
    "- 전체적으로 이번 프로젝트는 많이 방황하고 어떻게 해야할지 막막해서 어려웠다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d0bf5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45504c07",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df552f7e",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
