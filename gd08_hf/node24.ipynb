{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "978b8746",
   "metadata": {},
   "source": [
    "### Check. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985acdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import transformers\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e2fcd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.11.3\n",
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow.__version__)\n",
    "print(numpy.__version__)\n",
    "print(transformers.__version__)\n",
    "print(datasets.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a00109",
   "metadata": {},
   "source": [
    "### Load. Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f7e29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 저장 커스텀 모듈\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../custom\")\n",
    "\n",
    "from importlib import reload\n",
    "import custom_utils\n",
    "reload(custom_utils)\n",
    "\n",
    "from custom_utils import save_var, load_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7557ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82d5692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for and set up GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_cuda_memory_summary():\n",
    "    # Obtain and print GPU memory summary\n",
    "    memory_summary = torch.cuda.memory_summary(device=device, abbreviated=False)\n",
    "    print(memory_summary)\n",
    "    \n",
    "def empty_cuda_cache():\n",
    "    # Run your deep learning code on the GPU\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d09e9a9",
   "metadata": {},
   "source": [
    "# STEP 1. NSMC 데이터 분석 및 Huggingface dataset 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66333f66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cc9dbfb56945cda183e261075ac47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset('nsmc')\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3a256c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a3c98fba040dae08.arrow and /aiffel/.cache/huggingface/datasets/nsmc/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-4552e18e170dc7b2.arrow\n"
     ]
    }
   ],
   "source": [
    "ds_split = ds['train'].train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78c07b2",
   "metadata": {},
   "source": [
    "# STEP 2. klue/bert-base model 및 tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48f3c483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceb617d",
   "metadata": {},
   "source": [
    "# STEP 3. 위에서 불러온 tokenizer으로 데이터셋을 전처리하고, model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d842f0f",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61022d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "        padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ff60b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a747de184e497ba3f0b7d953581a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8371c20f5e43b1bac21833c6b41eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00784cbf4f75402c97021331599dbd0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_dataset_train = ds_split['train'].map(transform, batched=True)\n",
    "hf_dataset_val = ds_split['test'].map(transform, batched=True)\n",
    "hf_dataset_test = ds['test'].map(transform, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e350f7e6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '5050479', 'label': 0, 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [2, 6573, 6896, 9734, 2530, 18, 18, 18, 8875, 2119, 1, 555, 2073, 5093, 18, 18, 4828, 2434, 2015, 4635, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'document': '이정도로 지루함... 몹시도 굼벵이 같은 주인공..빨리감기 추천'}\n"
     ]
    }
   ],
   "source": [
    "print(hf_dataset_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13f0b4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 't0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b56e526a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_var(hf_dataset_train, f\"{prefix}_hf_dataset_train\")\n",
    "save_var(hf_dataset_val, f\"{prefix}_hf_dataset_val\")\n",
    "save_var(hf_dataset_test, f\"{prefix}_hf_dataset_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05b2b149",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_dataset_train = load_var(f\"{prefix}_hf_dataset_train\")\n",
    "hf_dataset_val = load_var(f\"{prefix}_hf_dataset_val\")\n",
    "hf_dataset_test = load_var(f\"{prefix}_hf_dataset_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3dfa8",
   "metadata": {},
   "source": [
    "## train - 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1356d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "output_dir = 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "15b561bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                        # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",       # evaluation하는 빈도\n",
    "    learning_rate = 2e-5,              # learning_rate\n",
    "    per_device_train_batch_size = 4,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 4,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 1,              # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,               # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61292b0",
   "metadata": {},
   "source": [
    "> batch size 를 '8' 만 되어도 메모리 부족이 된다. 하...  \n",
    "> '4' 로 해야 겨우 학습을 진행 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ca86263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a76b0cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_dataset_train,    # training dataset\n",
    "    eval_dataset=hf_dataset_val,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cc54f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94c915ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_cuda_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fd6f3b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  432881 KB |  432881 KB |  432881 KB |       0 B  |\n",
      "|       from large pool |  432384 KB |  432384 KB |  432384 KB |       0 B  |\n",
      "|       from small pool |     497 KB |     497 KB |     497 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  432881 KB |  432881 KB |  432881 KB |       0 B  |\n",
      "|       from large pool |  432384 KB |  432384 KB |  432384 KB |       0 B  |\n",
      "|       from small pool |     497 KB |     497 KB |     497 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  487424 KB |  487424 KB |  487424 KB |       0 B  |\n",
      "|       from large pool |  485376 KB |  485376 KB |  485376 KB |       0 B  |\n",
      "|       from small pool |    2048 KB |    2048 KB |    2048 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |   54542 KB |   54552 KB |  265210 KB |  210667 KB |\n",
      "|       from large pool |   52992 KB |   52992 KB |  263168 KB |  210176 KB |\n",
      "|       from small pool |    1550 KB |    2042 KB |    2042 KB |     491 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     203    |     203    |     203    |       0    |\n",
      "|       from large pool |      75    |      75    |      75    |       0    |\n",
      "|       from small pool |     128    |     128    |     128    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     203    |     203    |     203    |       0    |\n",
      "|       from large pool |      75    |      75    |      75    |       0    |\n",
      "|       from small pool |     128    |     128    |     128    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      21    |      21    |      21    |       0    |\n",
      "|       from large pool |      20    |      20    |      20    |       0    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      19    |      19    |      20    |       1    |\n",
      "|       from large pool |      18    |      18    |      19    |       1    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_cuda_memory_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e684531",
   "metadata": {},
   "source": [
    "> trainer 생성을 하면 CUDA memory 에 올라가는데, 16G 거의 꽉 찬다.  \n",
    "> 그런데 trainer 변수를 메모리에서 제거하고, torch.cuda.empty_cache() 로 메모리를 비우려고 해도 비워지지 않는다.  \n",
    "> 결과적으로 같은 설정으로 다시 학습시키려하면, 메모리 부족으로 진행이 되지 않는다.  \n",
    "> kernel 을 재시작하여 처음부터 다시 해야한다. 하...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bd2a3eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 90000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='34001' max='90000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34001/90000 4:03:35 < 6:41:13, 2.33 it/s, Epoch 1.13/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.386100</td>\n",
       "      <td>0.473481</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.489593</td>\n",
       "      <td>0.892240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.354200</td>\n",
       "      <td>0.489593</td>\n",
       "      <td>0.892240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n",
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n",
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n",
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n",
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n",
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n",
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n",
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n",
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n",
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n",
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000/config.json\n",
      "Model weights saved in results/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-6500\n",
      "Configuration saved in results/checkpoint-6500/config.json\n",
      "Model weights saved in results/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-7000\n",
      "Configuration saved in results/checkpoint-7000/config.json\n",
      "Model weights saved in results/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-7500\n",
      "Configuration saved in results/checkpoint-7500/config.json\n",
      "Model weights saved in results/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-8000\n",
      "Configuration saved in results/checkpoint-8000/config.json\n",
      "Model weights saved in results/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-8500\n",
      "Configuration saved in results/checkpoint-8500/config.json\n",
      "Model weights saved in results/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-9000\n",
      "Configuration saved in results/checkpoint-9000/config.json\n",
      "Model weights saved in results/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-9500\n",
      "Configuration saved in results/checkpoint-9500/config.json\n",
      "Model weights saved in results/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-10000\n",
      "Configuration saved in results/checkpoint-10000/config.json\n",
      "Model weights saved in results/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-10500\n",
      "Configuration saved in results/checkpoint-10500/config.json\n",
      "Model weights saved in results/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-11000\n",
      "Configuration saved in results/checkpoint-11000/config.json\n",
      "Model weights saved in results/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-11500\n",
      "Configuration saved in results/checkpoint-11500/config.json\n",
      "Model weights saved in results/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-12000\n",
      "Configuration saved in results/checkpoint-12000/config.json\n",
      "Model weights saved in results/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-12500\n",
      "Configuration saved in results/checkpoint-12500/config.json\n",
      "Model weights saved in results/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-13000\n",
      "Configuration saved in results/checkpoint-13000/config.json\n",
      "Model weights saved in results/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-13500\n",
      "Configuration saved in results/checkpoint-13500/config.json\n",
      "Model weights saved in results/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-14000\n",
      "Configuration saved in results/checkpoint-14000/config.json\n",
      "Model weights saved in results/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-14500\n",
      "Configuration saved in results/checkpoint-14500/config.json\n",
      "Model weights saved in results/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-15000\n",
      "Configuration saved in results/checkpoint-15000/config.json\n",
      "Model weights saved in results/checkpoint-15000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-15500\n",
      "Configuration saved in results/checkpoint-15500/config.json\n",
      "Model weights saved in results/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-16000\n",
      "Configuration saved in results/checkpoint-16000/config.json\n",
      "Model weights saved in results/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-16500\n",
      "Configuration saved in results/checkpoint-16500/config.json\n",
      "Model weights saved in results/checkpoint-16500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-17000\n",
      "Configuration saved in results/checkpoint-17000/config.json\n",
      "Model weights saved in results/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-17500\n",
      "Configuration saved in results/checkpoint-17500/config.json\n",
      "Model weights saved in results/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-18000\n",
      "Configuration saved in results/checkpoint-18000/config.json\n",
      "Model weights saved in results/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-18500\n",
      "Configuration saved in results/checkpoint-18500/config.json\n",
      "Model weights saved in results/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-19000\n",
      "Configuration saved in results/checkpoint-19000/config.json\n",
      "Model weights saved in results/checkpoint-19000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-19500\n",
      "Configuration saved in results/checkpoint-19500/config.json\n",
      "Model weights saved in results/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-20000\n",
      "Configuration saved in results/checkpoint-20000/config.json\n",
      "Model weights saved in results/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-20500\n",
      "Configuration saved in results/checkpoint-20500/config.json\n",
      "Model weights saved in results/checkpoint-20500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-21000\n",
      "Configuration saved in results/checkpoint-21000/config.json\n",
      "Model weights saved in results/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-21500\n",
      "Configuration saved in results/checkpoint-21500/config.json\n",
      "Model weights saved in results/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-22000\n",
      "Configuration saved in results/checkpoint-22000/config.json\n",
      "Model weights saved in results/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-22500\n",
      "Configuration saved in results/checkpoint-22500/config.json\n",
      "Model weights saved in results/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-23000\n",
      "Configuration saved in results/checkpoint-23000/config.json\n",
      "Model weights saved in results/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-23500\n",
      "Configuration saved in results/checkpoint-23500/config.json\n",
      "Model weights saved in results/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-24000\n",
      "Configuration saved in results/checkpoint-24000/config.json\n",
      "Model weights saved in results/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-24500\n",
      "Configuration saved in results/checkpoint-24500/config.json\n",
      "Model weights saved in results/checkpoint-24500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-25000\n",
      "Configuration saved in results/checkpoint-25000/config.json\n",
      "Model weights saved in results/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-25500\n",
      "Configuration saved in results/checkpoint-25500/config.json\n",
      "Model weights saved in results/checkpoint-25500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-26000\n",
      "Configuration saved in results/checkpoint-26000/config.json\n",
      "Model weights saved in results/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-26500\n",
      "Configuration saved in results/checkpoint-26500/config.json\n",
      "Model weights saved in results/checkpoint-26500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-27000\n",
      "Configuration saved in results/checkpoint-27000/config.json\n",
      "Model weights saved in results/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-27500\n",
      "Configuration saved in results/checkpoint-27500/config.json\n",
      "Model weights saved in results/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-28000\n",
      "Configuration saved in results/checkpoint-28000/config.json\n",
      "Model weights saved in results/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-28500\n",
      "Configuration saved in results/checkpoint-28500/config.json\n",
      "Model weights saved in results/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-29000\n",
      "Configuration saved in results/checkpoint-29000/config.json\n",
      "Model weights saved in results/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-29500\n",
      "Configuration saved in results/checkpoint-29500/config.json\n",
      "Model weights saved in results/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-30000\n",
      "Configuration saved in results/checkpoint-30000/config.json\n",
      "Model weights saved in results/checkpoint-30000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-30500\n",
      "Configuration saved in results/checkpoint-30500/config.json\n",
      "Model weights saved in results/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-31000\n",
      "Configuration saved in results/checkpoint-31000/config.json\n",
      "Model weights saved in results/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-31500\n",
      "Configuration saved in results/checkpoint-31500/config.json\n",
      "Model weights saved in results/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-32000\n",
      "Configuration saved in results/checkpoint-32000/config.json\n",
      "Model weights saved in results/checkpoint-32000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-32500\n",
      "Configuration saved in results/checkpoint-32500/config.json\n",
      "Model weights saved in results/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-33000\n",
      "Configuration saved in results/checkpoint-33000/config.json\n",
      "Model weights saved in results/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-33500\n",
      "Configuration saved in results/checkpoint-33500/config.json\n",
      "Model weights saved in results/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-34000\n",
      "Configuration saved in results/checkpoint-34000/config.json\n",
      "Model weights saved in results/checkpoint-34000/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:298] . unexpected pos 98344640 vs 98344592",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 28] No space left on device",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1381\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_substep_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   1585\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m             \u001b[0;31m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1587\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1588\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcaught_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m                 \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSCHEDULER_NAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:298] . unexpected pos 98344640 vs 98344592"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04691d6b",
   "metadata": {},
   "source": [
    "> 1 에퐄에 3시간 넘게 걸린다.  \n",
    "> 4시간쯤 지났을 때, 하드디스크 용량 초과로 학습이 중단되었다.  \n",
    "> 500 마다 checkpoint 저장이 되는데, 그 결과 90기가 이상 쌓이면서 하드디스크 용량이 초과되었다. 하...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e8759b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 4\n"
     ]
    }
   ],
   "source": [
    "result = trainer.evaluate(hf_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "141111dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4895934462547302, 'eval_accuracy': 0.89224}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bba6461",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"t0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1561deda",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aec718c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories[prefix] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b164bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_var(histories, \"histories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a69b15",
   "metadata": {},
   "source": [
    "Epoch 1 and more - about 34,000 steps  \n",
    "> Training Loss: 0.354200  \n",
    "> Validation Loss: 0.489593  \n",
    "> Accuracy: 0.892240  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fce24d",
   "metadata": {},
   "source": [
    "## 아래 fine-tuning 비교를 위해 6,500 steps 에서 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d57efc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                        # output이 저장될 경로\n",
    "    evaluation_strategy=\"steps\",       # evaluation하는 빈도\n",
    "    eval_steps=6000,\n",
    "    learning_rate = 2e-5,              # learning_rate\n",
    "    per_device_train_batch_size = 4,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 4,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 1,              # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,               # weight decay\n",
    "    save_total_limit=1,                # Limit the total number of checkpoints\n",
    "    save_steps=500,                    # Save a checkpoint every 500 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1caa8270",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/t0_checkpoint-500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file results/t0_checkpoint-500/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at results/t0_checkpoint-500.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load the model checkpoint\n",
    "prefix = \"t0\"\n",
    "model_checkpoint = output_dir + f\"/{prefix}_checkpoint-500\"\n",
    "ckp_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e9d575f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=ckp_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_dataset_train,    # training dataset\n",
    "    eval_dataset=hf_dataset_val,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0dd4f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6069' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6069/30000 56:31 < 3:42:55, 1.79 it/s, Epoch 0.20/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.694401</td>\n",
       "      <td>0.883133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n",
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n",
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n",
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n",
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n",
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n",
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n",
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n",
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n",
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n",
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000/config.json\n",
      "Model weights saved in results/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1316\u001b[0m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m                 if (\n\u001b[0m\u001b[1;32m   1319\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218f698",
   "metadata": {},
   "source": [
    "6,500 steps  \n",
    "> Training Loss: 0.120000  \n",
    "> Validation Loss: 0.694401  \n",
    "> Accuracy: 0.883133  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f42358",
   "metadata": {},
   "source": [
    "# STEP 4. Fine-tuning을 통하여 모델 성능(accuarcy) 향상시키기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03daeca",
   "metadata": {},
   "source": [
    "데이터 전처리, TrainingArguments 등을 조정하여 모델의 정확도를 90% 이상으로 끌어올려봅시다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac4fdbc",
   "metadata": {},
   "source": [
    "## train - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d5fa075",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,              # Output directory\n",
    "    evaluation_strategy=\"steps\",       # Evaluation frequency\n",
    "    eval_steps=10000,                    # Evaluation steps\n",
    "    learning_rate=2e-5,                # Learning rate\n",
    "    per_device_train_batch_size=4,      # Batch size per GPU\n",
    "    per_device_eval_batch_size=4,       # Evaluation batch size per GPU\n",
    "    num_train_epochs=2,                # Total number of training epochs\n",
    "    weight_decay=0.01,                 # Weight decay\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",  # Learning rate scheduler type\n",
    "    warmup_steps=500,                  # Number of warmup steps\n",
    "    save_total_limit=1,                # Limit the total number of checkpoints\n",
    "    save_steps=500,                    # Save a checkpoint every 500 steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88f6fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file results/archived/t1_checkpoint-6500/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"results/checkpoint-6000\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file results/archived/t1_checkpoint-6500/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Load the model checkpoint\n",
    "prefix = \"t1\"\n",
    "model_checkpoint = output_dir + f\"/archived/{prefix}_checkpoint-6500\"\n",
    "ckp_model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "128ffd2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='557' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  557/30000 20:07 < 17:48:03, 0.46 it/s, Epoch 0.02/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.686171</td>\n",
       "      <td>0.891100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_40/2029158057.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1314\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1316\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m                 if (\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1865\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1867\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1869\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=ckp_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_dataset_train,    # training dataset\n",
    "    eval_dataset=hf_dataset_val,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973c00ea",
   "metadata": {},
   "source": [
    "TrainingArguments 수정 후 6,500 step 에서,  \n",
    "> Training Loss: 0.198300  \n",
    "> Validation Loss: 0.686171  \n",
    "> Accuracy: 0.891100  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abfc8da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 60000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60000' max='60000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60000/60000 8:15:39, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.454100</td>\n",
       "      <td>0.483779</td>\n",
       "      <td>0.879267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.419600</td>\n",
       "      <td>0.453592</td>\n",
       "      <td>0.891867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.368800</td>\n",
       "      <td>0.451635</td>\n",
       "      <td>0.892567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.282600</td>\n",
       "      <td>0.506669</td>\n",
       "      <td>0.898667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.304900</td>\n",
       "      <td>0.466787</td>\n",
       "      <td>0.900367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.275100</td>\n",
       "      <td>0.457565</td>\n",
       "      <td>0.901933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n",
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n",
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n",
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n",
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n",
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n",
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n",
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n",
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n",
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n",
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000/config.json\n",
      "Model weights saved in results/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6500\n",
      "Configuration saved in results/checkpoint-6500/config.json\n",
      "Model weights saved in results/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7000\n",
      "Configuration saved in results/checkpoint-7000/config.json\n",
      "Model weights saved in results/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7500\n",
      "Configuration saved in results/checkpoint-7500/config.json\n",
      "Model weights saved in results/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8000\n",
      "Configuration saved in results/checkpoint-8000/config.json\n",
      "Model weights saved in results/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8500\n",
      "Configuration saved in results/checkpoint-8500/config.json\n",
      "Model weights saved in results/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9000\n",
      "Configuration saved in results/checkpoint-9000/config.json\n",
      "Model weights saved in results/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9500\n",
      "Configuration saved in results/checkpoint-9500/config.json\n",
      "Model weights saved in results/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-10000\n",
      "Configuration saved in results/checkpoint-10000/config.json\n",
      "Model weights saved in results/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-10500\n",
      "Configuration saved in results/checkpoint-10500/config.json\n",
      "Model weights saved in results/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11000\n",
      "Configuration saved in results/checkpoint-11000/config.json\n",
      "Model weights saved in results/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11500\n",
      "Configuration saved in results/checkpoint-11500/config.json\n",
      "Model weights saved in results/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12000\n",
      "Configuration saved in results/checkpoint-12000/config.json\n",
      "Model weights saved in results/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12500\n",
      "Configuration saved in results/checkpoint-12500/config.json\n",
      "Model weights saved in results/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13000\n",
      "Configuration saved in results/checkpoint-13000/config.json\n",
      "Model weights saved in results/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13500\n",
      "Configuration saved in results/checkpoint-13500/config.json\n",
      "Model weights saved in results/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14000\n",
      "Configuration saved in results/checkpoint-14000/config.json\n",
      "Model weights saved in results/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14500\n",
      "Configuration saved in results/checkpoint-14500/config.json\n",
      "Model weights saved in results/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15000\n",
      "Configuration saved in results/checkpoint-15000/config.json\n",
      "Model weights saved in results/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15500\n",
      "Configuration saved in results/checkpoint-15500/config.json\n",
      "Model weights saved in results/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16000\n",
      "Configuration saved in results/checkpoint-16000/config.json\n",
      "Model weights saved in results/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16500\n",
      "Configuration saved in results/checkpoint-16500/config.json\n",
      "Model weights saved in results/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17000\n",
      "Configuration saved in results/checkpoint-17000/config.json\n",
      "Model weights saved in results/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17500\n",
      "Configuration saved in results/checkpoint-17500/config.json\n",
      "Model weights saved in results/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18000\n",
      "Configuration saved in results/checkpoint-18000/config.json\n",
      "Model weights saved in results/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18500\n",
      "Configuration saved in results/checkpoint-18500/config.json\n",
      "Model weights saved in results/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19000\n",
      "Configuration saved in results/checkpoint-19000/config.json\n",
      "Model weights saved in results/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19500\n",
      "Configuration saved in results/checkpoint-19500/config.json\n",
      "Model weights saved in results/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-20000\n",
      "Configuration saved in results/checkpoint-20000/config.json\n",
      "Model weights saved in results/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-20500\n",
      "Configuration saved in results/checkpoint-20500/config.json\n",
      "Model weights saved in results/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-21000\n",
      "Configuration saved in results/checkpoint-21000/config.json\n",
      "Model weights saved in results/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-21500\n",
      "Configuration saved in results/checkpoint-21500/config.json\n",
      "Model weights saved in results/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-22000\n",
      "Configuration saved in results/checkpoint-22000/config.json\n",
      "Model weights saved in results/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-22500\n",
      "Configuration saved in results/checkpoint-22500/config.json\n",
      "Model weights saved in results/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-23000\n",
      "Configuration saved in results/checkpoint-23000/config.json\n",
      "Model weights saved in results/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-23500\n",
      "Configuration saved in results/checkpoint-23500/config.json\n",
      "Model weights saved in results/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-24000\n",
      "Configuration saved in results/checkpoint-24000/config.json\n",
      "Model weights saved in results/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-24500\n",
      "Configuration saved in results/checkpoint-24500/config.json\n",
      "Model weights saved in results/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-25000\n",
      "Configuration saved in results/checkpoint-25000/config.json\n",
      "Model weights saved in results/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-25500\n",
      "Configuration saved in results/checkpoint-25500/config.json\n",
      "Model weights saved in results/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-26000\n",
      "Configuration saved in results/checkpoint-26000/config.json\n",
      "Model weights saved in results/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-26500\n",
      "Configuration saved in results/checkpoint-26500/config.json\n",
      "Model weights saved in results/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-27000\n",
      "Configuration saved in results/checkpoint-27000/config.json\n",
      "Model weights saved in results/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-27500\n",
      "Configuration saved in results/checkpoint-27500/config.json\n",
      "Model weights saved in results/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-28000\n",
      "Configuration saved in results/checkpoint-28000/config.json\n",
      "Model weights saved in results/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-28500\n",
      "Configuration saved in results/checkpoint-28500/config.json\n",
      "Model weights saved in results/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-29000\n",
      "Configuration saved in results/checkpoint-29000/config.json\n",
      "Model weights saved in results/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-29500\n",
      "Configuration saved in results/checkpoint-29500/config.json\n",
      "Model weights saved in results/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-29000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-30000\n",
      "Configuration saved in results/checkpoint-30000/config.json\n",
      "Model weights saved in results/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-30500\n",
      "Configuration saved in results/checkpoint-30500/config.json\n",
      "Model weights saved in results/checkpoint-30500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-31000\n",
      "Configuration saved in results/checkpoint-31000/config.json\n",
      "Model weights saved in results/checkpoint-31000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-31500\n",
      "Configuration saved in results/checkpoint-31500/config.json\n",
      "Model weights saved in results/checkpoint-31500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-32000\n",
      "Configuration saved in results/checkpoint-32000/config.json\n",
      "Model weights saved in results/checkpoint-32000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-32500\n",
      "Configuration saved in results/checkpoint-32500/config.json\n",
      "Model weights saved in results/checkpoint-32500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-33000\n",
      "Configuration saved in results/checkpoint-33000/config.json\n",
      "Model weights saved in results/checkpoint-33000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-33500\n",
      "Configuration saved in results/checkpoint-33500/config.json\n",
      "Model weights saved in results/checkpoint-33500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-34000\n",
      "Configuration saved in results/checkpoint-34000/config.json\n",
      "Model weights saved in results/checkpoint-34000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-34500\n",
      "Configuration saved in results/checkpoint-34500/config.json\n",
      "Model weights saved in results/checkpoint-34500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-35000\n",
      "Configuration saved in results/checkpoint-35000/config.json\n",
      "Model weights saved in results/checkpoint-35000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-35500\n",
      "Configuration saved in results/checkpoint-35500/config.json\n",
      "Model weights saved in results/checkpoint-35500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-36000\n",
      "Configuration saved in results/checkpoint-36000/config.json\n",
      "Model weights saved in results/checkpoint-36000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-36500\n",
      "Configuration saved in results/checkpoint-36500/config.json\n",
      "Model weights saved in results/checkpoint-36500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-37000\n",
      "Configuration saved in results/checkpoint-37000/config.json\n",
      "Model weights saved in results/checkpoint-37000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-37500\n",
      "Configuration saved in results/checkpoint-37500/config.json\n",
      "Model weights saved in results/checkpoint-37500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-38000\n",
      "Configuration saved in results/checkpoint-38000/config.json\n",
      "Model weights saved in results/checkpoint-38000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-38500\n",
      "Configuration saved in results/checkpoint-38500/config.json\n",
      "Model weights saved in results/checkpoint-38500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-39000\n",
      "Configuration saved in results/checkpoint-39000/config.json\n",
      "Model weights saved in results/checkpoint-39000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-39500\n",
      "Configuration saved in results/checkpoint-39500/config.json\n",
      "Model weights saved in results/checkpoint-39500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-39000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-40000\n",
      "Configuration saved in results/checkpoint-40000/config.json\n",
      "Model weights saved in results/checkpoint-40000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-40500\n",
      "Configuration saved in results/checkpoint-40500/config.json\n",
      "Model weights saved in results/checkpoint-40500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-41000\n",
      "Configuration saved in results/checkpoint-41000/config.json\n",
      "Model weights saved in results/checkpoint-41000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-41500\n",
      "Configuration saved in results/checkpoint-41500/config.json\n",
      "Model weights saved in results/checkpoint-41500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-42000\n",
      "Configuration saved in results/checkpoint-42000/config.json\n",
      "Model weights saved in results/checkpoint-42000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-42500\n",
      "Configuration saved in results/checkpoint-42500/config.json\n",
      "Model weights saved in results/checkpoint-42500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-42000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-43000\n",
      "Configuration saved in results/checkpoint-43000/config.json\n",
      "Model weights saved in results/checkpoint-43000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-43500\n",
      "Configuration saved in results/checkpoint-43500/config.json\n",
      "Model weights saved in results/checkpoint-43500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-44000\n",
      "Configuration saved in results/checkpoint-44000/config.json\n",
      "Model weights saved in results/checkpoint-44000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-44500\n",
      "Configuration saved in results/checkpoint-44500/config.json\n",
      "Model weights saved in results/checkpoint-44500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-45000\n",
      "Configuration saved in results/checkpoint-45000/config.json\n",
      "Model weights saved in results/checkpoint-45000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-44500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-45500\n",
      "Configuration saved in results/checkpoint-45500/config.json\n",
      "Model weights saved in results/checkpoint-45500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-46000\n",
      "Configuration saved in results/checkpoint-46000/config.json\n",
      "Model weights saved in results/checkpoint-46000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-46500\n",
      "Configuration saved in results/checkpoint-46500/config.json\n",
      "Model weights saved in results/checkpoint-46500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-47000\n",
      "Configuration saved in results/checkpoint-47000/config.json\n",
      "Model weights saved in results/checkpoint-47000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-47500\n",
      "Configuration saved in results/checkpoint-47500/config.json\n",
      "Model weights saved in results/checkpoint-47500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-48000\n",
      "Configuration saved in results/checkpoint-48000/config.json\n",
      "Model weights saved in results/checkpoint-48000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-48500\n",
      "Configuration saved in results/checkpoint-48500/config.json\n",
      "Model weights saved in results/checkpoint-48500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-49000\n",
      "Configuration saved in results/checkpoint-49000/config.json\n",
      "Model weights saved in results/checkpoint-49000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-49500\n",
      "Configuration saved in results/checkpoint-49500/config.json\n",
      "Model weights saved in results/checkpoint-49500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-49000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-50000\n",
      "Configuration saved in results/checkpoint-50000/config.json\n",
      "Model weights saved in results/checkpoint-50000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-50500\n",
      "Configuration saved in results/checkpoint-50500/config.json\n",
      "Model weights saved in results/checkpoint-50500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-51000\n",
      "Configuration saved in results/checkpoint-51000/config.json\n",
      "Model weights saved in results/checkpoint-51000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-51500\n",
      "Configuration saved in results/checkpoint-51500/config.json\n",
      "Model weights saved in results/checkpoint-51500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-52000\n",
      "Configuration saved in results/checkpoint-52000/config.json\n",
      "Model weights saved in results/checkpoint-52000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-51500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-52500\n",
      "Configuration saved in results/checkpoint-52500/config.json\n",
      "Model weights saved in results/checkpoint-52500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-53000\n",
      "Configuration saved in results/checkpoint-53000/config.json\n",
      "Model weights saved in results/checkpoint-53000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-53500\n",
      "Configuration saved in results/checkpoint-53500/config.json\n",
      "Model weights saved in results/checkpoint-53500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-54000\n",
      "Configuration saved in results/checkpoint-54000/config.json\n",
      "Model weights saved in results/checkpoint-54000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-53500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-54500\n",
      "Configuration saved in results/checkpoint-54500/config.json\n",
      "Model weights saved in results/checkpoint-54500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-55000\n",
      "Configuration saved in results/checkpoint-55000/config.json\n",
      "Model weights saved in results/checkpoint-55000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-55500\n",
      "Configuration saved in results/checkpoint-55500/config.json\n",
      "Model weights saved in results/checkpoint-55500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-56000\n",
      "Configuration saved in results/checkpoint-56000/config.json\n",
      "Model weights saved in results/checkpoint-56000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-56500\n",
      "Configuration saved in results/checkpoint-56500/config.json\n",
      "Model weights saved in results/checkpoint-56500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-56000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-57000\n",
      "Configuration saved in results/checkpoint-57000/config.json\n",
      "Model weights saved in results/checkpoint-57000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-57500\n",
      "Configuration saved in results/checkpoint-57500/config.json\n",
      "Model weights saved in results/checkpoint-57500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-58000\n",
      "Configuration saved in results/checkpoint-58000/config.json\n",
      "Model weights saved in results/checkpoint-58000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-58500\n",
      "Configuration saved in results/checkpoint-58500/config.json\n",
      "Model weights saved in results/checkpoint-58500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-59000\n",
      "Configuration saved in results/checkpoint-59000/config.json\n",
      "Model weights saved in results/checkpoint-59000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-58500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-59500\n",
      "Configuration saved in results/checkpoint-59500/config.json\n",
      "Model weights saved in results/checkpoint-59500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-59000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-60000\n",
      "Configuration saved in results/checkpoint-60000/config.json\n",
      "Model weights saved in results/checkpoint-60000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-59500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60000, training_loss=0.3681246737162272, metrics={'train_runtime': 29739.8295, 'train_samples_per_second': 8.07, 'train_steps_per_second': 2.017, 'total_flos': 6.31466532864e+16, 'train_loss': 0.3681246737162272, 'epoch': 2.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_dataset_train,    # training dataset\n",
    "    eval_dataset=hf_dataset_val,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71999c4",
   "metadata": {},
   "source": [
    "2 epochs - 60,000 steps  \n",
    "> Training Loss: 0.275100  \n",
    "> Validation Loss: 0.457565  \n",
    "> Accuracy: 0.901933  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54fde643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 27:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.evaluate(hf_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43d5ed74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4542900025844574,\n",
       " 'eval_accuracy': 0.90268,\n",
       " 'eval_runtime': 1649.7985,\n",
       " 'eval_samples_per_second': 30.307,\n",
       " 'eval_steps_per_second': 7.577,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "312655e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"t1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf3063d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t0': {'eval_loss': 0.4895934462547302, 'eval_accuracy': 0.89224}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09f1b26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories[prefix] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f876e87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_var(histories, \"histories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db182c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories = load_var(\"histories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa0c0f6",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "> eval_loss: 0.454290  \n",
    "> eval_accuracy: 0.90268  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f3a949",
   "metadata": {},
   "source": [
    "# STEP 5. Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf5d304",
   "metadata": {},
   "source": [
    "아래 링크를 바탕으로 bucketing과 dynamic padding이 무엇인지 알아보고, 이들을 적용하여 model을 학습시킵니다.\n",
    "\n",
    "- [Data Collator](https://huggingface.co/docs/transformers/v4.30.0/en/main_classes/data_collator)\n",
    "- [Training Arguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) - group_by_length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3bd81e",
   "metadata": {},
   "source": [
    "## train - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06220bff",
   "metadata": {},
   "source": [
    "STEP 4에 학습한 결과와 bucketing을 적용하여 학습시킨 결과를 비교해보고, 모델 성능 향상과 훈련 시간 두 가지 측면에서 각각 어떤 이점이 있는지 비교해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b70f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1aa9706",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(huggingface_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c4b52e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,              # Output directory\n",
    "    evaluation_strategy=\"steps\",       # Evaluation frequency\n",
    "    eval_steps=10000,                    # Evaluation steps\n",
    "    learning_rate=2e-5,                # Learning rate\n",
    "    per_device_train_batch_size=4,      # Batch size per GPU\n",
    "    per_device_eval_batch_size=4,       # Evaluation batch size per GPU\n",
    "    num_train_epochs=1,                # Total number of training epochs\n",
    "    weight_decay=0.01,                 # Weight decay\n",
    "    lr_scheduler_type=\"cosine_with_restarts\",  # Learning rate scheduler type\n",
    "    warmup_steps=500,                  # Number of warmup steps\n",
    "    save_total_limit=1,                # Limit the total number of checkpoints\n",
    "    save_steps=500,                    # Save a checkpoint every 500 steps\n",
    "    group_by_length=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b092433",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "del huggingface_model\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6358a24e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 4:07:36, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.444000</td>\n",
       "      <td>0.435730</td>\n",
       "      <td>0.884500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.382300</td>\n",
       "      <td>0.401782</td>\n",
       "      <td>0.896600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.391521</td>\n",
       "      <td>0.899567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n",
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n",
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n",
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n",
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n",
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n",
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n",
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n",
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n",
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n",
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000/config.json\n",
      "Model weights saved in results/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6500\n",
      "Configuration saved in results/checkpoint-6500/config.json\n",
      "Model weights saved in results/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7000\n",
      "Configuration saved in results/checkpoint-7000/config.json\n",
      "Model weights saved in results/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7500\n",
      "Configuration saved in results/checkpoint-7500/config.json\n",
      "Model weights saved in results/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8000\n",
      "Configuration saved in results/checkpoint-8000/config.json\n",
      "Model weights saved in results/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8500\n",
      "Configuration saved in results/checkpoint-8500/config.json\n",
      "Model weights saved in results/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9000\n",
      "Configuration saved in results/checkpoint-9000/config.json\n",
      "Model weights saved in results/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9500\n",
      "Configuration saved in results/checkpoint-9500/config.json\n",
      "Model weights saved in results/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-10000\n",
      "Configuration saved in results/checkpoint-10000/config.json\n",
      "Model weights saved in results/checkpoint-10000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-10500\n",
      "Configuration saved in results/checkpoint-10500/config.json\n",
      "Model weights saved in results/checkpoint-10500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11000\n",
      "Configuration saved in results/checkpoint-11000/config.json\n",
      "Model weights saved in results/checkpoint-11000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-11500\n",
      "Configuration saved in results/checkpoint-11500/config.json\n",
      "Model weights saved in results/checkpoint-11500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12000\n",
      "Configuration saved in results/checkpoint-12000/config.json\n",
      "Model weights saved in results/checkpoint-12000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-12500\n",
      "Configuration saved in results/checkpoint-12500/config.json\n",
      "Model weights saved in results/checkpoint-12500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13000\n",
      "Configuration saved in results/checkpoint-13000/config.json\n",
      "Model weights saved in results/checkpoint-13000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-13500\n",
      "Configuration saved in results/checkpoint-13500/config.json\n",
      "Model weights saved in results/checkpoint-13500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14000\n",
      "Configuration saved in results/checkpoint-14000/config.json\n",
      "Model weights saved in results/checkpoint-14000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-14500\n",
      "Configuration saved in results/checkpoint-14500/config.json\n",
      "Model weights saved in results/checkpoint-14500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15000\n",
      "Configuration saved in results/checkpoint-15000/config.json\n",
      "Model weights saved in results/checkpoint-15000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-15500\n",
      "Configuration saved in results/checkpoint-15500/config.json\n",
      "Model weights saved in results/checkpoint-15500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16000\n",
      "Configuration saved in results/checkpoint-16000/config.json\n",
      "Model weights saved in results/checkpoint-16000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-16500\n",
      "Configuration saved in results/checkpoint-16500/config.json\n",
      "Model weights saved in results/checkpoint-16500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17000\n",
      "Configuration saved in results/checkpoint-17000/config.json\n",
      "Model weights saved in results/checkpoint-17000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-17500\n",
      "Configuration saved in results/checkpoint-17500/config.json\n",
      "Model weights saved in results/checkpoint-17500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18000\n",
      "Configuration saved in results/checkpoint-18000/config.json\n",
      "Model weights saved in results/checkpoint-18000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-18500\n",
      "Configuration saved in results/checkpoint-18500/config.json\n",
      "Model weights saved in results/checkpoint-18500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19000\n",
      "Configuration saved in results/checkpoint-19000/config.json\n",
      "Model weights saved in results/checkpoint-19000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-19500\n",
      "Configuration saved in results/checkpoint-19500/config.json\n",
      "Model weights saved in results/checkpoint-19500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-20000\n",
      "Configuration saved in results/checkpoint-20000/config.json\n",
      "Model weights saved in results/checkpoint-20000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-20500\n",
      "Configuration saved in results/checkpoint-20500/config.json\n",
      "Model weights saved in results/checkpoint-20500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-21000\n",
      "Configuration saved in results/checkpoint-21000/config.json\n",
      "Model weights saved in results/checkpoint-21000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-21500\n",
      "Configuration saved in results/checkpoint-21500/config.json\n",
      "Model weights saved in results/checkpoint-21500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-22000\n",
      "Configuration saved in results/checkpoint-22000/config.json\n",
      "Model weights saved in results/checkpoint-22000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-22500\n",
      "Configuration saved in results/checkpoint-22500/config.json\n",
      "Model weights saved in results/checkpoint-22500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-23000\n",
      "Configuration saved in results/checkpoint-23000/config.json\n",
      "Model weights saved in results/checkpoint-23000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-23500\n",
      "Configuration saved in results/checkpoint-23500/config.json\n",
      "Model weights saved in results/checkpoint-23500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-24000\n",
      "Configuration saved in results/checkpoint-24000/config.json\n",
      "Model weights saved in results/checkpoint-24000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-24500\n",
      "Configuration saved in results/checkpoint-24500/config.json\n",
      "Model weights saved in results/checkpoint-24500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-25000\n",
      "Configuration saved in results/checkpoint-25000/config.json\n",
      "Model weights saved in results/checkpoint-25000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-25500\n",
      "Configuration saved in results/checkpoint-25500/config.json\n",
      "Model weights saved in results/checkpoint-25500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-26000\n",
      "Configuration saved in results/checkpoint-26000/config.json\n",
      "Model weights saved in results/checkpoint-26000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-26500\n",
      "Configuration saved in results/checkpoint-26500/config.json\n",
      "Model weights saved in results/checkpoint-26500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-27000\n",
      "Configuration saved in results/checkpoint-27000/config.json\n",
      "Model weights saved in results/checkpoint-27000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-27500\n",
      "Configuration saved in results/checkpoint-27500/config.json\n",
      "Model weights saved in results/checkpoint-27500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-28000\n",
      "Configuration saved in results/checkpoint-28000/config.json\n",
      "Model weights saved in results/checkpoint-28000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-28500\n",
      "Configuration saved in results/checkpoint-28500/config.json\n",
      "Model weights saved in results/checkpoint-28500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-29000\n",
      "Configuration saved in results/checkpoint-29000/config.json\n",
      "Model weights saved in results/checkpoint-29000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-29500\n",
      "Configuration saved in results/checkpoint-29500/config.json\n",
      "Model weights saved in results/checkpoint-29500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-29000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to results/checkpoint-30000\n",
      "Configuration saved in results/checkpoint-30000/config.json\n",
      "Model weights saved in results/checkpoint-30000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-29500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30000, training_loss=0.4265346171061198, metrics={'train_runtime': 14857.1138, 'train_samples_per_second': 8.077, 'train_steps_per_second': 2.019, 'total_flos': 3.15733266432e+16, 'train_loss': 0.4265346171061198, 'epoch': 1.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=hf_dataset_train,    # training dataset\n",
    "    eval_dataset=hf_dataset_val,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a09dd0c",
   "metadata": {},
   "source": [
    "30,000 step\n",
    "\n",
    "> Training Loss: 0.35  \n",
    "> Validation Loss: 0.391521  \n",
    "> Accuracy: 0.898567  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284631b1",
   "metadata": {},
   "source": [
    "> 1 epoch, 30,000 step 에서,  \n",
    "> Data Collator, group_by_length 를 썼을 때,  \n",
    "> 이전 결과인 0.368800 / 0.451635 / 0.892567 에 비해서  \n",
    "> Loss 는 좀 더 낮아지고, Accuracy 는 좀 더 올라간다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "743b1923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 50000\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17884' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12500/12500 2:14:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = trainer.evaluate(hf_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93920a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.39486292004585266,\n",
       " 'eval_accuracy': 0.8979,\n",
       " 'eval_runtime': 1651.5242,\n",
       " 'eval_samples_per_second': 30.275,\n",
       " 'eval_steps_per_second': 7.569,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "08bd32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"t2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "60cdb09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "histories[prefix] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dae8efd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_var(histories, \"histories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfba0333",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "> eval_loss: 0.394862...  \n",
    "> eval_accuracy: 0.8979  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecc87f",
   "metadata": {},
   "source": [
    "> 시간상 30,000 step 밖에 진행을 못했지만,  \n",
    "> 50,000 step 에서 0.9 에 도달했던 앞선 결과와 비교하면,  \n",
    "> 40,000 step 에서 0.9 에 도달할 것으로 판단된다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813747d",
   "metadata": {},
   "source": [
    "### 추가 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e663b2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 30000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10001' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10001/30000 1:05:43 < 2:11:28, 2.54 it/s, Epoch 0.33/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to results/checkpoint-500\n",
      "Configuration saved in results/checkpoint-500/config.json\n",
      "Model weights saved in results/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to results/checkpoint-1000\n",
      "Configuration saved in results/checkpoint-1000/config.json\n",
      "Model weights saved in results/checkpoint-1000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-1500\n",
      "Configuration saved in results/checkpoint-1500/config.json\n",
      "Model weights saved in results/checkpoint-1500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2000\n",
      "Configuration saved in results/checkpoint-2000/config.json\n",
      "Model weights saved in results/checkpoint-2000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-2500\n",
      "Configuration saved in results/checkpoint-2500/config.json\n",
      "Model weights saved in results/checkpoint-2500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3000\n",
      "Configuration saved in results/checkpoint-3000/config.json\n",
      "Model weights saved in results/checkpoint-3000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-3500\n",
      "Configuration saved in results/checkpoint-3500/config.json\n",
      "Model weights saved in results/checkpoint-3500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4000\n",
      "Configuration saved in results/checkpoint-4000/config.json\n",
      "Model weights saved in results/checkpoint-4000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-4500\n",
      "Configuration saved in results/checkpoint-4500/config.json\n",
      "Model weights saved in results/checkpoint-4500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5000\n",
      "Configuration saved in results/checkpoint-5000/config.json\n",
      "Model weights saved in results/checkpoint-5000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-5500\n",
      "Configuration saved in results/checkpoint-5500/config.json\n",
      "Model weights saved in results/checkpoint-5500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6000\n",
      "Configuration saved in results/checkpoint-6000/config.json\n",
      "Model weights saved in results/checkpoint-6000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-6500\n",
      "Configuration saved in results/checkpoint-6500/config.json\n",
      "Model weights saved in results/checkpoint-6500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7000\n",
      "Configuration saved in results/checkpoint-7000/config.json\n",
      "Model weights saved in results/checkpoint-7000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-7500\n",
      "Configuration saved in results/checkpoint-7500/config.json\n",
      "Model weights saved in results/checkpoint-7500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8000\n",
      "Configuration saved in results/checkpoint-8000/config.json\n",
      "Model weights saved in results/checkpoint-8000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-8500\n",
      "Configuration saved in results/checkpoint-8500/config.json\n",
      "Model weights saved in results/checkpoint-8500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9000\n",
      "Configuration saved in results/checkpoint-9000/config.json\n",
      "Model weights saved in results/checkpoint-9000/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to results/checkpoint-9500\n",
      "Configuration saved in results/checkpoint-9500/config.json\n",
      "Model weights saved in results/checkpoint-9500/pytorch_model.bin\n",
      "Deleting older checkpoint [results/checkpoint-9000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 4\n"
     ]
    }
   ],
   "source": [
    "res = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3aa3ffd",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1249fb7c",
   "metadata": {},
   "source": [
    "- 이틀 안에 1 epoch 당 3~4시간 걸리는 학습을 진행하기는 좀 버거웠다.\n",
    "  - 이틀이라는 시간은 좀 짧은 시간이었지만, \n",
    "  - 별다른 커스터마이징과 큰 파인튜닝 없이도 이 정도 성능이라면 실제 간단한 프로젝트에서는 괜찮지 않을까.\n",
    "- 아쉬운 점\n",
    "  - 학습 데이터를 많이 줄여서 빠른 학습으로 TrainingArguments 파라미터를 이것저것 조정해봤어도 좋았을 것 같다.\n",
    "  - Loss/Accuracy 값으로 결과를 확인하는 것 외에 어떤 문장을 잘못 판단했는지 분석해보지 못한 것도 좀 아쉽다. \n",
    "  - nsmc 데이터를 가지고 klue/bert-base 모델의 결과와 이전에 다른 모델을 썼던 결과를 비교해보면 좋겠다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
